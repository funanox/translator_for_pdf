Chapter 28Assurance andSustainabilityThere are two ways of constructing a software design.
 One way is tomake it so simple that there are obviously no deﬁciencies.
And the other way is to make it so complicated thatthere are no obvious deﬁciences.
– Tony HoareSecurity engineers are the litigation lawyers of tech.
We only get paid when something is wrongand we can always ﬁnd something wrong.
– Dave WestonTo improve is to change; to be perfect is to change often.
– Winston Churchill28.
1IntroductionI’ve covered a lot of material in this book, some of it quite tricky.
 But I’ve leftthe hardest parts to the last.
 These are the questions of assurance – whetherthe system will work; its cousin compliance – how you satisfy other people aboutthis; and sustainability – how long it will keep on working.
 How do you decide toship the product? How do you sell the security and safety case to your insurers?How long are you going to have to maintain it, and at what cost?What’s new in 2020 is sustainability.
 In the 2008 edition, I called this chapter‘Evaluation and Assurance’, and ended up by remarking that sound processes forvulnerability disclosure and product update were beginning to be as importantas pre-market testing.
 The emphasis back then was on testing and evaluationschemes like the Common Criteria.
 That world is now moribund: the idea thata device should be secure because someone spent $100,000 getting an evalua-tion lab to test it ﬁve years ago would strike most people nowadays as quaint.
91128.
1.
 INTRODUCTIONAssurance is no longer static.
Ten years ago, we knew how to make two types of secure system.
 We hadthings like phones and laptops, which contained software and were online, butwere sort-of secure because the software got patched once a month.
 And wehad things like cars and medical devices, which contained software but were notonline; you tested them to death before they were put on sale, and then hopedfor the best, as patching meant a physical recall.
 Now we’ve started to put carsand medical devices online, so they have to be patched online too.
The number of vulnerabilities reported in common platforms is so great thatwe have to automate the process.
As we described in the previous chapter,the software development lifecycle has become DevOps and then DevSecOps;the online components of systems are maintained using continuous integration,while components in the ﬁeld need regular upgrades.
With a new product, assurance can be measured roughly by whether capablemotivated people have beat up on the system enough.
 But how do you deﬁne‘enough’? And how do you deﬁne the ‘system’? How do you deal with peoplewho protect the wrong thing? And how do you deal with usability? Too manysystems are designed for use by alert experienced professionals, but are tootricky for ordinary folk or are intolerant of error.
 Once they get ﬁelded, theinjury claims or fraud disputes start to roll in.
In the security engineering of a decade ago, we often talked of assurancein terms of evaluation, which was about how you assembled the evidence toconvince your boss, your clients, and (if need be) a jury, that it did indeed work(or that it did work at some particular time in the past).
 As we’ve seen againand again, things often fail because one principal carries the cost of protectionwhile another carries the risk of failure.
 Third-party evaluation schemes suchas the Common Criteria were supposed to make these risks more transparentand mitigate them, but ended up acting as a liability shield – particularly in thepublic sector and in regulated industries such as banking.
 Systems protectingclassiﬁed information were subjected to extensive compliance requirements andhad to use evaluated products at the attack surface; much the same held, withdi↵erent details, for payment systems.
 Evaluation was driven by compliance.
Compliance is still the main driver of security design and investment, butit places much less emphasis on requiring evaluated products at speciﬁc trustboundaries.
 The details vary from one industry to another.
 When we look atmedical systems, cars or aircraft we ﬁnd regulatory regimes driven by safety thatare starting to incorporate security.
 General business systems have policy setby the Big Four audit ﬁrms, and payment systems by PCI.
 We have touched onsome of their speciﬁc requirements in previous chapters; there are some broaderissues and principles that we’ll try to pull together here.
Right at the start of this book, in Figure 1.
1, I presented a framework forsecurity engineering based on incentives, policy, mechanism and assurance.
• Incentives are critical, as we’ve seen time and again.
 They often fall outsidea formal assurance process, but are the most critical part of the environ-ment within which the security policy has to be deﬁned.
• Policy is often neglected, as we’ve seen: people often end up protectingSecurity Engineering912Ross Anderson28.
1.
 INTRODUCTIONthe wrong things, or protecting the right things in the wrong way.
 Wespent much of Part II of the book exploring security policies for di↵erentapplications.
• Mechanisms may be independent of policy, but can interact with it bymaking some policy options easier to implement.
• Assurance is our estimate of the likelihood that a system will not fail in aparticular way.
 This estimate can be based on a number of factors, suchas the process used to develop and maintain it; the people who developand maintain it; and speciﬁc technical assessments, such as the statisticsof failure rates, bug reports, breach reports and insurance claims.
 It wastraditionally about evaluation – whether, given the agreed security policyand strength of mechanisms, a product had been implemented correctly.
Had the bugs been found and ﬁxed? Could you quantify the mean time tofailure? Nowadays it’s increasingly about the vendor’s future commitment.
For how long, and how diligently, will the system be patched?By the second edition of this book in 2008, I noted that the big missingfactor was usability.
 Most system failures have a signiﬁcant human component.
Usability is a cross-cutting issue in the above framework: if done properly, it hasa subtle e↵ect on policy, a large e↵ect on choice of mechanisms, and a huge e↵ecton how systems are tested.
 It cuts across individual products: a common reasonfor accidents is that di↵erent products have di↵erent user interfaces, an issue towhich we’ll return later.
 However, designers often saw assurance simply as anabsence of obvious bugs, and designed technical protection mechanisms withoutstopping to consider human frailty.
 (There are some exceptions: bookkeepingsystems are designed to cope with both error and fraud.
)Usability is not purely a matter for end-users, but for developers too.
 Manyvulnerabilities arise because security mechanisms are too hard to understand ortoo ﬁddly to use.
 Developers often didn’t use operating-system access controls,but just ran their code with administrator privilege instead; when mobile phonesdidn’t allow this, they kept demanding too many permissions for their apps;and cryptography often uses ECB mode as it’s the default with many cryptolibraries.
Customers and vendors want di↵erent things at multiple points in the valuechain.
 Regulation doesn’t always help, because governments have multiple agen-das of their own, often in conﬂict: intelligence agencies, safety regulators andcompetition authorities pull in di↵erent directions.
 It’s in this treacherous land-scape that the assurance game is played.
Assurance is thus a political and economic process.
 It is also a dynamicprocess, just like the development of code or of documents.
 Just as you havebugs in your code, and in your speciﬁcation, you will also have things wrongwith your security and safety policies, leading to omissions and errors in yourtest suite.
 So assurance is steadily turning from something done as a one-o↵project to another aspect of continuous evolution.
With that warning, it’s helpful to start with the classic problem of evaluatinga static product that is built in a single project.
Security Engineering913Ross Anderson28.
2.
 EVALUATION28.
2EvaluationProduct evaluation tackles the problem of the lemons market we discussed insection 8.
3.
3: when customers can’t measure quality, bad products drive outgood ones.
 Security has been a lemons market for generations.
 An 1853 bookon locksmithing justiﬁed disclosing the ‘secrets’ of the trade on the groundsthat the burglars knew them already; it was just the locksmiths’ customerswho were ignorant [1895].
 Modern consumer-grade products, from anti-virussoftware to mobile phone apps, are way beyond the ability of most consumersto assess technically.
 If they are just going to rely on the brand name, the vendormay as well buy ads rather than hiring security engineers.
 As for professionalproducts, the tech majors may employ enough PhDs to do an assessment, butbanks don’t – not even money-centre banks1.
 In earlier chapters, we discussed anumber of examples of static security standards against which various productsget evaluated and certiﬁed.
Banks and governments are among the keenestpurchasers of certiﬁed security products.
That may have been where computer security got started ﬁfty years ago, butas computers end up everywhere, we have to look at other industries too.
 Dozensof industries have their own safety standards, with which security mechanismsare increasingly intertwined.
 We already talked about electricity transmissionand distribution in section 23.
8.
1.
 Safety standards for software in road vehicleshave developed over decades; we talked about trucks in 14.
3.
3.
 Now that bothtrucks and cars have multiple systems for assisted driving and are connectedto the Internet, they have critical security as well as safety requirements.
 Thesame is happening for medical equipment and much else.
I’ll explore this via a number of case studies.
 Two important questions arewhether the evaluation is conducted by the relying party or by a third party,and whether the standards are static or dynamic.
28.
2.
1Alarms and locksThe US insurance industry set up a joint testing lab in 1894, alarmed at theﬁre risks from electric lightbulbs; it was incorporated in 1901 as Underwriters’Laboratories, a nonproﬁt that develops ﬁre safety and other standards, andstarted approving security products in 1913 [1916].
 Other countries have similarbodies.
 An evaluator spends a ﬁxed budget of e↵ort looking for ﬂaws and writesa report, after which the lab either approves a device, turns it down or demandssome changes.
As the insurance industry bears much of the cost of ﬁres and burglaries,incentives are somewhat aligned, although in practice these labs get much oftheir income from testing fees.
 One risk is inertia: the standards may not keepup with progress.
 In the case of high-security locks, a lab in 2000 might havedemanded ten minutes’ resistance to picking and say nothing about bumping.
We described in section 13.
2.
4 how bumping tools had improved enough to be1In my late 20s and early 30s I worked in banking, and when I went to an interbank securitystandards committee there were only about four of us in the room who knew what we weretalking about – of whom one was from IBM.
 Fintech has become an order of magnitude morecomplex since then.
Security Engineering914Ross Anderson28.
2.
 EVALUATIONa major threat by 2010, and picks have got better too.
 We also described insection 13.
2.
3 how bank vaults certiﬁed to resist attack for ten minutes can bedefeated in much less by a modern angle grinder or a burning bar.
 Insurancelabs in some countries, such as Germany, have been prepared to withdraw cer-tiﬁcations as attacks got better; in the USA, they appear reluctant to, perhapsfor fear of being sued.
 The willingness of an industry to tolerate changing stan-dards may depend on its structure: a mature industry with a handful of largeplayers can drag its feet a lot more than a growing competitive one.
28.
2.
2Safety evaluation regimesSafety standards tend to emerge one industry at a time in response to majoraccidents or scandals.
 The safety of drugs and medical devices is regulated inthe USA by the FDA, set up in 1906 by President Theodore Roosevelt afterjournalists exposed abuses in the patent medicine industry.
 It turned out thatthe top-selling medicine in America was just a dilute solution of sulphuric acidand turpentine – really cheap to manufacture, yet tasting nasty enough thatpeople could believe it was good for them [2050].
 As for air safety, the ﬁrststep was in 1931, when America’s top football coach Knute Rockne died in aplane crash caused by structural failure, causing a public outcry that led to theestablishment of the National Transportation Safety Board.
 The FAA was setup later by President Eisenhower after a 1956 crash between two airliners overthe Grand Canyon killed all 128 people aboard the two planes [684].
 As for thecar industry, it managed to disclaim liability for safety for decades.
 Vendorscompeted to decorate cars with chromium rather than ﬁt them with seat belts,until Ralph Nader’s book ‘Unsafe at Any Speed’ spurred Congress to set upNational Highway Tra�c Safety Administration (NHTSA) in 1970; its powerand inﬂuence grew with successive safety scandals.
Europe harmonised a patchwork of national laws into the Product LiabilityDirective in 1985, adding further regulations and safety agencies by industrysector.
 Since then, the European Union has developed into the world’s leadsafety regulator, with its agencies setting safety standards in industries fromaviation through railway signals to toys [1148].
 With cars, for example, Europegenerally requires safety testing by independent labs2, while America doesn’t;but most US vendors have their US models tested independently too, as Europecreated the ‘industry norm’ by which US courts assess tort cases when thingsgo wrong.
 In this sense, Europe has become a ‘regulatory superpower’.
The EU’s overall safety strategy is to evolve a set of standards by negotiationwith industry working groups and lobbyists and update them every seven toten years.
 Many products that cause serious harm, such as cars, have to getexplicit approval, typically following testing in an independent laboratory.
 Lessdangerous goods such as toys require self-certiﬁcation: the vendor places a ‘CE’mark on the product to assert that it complies with all relevant standards.
This removes some of the excuses that vendors might use when non-compliant2Europe delegates type approval to Member States, most of which have a Type ApprovalAuthority which delegates testing to a specialist lab.
 In Germany, that’s T¨UV.
 Some smallercountries have a TAA that allows the manufacturer to do its own testing, with a TAA inspectorpresent.
Security Engineering915Ross Anderson28.
2.
 EVALUATIONproducts cause accidents; it’s also used for a wide range of components from carbrakes to industrial pressure valves.
28.
2.
3Medical device safetySafety regulation is a complex ecosystem, imperfect in many ways.
 For example,there has long been controversy in both America and Europe over medical devicesafety.
This came to prominence in the 1980s when bugs in the Therac 25medical accelerator caused the death of three patients and injured three more.
The cause was a software bug that surfaced as a usability issue: if the operatoredited the machine’s parameters too quickly, they could get the machine into adangerous state where it delivered far too much radiation to the patient.
 Thecase study is set reading for my software engineering students even today [1149].
Figure 28.
1: – two infusion pumps that are apparently of the same model (photocourtesy of Harold Thimbleby)The most lethal medical devices nowadays are probably infusion pumps, usedto administer intravenous drugs and other ﬂuids to patients in hospital.
 Manyof the fatal accidents are usability failures.
 Just look at Figure 28.
1: each ofthese claims to be a ‘BodyGuard 545’ yet to increase the dose on the machineon the left, you press ‘2’ while on the right you press ‘5’.
 An emergency roommight have equipment from half-a-dozen di↵erent vendors, all with di↵erent userinterfaces.
 Doctors and nurses occasionally press the wrong button, the wrongdose gets administered, or the dose for an eight-hour transfusion is given all inone bolus – and patients die.
 Infusion pumps kill about as many people as carsdo, with the body count being in the low thousands in the UK and the low tensof thousands in the USA [1878].
Surely this could be ﬁxed with standards? Well, there are standards.
 Forexample, ‘litres’ is supposed to be marked with a capital ‘L’ so it’s not mistakenfor a ‘1’, but you can see on the right-hand image that although the ‘0L/h’complies with this, the ‘500ml’ does not.
 So why is the standard not enforced?Well, the FDA budget of engineering e↵ort is about half a day per device,and vendors don’t give the engineers actual devices to play with.
 It’s just apaperwork review3.
 In addition, usability falls outside the FDA’s scope.
 This3By way of comparison, when colleagues and I helped to evaluate a burglar alarm designedfor low-consequence risks such as small shops and houses, our budget was two person-weeks.
Security Engineering916Ross Anderson28.
2.
 EVALUATIONis, I hear, a result of lobbying by the industry to ‘cut red tape’.
 The fact thattwo di↵erent devices are marketed as the same product is a common strategyto minimise compliance costs.
There has recently been international guidance for usability engineering ofmedical devices in the form of ISO/IEC 62366-2, which took e↵ect in 2018.
This is a signiﬁcant advance which covers a lot of ground, but usability is a hugeﬁeld.
 The new standard is very basic, and explains at length that manufacturersshould not just list hazards in a legal warning leaﬂet, or even highlight themwith notices on the equipment – they should actually try to mitigate them, andin the process understand how their equipment is likely to be used and abused.
It describes a number of assessment techniques the engineer could use, but“insu�cient experience with the type of medical device” is just one bullet pointon its list of factors that might contribute to use errors.
 Manufacturers will ﬁndall this expensive, and will no doubt talk to their lawyers about how much reallyhas to be done.
 Safety in number entry alone is a complex ﬁeld [1879]; everyvendor should probably train an expert in it, and in dozens of other techniquestoo, but many will do as little as they think they can get away with.
 In theend, a usability assessment will now be in the trolleyload of paperwork themanufacturer presents to regulators, at least outside the USA.
 But it’s unclearwhether the confusion arising when nurses also use the di↵erent interfaces ofcompetitors’ equipment will be taken as seriously as it should be.
This is all teaching us that pre-market testing isn’t enough for medical de-vice safety – you need diligent post-market surveillance too.
 This started tobe introduced throughout Europe in 2017 following a scandal about defectivebreast implants [233].
 In the UK, a further scandal about teratogenic drugsand pelvic mesh implants let to an Independent Medicines and Medical De-vices Safety Review, which in 2020 documented decades of indi↵erence to safetyand recommended among many other things that regulation ‘needs substantialrevision particularly in relation to adverse event reporting and medical deviceregulation’ [503].
 In May 2020, a new EU medical device regulation (2017/745)was supposed to require post-market surveillance systems and a public databaseof anonymised incident reports; implementation was postponed until May 2021.
And in June 2020, the UK Parliament passed a Medicines and Medical DevicesAct that will enable ministers to amend the existing regulations after Brexit.
The mood music there, however, is to make Britain a more attractive placefor drug companies and medical device makers, not a safer place for patients.
Within Britain’s National Health Service, it’s hard to make a career as a safetyspecialist4.
Now here’s an interesting question.
 If infusion pumps kill as many peopleas cars or – in the USA – as guns, why aren’t people more worked up, as theyare about road safety and gun control? Well, the harm is both low-key anddi↵use.
 At your local hospital, such accidents probably kill less than one persona month, and many of them won’t be noticed, as people on infusion pumps tendto be fairly sick anyway.
 When they are noticed, they are more likely to beblamed on the nurse, rather than on the medical director who bought pumps4The UK NHS has a Healthcare Safety Investigations Branch, established in 2016, but itinvestigates what it’s told to, often has to keep its ﬁndings conﬁdential, and doesn’t have orseek enforcement powers to require other healthcare organisations to make changes [875].
Security Engineering917Ross Anderson28.
2.
 EVALUATIONfrom half a dozen di↵erent suppliers following nice lunches with the sales folks.
As a cause of death in the hospital, recorded safety usability failures don’t makeit into the top twenty, and so don’t get attention from politicians or the press.
(The exception is when a safety failure has a security angle, as people are verysensitive indeed about hostile intent.
 I’ll discuss this in section 28.
4.
2 below.
)The standardisation of user interfaces is managed better in industries whereaccidents and their causes are more visible.
 Road tra�c accidents are fairlyvisible and most people drive, so car crashes and their causes are a topic ofconversation.
 The controls in cars are now fairly standard, with the acceleratoron the right, the brake in the middle and the clutch on the left.
 Things aren’tperfect; if you’re in a hurry, you might get in a rental car, drive o↵ down thefreeway, then struggle to ﬁnd the light switch as night falls.
 But it used to bemuch worse.
 Some cars in the 1930s had the accelerator in the middle, whilethe ﬁrst mass-produced car, the Model T Ford, had a hand throttle and a pedalgear-change, like a motorcycle.
 The average modern driver would have a hardtime getting such a car out of the rental lot.
28.
2.
4Aviation safetyAviation has much stronger safety incentives still: airliners are worth eight ornine ﬁgures, crashes are front-page news, they cause pilots as well as passengersto lose their lives and airline CEOs may even lose their bonuses.
 Pilots payattention to accident reports, and are required to train on each type of planethey ﬂy.
 This has led the vendors to standardise cockpit design, starting withthe Boeing 757 and 767, which were designed from the start to be so similar thata pilot trained on one could ﬂy the other.
 If nurses were similarly required toget a type rating for each infusion pump, that would cost real money, hospitalexecutives would pay attention, the vendors would eventually follow Boeing,and a lot of lives could be saved.
Yet we ﬁnd regulatory failure in aviation too, and an example was exposedwith the Boeing 737Max crashes.
 Since Boeing had bought McDonnell Dou-glas in 1997 and become the only US ﬁrm making large aircraft, the FederalAviation Administration had come to see its role as supporting Boeing.
 Thecompany’s engineers were allowed to take over much of the safety evaluationand certiﬁcation work that the FAA had done in the past.
 An even more toxice↵ect of the takeover was that McDonnell Douglas executives took over, thecompany moved its headquarters from Seattle to Chicago, and was no longerrun by engineers but by ﬁnance people who had already destroyed one engi-neering company and whose goal now was to milk the maximum proﬁts fromthe new monopoly.
 Boeing’s traditional engineering culture was sidelined andcorners were cut [729].
 Two crashes followed, in Indonesia and Ethiopia, killing346 people.
 The cause was reminiscent of the Therac case a generation earlier:a design error in software that surfaced as a life-threatening usability failure.
In order to compete with the latest model of Airbus, Boeing needed to makethe 737 more fuel e�cient quickly, and this meant larger engines, which had to beﬁtted further forward, or it would have required re-engineering the airframe tothe point that it would have been a new plane for regulatory purposes, and wouldhave taken much longer to certify.
 The new engine location made the aircraftSecurity Engineering918Ross Anderson28.
2.
 EVALUATIONharder to trim at high speeds, so Boeing added software called the ManeuveringCharacteristics Augmentation System (MCAS) to the ﬂight control computerto compensate for this.
The MCAS software needed to know the aircraft’s angle of attack, and thecritical design error was to rely on one angle-of-attack sensor rather than two,although these are often damaged by ground handlers and bird strikes.
 Theimplementation error was that, with an incorrect angle-of-attack input, the planecould get into a regime where the pilots needed to pull about 50kg on the yoketo keep the plane level.
 This was compounded by an error in safety analysis:the unintended activation of the MCAS software was not anticipated.
As aresult, Boeing didn’t do a proper failure modes and e↵ects analysis and thesoftware’s behaviour was not even documented in the pilot manual.
 The pilotswere not trained how to diagnose the problem or switch MCAS o↵.
 Boeinghad become complacent about the ability of pilots to cope with the chaos of acockpit emergency with many alarms going o↵ at once [1055].
The company had also got away with bullying investigators over a similarprevious crash in the Netherlands in 2009, and initially hoped that the Indonesiacrash could be blamed on pilot error [857].
 The FAA responded to the crashby sending an emergency airworthiness directive to all known U.
S.
 operators ofthe airplane, which consisted of inserting a warning notice in the airplane ﬂightmanual [665].
 ; However, the warning light that alerted pilots to disagreementbetween the two sensors had been made an airline option, like a sun roof in acar, and the operation of the switch that could disable MCAS was changed tomake it less intuitive [155].
 A number of U.
S.
 pilots logged complaints, with onedescribing the manual as ‘almost criminally insu�cient’ [139]; but the FAA sawsuch complaints as only relevant to air carrier operations and did not analysethem for global safety hazards [664].
After the second crash in Ethiopia, other countries’ regulators started ground-ing the 737Max, and the FAA could no longer protect them.
 Boeing had lost$18.
7bn in sales by March 2020, when the coronavirus pandemic closed downcommercial aviation sales, as well as $60bn in market capitalisation.
 This wasby some distance the world’s biggest ever software failure, in terms of both liveslost and economic damage.
 The ﬁx, approved in August 2020, involves not justa software change so that MCAS reads both angle-of-attack sensors and deploysonly once per ﬂight and with limited stick force; but a procedural change sothat both sensors are checked pre-ﬂight; an update to pilot training; and a reg-ulatory change so that the FAA, rather than Boeing, checks each plane aftermanufacture [592].
When analysing safety, it’s not enough to think of it as a technical test-ing matter.
Psychology, incentives, institutions and power matter too.
Thepower of lobbyists, and the risk that regulators will be captured by the in-dustry they’re supposed to regulate, place real limits on what can be achievedby testing regimes.
 Over time, measures designed for risk assessment and riskreduction become industrialised and tend to become a matter of compliance,which ﬁrms then seek to pass at minimum cost.
 It’s also important to stopthinking of problems as ‘aerospace engineering’ versus ‘software engineering’,or ‘safety engineering’ versus ‘security engineering’.
 If you want to be a goodengineer you need to try to understand every aspect of the whole system thatSecurity Engineering919Ross Anderson28.
2.
 EVALUATIONmight be relevant.
28.
2.
5The Orange BookThe ﬁrst serious computer security testing regime was the Orange Book – theTrusted Computer Systems Evaluation Criteria [544].
 We touched on this insection 9.
4, where I described the multilevel security model that the US Depart-ment of Defense was trying to promote through it.
 Orange Book evaluationswere done from 1985–2000 at the NSA on computer systems proposed for govern-ment use and on security products such as cryptographic devices.
 In incentiveterms, it was a collective relying-party scheme, as with insurance.
The Orange Book and its supporting documents set out a number of eval-uation classes, in three bands.
 C1 meant just that there was an access-controlsystem; C2 corresponded to carefully conﬁgured commercial systems.
 In thenext band, B1 meant mandatory access control; B2 added covert channel anal-ysis, a trusted path to the TCB from the user, and severe penetration testing;while B3 required the TCB had to be minimal, tamper-resistant, and subjectto formal analysis and testing.
 At the top band, A1 added a requirement forformal veriﬁcation.
 (Very few systems made it to that level.
)The evaluation class of a system determined what spread of informationcould be processed on it.
 The example I gave in section 9.
6.
2 was that a systemevaluated to B3 could process information at Unclassiﬁed, Conﬁdential andSecret, or at Conﬁdential, Secret and Top Secret.
When the Orange Book was written, the Department of Defense thought thatthey paid high prices for high-assurance computers because the markets weretoo small, and hoped that security standards would expand the market.
 ButOrange Book evaluations followed government work practices.
 A governmentuser would want some product evaluated; the NSA would allocate people to doit; given traditional civil service caution and delay, this could take two or threeyears; the product, if successful, would join the evaluated products list; and thebill was picked up by the taxpayer.
 Evaluated products were always obsolete,so the market stayed small, and prices stayed high5.
Other governments had similar ideas.
European countries developed theInformation Technology Security Evaluation Criteria (ITSEC), a shared schemeto help their defense contractors compete against US suppliers.
 This introduceda pernicious innovation – that the evaluation was not arranged by the relyingparty (the government) but by the vendor.
 Vendors started to shop aroundfor the lab that would give their product the easiest ride, whether by askingfewer questions, charging less money, taking the least time, or all of the above.
Contractors could obtain approval as a commercial licensed evaluation facility(CLEF), and in theory the CLEF might have its license withdrawn if it cutcorners.
 That never happened.
5To this day, most governments are hopeless at buying technology and pay several timesthe market rate, if they make it work at all.
 The reasons are much broader and deeper thanstandards.
See for example section 10.
4.
4 on the £11bn failure of a project to moderniseBritain’s National Health Service, and section 23.
5 for the $6bn failure of the Pentagon’sJoint Tactical Radio System.
Security Engineering920Ross Anderson28.
2.
 EVALUATION28.
2.
6FIPS 140 and HSMsThe second evaluation scheme promoted by the US government in the 20thcentury was NIST’s FIPS 140 scheme for assessing the tamper-resistance ofcryptographic processors.
 This was aimed at helping the banking industry aswell as the government, and as I described in section 18.
4 it uses a numberof independent laboratories as contractors.
 Launched in 1994, it is still goingstrong today, and is favoured by US customers of cryptographic equipment.
There are two main failure modes of FIPS 140.
 The ﬁrst is that it covers thecryptographic device’s hardware, not its software, and many FIPS 140 evaluateddevices (even at the highest levels) run applications with intrinsic vulnerabil-ities.
Weak algorithms, legacy modes of operation and vulnerable APIs aremandated by bank standards bodies for backwards compatibility, as describedin section 20.
5.
The ﬁx for this has been a growing emphasis on standardsset by PCI, the payment industry’s self-regulation scheme, which I describe insection 12.
5.
2.
The second is that the FIPS 140-1 standard has a big gap between level 3and level 4 for historical reasons I discussed in section 18.
4.
 FIPS 140 level 3 iseasy to obtain (you just pot the circuit in epoxy to make it inaccessible to casualprobing) and some level-3 devices are not too hard to break (you just scrape o↵the epoxy with a knife).
 Level 4 is really hard, and only a few devices ever madethat grade.
 So many vendors aim at what the industry calls, informally, ‘level3.
5’.
 As this doesn’t have any formal expression in the FIPS standard, ﬁrmsoften rely on the Common Criteria instead when talking to customers outsidethe USA.
28.
2.
7The Common CriteriaThis sets the stage for the Common Criteria.
Following the collapse of theSoviet Union in 1989, military budgets were cut, and it wasn’t clear where theopponents of the future would come from.
Eventually the US and its alliesagreed to scrap their national schemes and replace them with a single standard– the Common Criteria for Information Technology Security Evaluation [1396].
The work was substantially done in 1994–1995, and the European ITSECmodel won out over the Orange Book approach.
 Evaluations at all but the high-est levels are done by CLEFs, are supposed to be recognised in all participatingcountries, and vendors pay for them.
The innovation was support for multiple security policies.
 Rather than ex-pecting all systems to conform to Bell-LaPadula, the Common Criteria evaluatea product against a protection proﬁle (PP), which is a set of security functionalrequirements and assurance requirements for a class of product.
 You can thinkof it as a detailed security policy, but oriented at products rather than systems,and expanded into several dozen pages of detail.
 There are protection proﬁles foroperating systems, access control systems, boundary control devices, intrusiondetection systems, smartcards, key management systems, VPN clients, votingmachines, and even transponders that identify when a domestic waste bin waslast emptied.
 Anyone could propose a protection proﬁle and have it evaluatedSecurity Engineering921Ross Anderson28.
2.
 EVALUATIONby the lab of their choice.
 It’s not that the defence community abandoned mul-tilevel security, so much as tried to mainstream its own evaluation system bygetting commercial ﬁrms to use it for other purposes too.
 But an evaluationdepends entirely on what was measured and how.
 Some aspects of security wereexplicitly excluded, including cryptography, emission security (as the NATOstandards were classiﬁed) and administrative procedures (which was bad newsfor usability testing).
The Common Criteria have enjoyed some limited success.
 Its evaluationsare used in specialised markets, such as smartcards, hardware security modules,TPMs and electronic signature devices, where sectoral due-diligence rules (suchas PCI) or regulation (such as electronic signature laws) create a compliancerequirement.
 Evaluations of such devices were kept honest for a while by aninformal cartel run by SOG-IS (the senior o�cials group – information security)– a committee of representatives of the intelligence agencies of EU countries.
However the operation of the CC outside Europe has been a bit of a joke, andeven within Europe it has been undermined by both companies and countriesgaming the system.
 The UK withdrew in 2019.
28.
2.
7.
1The gory detailsTo discuss the Common Criteria in detail, we need some jargon.
 The productunder test is known as the target of evaluation (TOE).
 The rigor with which theexamination is carried out is the evaluation assurance level (EAL) and can rangefrom EAL1, for which functional testing is su�cient, all the way up to EAL7which demands not only thorough testing but a formally veriﬁed design.
 Thehighest evaluation level commonly obtained for commercial products is EAL4,although in 2020 there are 85 products at EAL6 or above out of 1472 certiﬁedunder CC, and many smartcards are evaluated to EAL4+ which means EAL4plus one or more of the requirements set at higher levels.
When devising something from scratch, the idea is to ﬁrst work out a threatmodel, then create a security policy, reﬁne it to a protection proﬁle (PP) andevaluate it (if a suitable one doesn’t exist already), then do the same for thesecurity target, then ﬁnally evaluate the actual product.
 A protection proﬁleconsists of security requirements, their rationale, and an EAL, all for a classof products.
 It’s supposed to be expressed in an implementation-independentway to enable comparable evaluations across products and versions.
 A securitytarget (ST) is a reﬁnement of a protection proﬁle for a speciﬁc product.
 Onecan evaluate a PP to ensure that it’s complete, consistent and technically sound,and an ST too.
 The evaluations are ﬁled with the national authority, which istypically the defensive arm of the local signals intelligence agency.
 The endresult is a registry of protection proﬁles and a catalogue of certiﬁed products.
There is a stylized way of writing a PP or ST.
 For example, FCO_NRO is afunctionality component (hence F) relating to communications (CO) and it refersto non-repudiation of origin (NRO).
 Other classes include FAU (audit) and FCS(crypto support).
There are also catalogues of• threats, such as T.
Load_Mal – “Data loading malfunction: an attackerSecurity Engineering922Ross Anderson28.
2.
 EVALUATIONmay maliciously generate errors in set-up data to compromise the securityfunctions of the TOE”• assumptions, such as A.
Role_Man – “Role management: management ofroles for the TOE is performed in a secure manner” (in other words, thedevelopers, operators and so on behave themselves)• organizational policies, such as P.
Crypt_Std – “Cryptographic standards:cryptographic entities, data authentication, and approval functions mustbe in accordance with ISO and associated industry or organizational stan-dards”• objectives, such as O.
Flt_Ins – “Fault insertion: the TOE must be resis-tant to repeated probing through insertion of erroneous data”• assurance requirements, such as ADO_DEL.
2 – “Detection of modiﬁcation:the developer shall document procedures for delivery of the TOE or partsof it to the user”A protection proﬁle should now contain a rationale, which typically consistsof tables showing how each threat is controlled by one or more objectives, andin the reverse direction how each objective is necessitated by some combinationof threats and environmental assumptions.
 It will also justify the selection ofan assurance level and requirements for strength of mechanism.
The fastest way to get the hang of this may be to read the core CC docu-mentation itself, then a few proﬁles.
 The quality varies widely.
 For example, aprotection proﬁle for automatic cash dispensers, written in management-speakwith clip art, ‘has elected not to include any security policy’ and misses many ofthe problems that were well known when it was written in 1999 [340].
 A proﬁlefor voting machines from 2007 [563] was written more in politicians’ language,but at least with reasonable clarity6.
Protection proﬁles for smartcards emphasise maintaining conﬁdentiality ofthe chip design by imposing NDAs on contractors, shredding waste and soon [650], while in practice most attacks on smartcards used probing or power-analysis attacks for which knowledge of the chip mask was irrelevant.
 This hasdeveloped into a political row, as I discussed in section 18.
6.
4: the smartcardvendors have pushed the evaluation labs into demanding that all cryptographicproducts be secure against ‘advanced persistent threats’.
 The ﬁght is over as-surance requirement AVA_VAN.
5 which essentially requires that the entire de-velopment environment should be air-gapped, like the Top Secret systems atan intelligence agency.
 An air gap in itself won’t stop a capable opponent, asthe Iranians found out with Stuxnet and the Americans with Snowden; but itcauses real inconvenience to normal IT companies who rely on Github and othercloud-based systems.
 And that’s entirely the point: the smartcard ﬁrms don’twant HSMs or enclaves encroaching on their markets.
6This appears designed to support French ﬁrms’ drive to export population registrationsystems, and it is these rather than the actual voting machines that are often the real weakpoint in elections – as I discussed in section 7.
4.
2.
2.
Security Engineering923Ross Anderson28.
2.
 EVALUATION28.
2.
7.
2What goes wrong with the Common CriteriaBy the time the second edition of this book came out in 2008, industry peoplehad a lot of complaints about the Common Criteria, which I discussed thereand which I update more brieﬂy here.
• The biggest complaint for years has been the cost and bureaucracy of theprocess.
 A startup wanting to sell devices such as HSMs will nowadayshave to spend several million Euros and several years of e↵ort to navi-gate the process.
 In practice the CC have become a moat that defendsestablished cartels.
• The next biggest is that, as well as avoiding ‘technical physical’ aspectssuch as Emsec or crypto algorithms, the CC ignore administrative securitymeasures, which means in practice ignoring usability.
In general, userinterfaces are considered to be somebody else’s problem.
• Protection proﬁles are designed by their sponsor ﬁrms to rig the market.
I mentioned above how the smartcard ﬁrms demand that HSM vendorsalso use air-gapped systems to push their costs up.
The gaming oftenleads to insecure products: vendors write their PPs to cover the thingsthey can do easily.
 They might evaluate the boot code, but leave most ofthe operating system outside the scope.
 Recall the API attacks on HSMsdescribed in section 20.
5; some vulnerable HSMs were CC-certiﬁed, andsimilar failures are seen in other CC-certiﬁed products too.
• Sometimes the protection proﬁles might be sound, but the way they’remapped to the application isn’t.
 In section 26.
5.
2 I discussed the Euro-pean eIDAS regulation which requires businesses to recognise digital sig-natures made using smartcards, and encouraged governments to demandthem for interactions such as ﬁling tax returns.
 The main problem in thisapplication, as I discussed in section 18.
6.
1, is the lack of a trusted inter-face.
 As that problem’s too hard, it’s excluded, and the end result is a‘secure’ signature on whatever the virus or Trojan in your PC sent to yoursmartcard.
 This hole was duly slathered with several layers of fudge.
 PPswere written for a smartcard to function as a ‘Secure Signature-CreationDevice’; other PPs appeared for HSMs, and for the signature activationmodule (SAM) – the server software that passes them digital objects tobe signed.
 The HSM plus the SAM are evaluated as a qualiﬁed signa-ture creation device (QSCD) [29].
 But the front-end server software usedby the service provider is only audited, not certiﬁed, and if you’re luckythe app on your phone or tablet might have RASP on it as a malwarecountermeasure, as I discussed in section 12.
7.
4.
 That is what lobbyistscan achieve: the whole certiﬁcation machinery has been twisted to allowservices like Docusign inside the tent, so long as they use a CC certiﬁedHSM to hold their signature keys.
• The CC claim not to assume any speciﬁc development methodology, butin practice assume a waterfall approach.
 There’s a nod in the directionof policy evolving in response to experience but re-evaluation of PPs orproducts is declared to be outside the scope.
 So they’re unable to cope withSecurity Engineering924Ross Anderson28.
2.
 EVALUATIONnormal security development lifecycles, or with commercial products thatget monthly security patches.
 (The same goes for FIPS; of the availablestandards, only PCI can cope with updates.
)• The Criteria are technology-driven, when in most applications it’s thebusiness processes that should drive protection decisions.
 We’re learningthe hard way that hand-marked paper ballots are way better than votingmachines for all sorts of reasons.
 Security is a property of systems, not ofproducts.
• The rigour of the evaluations varies widely between countries, with Ger-many generally considered to be almost impossibly di�cult, the Nether-lands in the middle, while Spain and Hungary let their CLEFs give spon-sors an easy ride.
 Nobody within the system can actually say this in publicwithout causing a diplomatic incident, so it cannot be ﬁxed.
 The costsalso vary, with an evaluation in Germany costing perhaps three times whatyou pay in Hungary.
• The Common Criteria brand isn’t well defended.
I described in sec-tion 12.
6.
1.
1 how PIN entry devices claimed by VISA to have been eval-uated under the Common Criteria were insecure; GCHQ’s response wasthat as the evaluation had not been registered with them, and the deviceswere not claimed to be ‘CC certiﬁed’ it wasn’t their problem.
 So suppliersare free to continue describing a defective terminal as ’CC evaluated’.
 Abusiness would not tolerate such abuse of its trademark.
• More generally, there’s nothing on liability: ‘The procedures for use ofevaluation results in accreditation are outside the scope of the CC’.
In the second edition of this book, I took the view that Common Criteriaevaluations were somewhat like a rubber crutch.
 Such a device has all sortsof uses, from winning a judge’s sympathy through wheedling money out of agullible government to whacking people round the head.
 Just don’t try to putserious weight on it.
28.
2.
7.
3Collaborative protection proﬁlesIn an attempt to deal with these criticisms, collaborative protection proﬁles(cPPs) started to appear in 2015.
 The idea was to move away from the EALlevels towards a single protection proﬁle for each class of secure device, and todevelop that proﬁle as a collaborative e↵ort among ﬁrms in an industry, withinput from government and academics [462].
 The hope was to stop securityevaluations being abused in strategic games between competitor ﬁrms.
Theresults of this can now be seen in 2020 by browsing the catalogue of evaluatedproducts on the CC website.
 Vendors in France and Germany still o↵er manysmartcards, and related products such as electronic signature creation devices,with certiﬁcates at EAL4+ or EAL6; that’s the legacy of the SOG-IS cartel.
Outside Europe, though, the CC system has been completely captured byvendor interests.
 American ﬁrms o↵er many ﬁrewalls, routers and other net-working products, evaluated according to industry cPPs; and Japanese ﬁrmsSecurity Engineering925Ross Anderson28.
2.
 EVALUATIONo↵er a range of printers and fax machines.
 So what is a secure fax machine– does it encrypt faxes? Not at all; it just behaves as you’d expect a fax ma-chine to (if you’re old enough to remember them).
 In short, cPPs have become amarketing mechanism, and are now undermining the traditional CC core.
 Firmswanting to sell electronic signature systems can have them evaluated under acPP which is considered EAL4, and most customers can’t tell the di↵erencebetween that and an EAL4+ evaluation done under the old rules.
28.
2.
8The ‘Principle of Maximum Complacency’There’s a substantial literature on the economics of standards, as there aremany contexts in which people have to choose between them.
 If you’re a brightteenager, do you apply to a top university and risk getting a second-class degree,or should you go to a local college and be a star? Should you worry about gradeinﬂation eroding the value of your degree in either case? If you’re raising moneyfor a startup, should you get your money from business angels or try to get abig-name venture fund on board? An IT vendor wondering whether to go forsome kind of certiﬁcation faces somewhat similar choices.
 And even nationsplay certiﬁcation games.
 The large service ﬁrms all have their EU headquartersin Ireland as it has long been Dublin’s policy to have the most relaxed regimeof privacy regulation in Europe, as well as the lowest corporate taxes.
 Whatoptions are there for dealing with such games?The most inﬂuential model of such choices is a 2006 paper on forum shop-ping by Josh Lerner and Jean Tirole7.
 Their model is a three-stage game inwhich the sponsor selects a certiﬁer, the certiﬁer then studies the o↵ering andperhaps demands some changes, and ﬁnally the end-users make decisions tobuy or not [1143].
 The big question is whether competition between certiﬁerswill result in better standards, or in a race to the bottom.
 In most cases theprinciple of maximum complacency wins out: owners seek endorsement from asingle certiﬁer, and resist attempts to get them to improve the product.
 Only incertain circumstances can competition improve quality.
 One example is whereNGOs compete to certify products as sustainable: there, the certiﬁer cares moreabout the users’ outcome than the sponsors do, and the desired property isn’tstrongly controlled by a single sponsor.
 Another is competition between eliteuniversities: students have no market power, and enough employers will pay apremium for elite graduates that there’s plenty of incentive for Cambridge tocompete with Oxford, MIT and Berkeley.
Where there are more players than just the sponsor, the certiﬁer and theusers, things get more complicated.
Certiﬁcation games take place in a much larger ecosystem.
 A company in-vents some new product and sells it to some customers.
 The customers thenwant a standard, and some tests to satisfy their auditors.
 They may want theinventor to license the product to their established suppliers, or at least to asecond supplier.
 Other inventors pile in, and all of a sudden there’s a patentpool.
 The ﬁrms negotiate long and hard to get their patents in to maximisetheir share of the royalties; this often results in horrible standards that are in-7Tirole won the 2014 Nobel for this and much other work in market power and regulation.
Security Engineering926Ross Anderson28.
2.
 EVALUATIONsecure and hard to ﬁx (see section 14.
2.
4 on smart meters; there are many moreexamples).
 The patent pools may become cartels that prevent new market en-trants; this complaint has been made of the GSMA standards around 5G (seesection 22.
2.
4).
 The GSMA has also been criticised for its Network EquipmentSecurity Assurance Scheme (NESAS) where the vendor pays for a security as-sessment that only takes a few days (and now allows remote audits because ofthe pandemic).
 In short, industrial strategy doesn’t optimise for great productsso much as for monopolies or cartels.
Where a market is dominated by a monopoly, customer and political pressuremay eventually cause the monopolist to pay attention to security, and it can evenbe rational for a monopolist to internalise some of the security externalities(see the Microsoft case in section 27.
5.
3).
 But in the general case, of complexsupply chains with some steps dominated by cartels, it can be a lot harder.
The complexities in security certiﬁcation are roughly (a) the relying parties –those at risk if the thing gets hacked – may be customers, third parties suchas insurers, or the public (b) the sponsors may be vendors, customers, relyingparties or associations of any of these (c) the testers may compete on priceor on quality, and this means the lowest quality threshold they can get awaywith subject to not losing a license from an accreditation body, which may bea government entity or a trade association (d) there may be more than oneaccreditation body, plus politics between them.
 So we can have multiple layersof indirection and we occasionally even get competition about “who certiﬁes thecertiﬁers”.
 To make sense of things we have to look at actual cases in detail.
In the case of CC evaluated products at EAL4 or above such as smartcardsand HSMs, suppose Alice’s company sells a product to Bob’s Bank and getsCharlie the certiﬁer to say it’s secure, after which Bob’s customer Dorothydefrauds another customer Eve and absconds.
 How does the evaluation changethings when Eve now claims her money back from Bob in court?Bob willargue he wasn’t negligent because he operated according to the standards of theindustry, so isn’t liable to reimburse Eve.
 This argument is even more powerfulif Charlie signed o↵ on his system.
 Charlie’s role is not so much a technicalauthority as a liability shield.
 So Alice will work only as hard as she has toto satisfy Charlie.
 Charlie will compete with his competitors and a race to thebottom will ensue.
 The upshot in real life was that the payment card brandsset up PCI to take over Charlie’s role.
We discussed in section 12.
5.
2 howsuch standards shift liability in banking: they protect the bank more than themerchant (surprise, surprise).
In the case of electronic signature devices, as we discussed in section 28.
2.
7.
2above, smartcard industry lobbying led Europe to pass signature laws that givesspecial force to signatures created with certiﬁed products, even when these areinsecure.
 Lobbying by online service signature providers such as Docusign gotthem on board too.
 The ultimate e↵ect is not security but a tax.
 (And to ﬁlea tax return in some EU countries you have to get it signed by such a service,adding an extra twenty Euros to your tax accountant’s fee.
)So should certiﬁcation be voluntary? An interesting case study is by BenEdelman of the Trust-e scheme to certify websites.
 He discovered that certiﬁedwebsites were more likely to attempt to load malware on to your computer,rather than less.
 Adverse selection turned the scheme into a negative signal ofSecurity Engineering927Ross Anderson28.
2.
 EVALUATIONquality: the weaker vendors certiﬁed their websites, while well-known consumerbrands didn’t bother [612].
 The reason for this was that Trust-e certiﬁcation,being voluntary, was cheap, and the technical barrier to certiﬁcation was alsolow.
But although industry lobbies like to talk of ‘cutting red tape’, how manymight be happy with the outright abolition of a government-backed safety orsecurity standard or agency? In practice, lobbyists seek to capture regulatorsrather than abolish them.
 Many regulatory regimes function both as moats toprevent incumbents being challenged too easily by startups and also as liabilityshields.
 As an example, we discussed in section 17.
3 how Amazon, Microsoft,Google and IBM have restricted sales of face-recognition software – among themost controversial of their products – until it’s regulated.
28.
2.
9Next stepsSince Brexit, the UK and Europe have diverged.
 Europe passed a Cybersecu-rity Act (regulation 2019/881) which strengthens the European Network andInformation Security Agency (ENISA) and places it at the centre of its strat-egy.
 ENISA is to act as a centre of expertise and liaise with sectoral regulatorsin banking, aviation, energy and telecomms, as well as the data protection au-thorities.
 I expect this will be of major importance in the long run, as safetyand security regulation are coming together and will inevitably be managed on asectoral basis by the standards bodies for cars, aircraft, medical devices, railwaysignals and so on.
 I will return to this later.
As for the certiﬁcation of information security products, its approach mightbe described as ‘one more heave’: it is setting up an EU Cybersecurity Cer-tiﬁcation Framework under ENISA which will take over as the top-level certi-ﬁer.
 It’s supposed to “help avoid the multiplication of conﬂicting or overlappingnational cybersecurity certiﬁcation schemes and thus reduce costs for under-takings operating in the digital single market” [655].
 It will apply to servicesand processes as well as products.
As I write in 2020, the details are stillbeing worked out, but the intention is that sponsoring bodies of EU memberstates will run certiﬁcation at three levels, ranging from ‘basic’ which entails thevendor self-assessing conformance with standards and assuming responsibilityfor compliance, through ‘substantial’ which will involve veriﬁcation of securityfunctionality, to ‘high’ which will involve ENISA taking over from SOG-IS thesupervision of the smartcard / HSM / e-signature kit currently evaluated atEAL4 and above.
The UK government was concerned about certiﬁcation for many years andwas involved in pushing cPPs in order to try to make certiﬁcation more stan-dardised.
 But by 2017 they had come to the conclusion that the Criteria wereneither necessary nor su�cient for security, and GCHQ withdrew as a sponsorfrom 2019.
 It no longer licenses CLEFs or approves certiﬁcations, although UKorganisations may continue to use certiﬁcations created elsewhere8.
 It has longhad its own national product certiﬁcation scheme, now known as commercial8One of my spies in the Doughnut says ‘We absolutely recognise any CC certiﬁcate fromany producing nation as though it were our own and our assurance processes assign thatcertiﬁcate precisely the weight it deserves :-)’Security Engineering928Ross Anderson28.
2.
 EVALUATIONproducts assurance (CPA), but the only consumer product for which it currentlymaintains CPA certiﬁcation is the smart meter discussed in section 14.
2.
4.
 Fu-ture legislation will require basic security for IoT devices, including a ban ondefault passwords and a requirement for a software update mechanism; this isbeing done in harness with ETSI, leading to a draft European standard ETSIEN 303 645 V2.
1 [646].
The direction of travel is now to look at process rather than product, bothfor ﬁrms developing critical equipment for Britain’s national infrastructure, andmore generally.
 The general scheme, Cyber Essentials, is mandated for govern-ment contractors supplying IT services or handling personal information.
There was already the ISO 27001 standard for security management, whichwe mentioned in section 12.
2.
4: this is expensive, having been turned into anincome stream by the big accountancy ﬁrms, and about as useless as CC.
 Almostall of the large security breaches happen at ﬁrms with ISO 27001 certiﬁcation,where the auditor said something was OK that wasn’t.
 The auditors have torely on what the ﬁrms tell them, and a ﬁrm that doesn’t know how to protectits systems will just say ‘We have a great process for X’ when they don’t.
 Whyshould a small business owner cough up tens of thousands for that, unless theyneed it to bid for government contracts? And why should a government imposesuch a tax?So the Cyber Essentials scheme focuses on the very basic stu↵and costs only £300 for a validated self-certiﬁcation.
 Its target was small andmedium enterprises, but the ﬁrst ﬁrms to be actually certiﬁed under it werelarge ﬁrms like banks and phone companies who wanted to add every singletassel to their corporate due diligence.
As governments bicker, we’ve seen the emergence of a private sector stan-dard, Bitsight.
 Recall how in the ﬁrst chapter I remarked that in the corporateworld, a trusted system often means one acceptable to insurers.
 Recall also howin section 2.
2.
1.
6 we described how the NSA has a system called Mugshot thatcrawls the Internet looking for vulnerable systems, and another called Xkeyscorethat enables cyber-warriors to ﬁnd vulnerable systems near a target of interest?Well, Bitsight does Mugshot for the private sector, but instead of attackingcompanies’ systems it rates ﬁrms for cybersecurity risk by counting how manyof their servers are not patched up to date, and how many other indicators ofcompromise are visible.
 They have come to dominate insurance market assess-ments because they give a single numerical rating at a time when the insuranceindustry, which is cyclical, is having its proﬁts squeezed and can no longer getclients to ﬁll out long questionnaires about their cybersecurity practices.
 Thismakes sense in the Lerner-Tirole model, as Bitsight is motivated to keep aheadof possible competitors, just like an elite university.
 Their ratings are bringingmore honesty to the ecosystem than most of the schemes promoted by gov-ernments and audit ﬁrms, but have some interesting side-e↵ects.
 For example,service ﬁrms are now less willing to sponsor capture-the-ﬂag competitions forschools; if the Bitsight crawler sees a vulnerable system in your IP address spacethat you set up as a target for such an exercise, it can cut your Bitsight ratingby more than 10%, which can cost you real business.
So much for certifying products and business processes.
 In the next section,we look more closely at dependability metrics from the viewpoints of failureanalysis, bug tracking, cross-product dependencies, open-source software andSecurity Engineering929Ross Anderson28.
3.
 METRICS AND DYNAMICS OF DEPENDABILITYthe development team.
28.
3Metrics and dynamics of dependabilityAs dependability becomes a lifetime property we need better ways of measuringit.
 We know that it is often a function of the development team; we discussedthe capability maturity model in section 27.
5.
3.
 To get secure code, you need tohire smart people with a suitable mix of skills and get them to work together onshared projects so they learn to work together.
 In the process, you measure howwell they’re doing and improve it by giving feedback and constantly improvingthe process and tools.
 But how do you do the measurement?This has two main aspects: reliability growth, as systems become moredependable over time with testing and bug ﬁxing, and vulnerability disclosure,as bugs are found and may or may not be ﬁxed.
28.
3.
1Reliability growth modelsThe growth of reliability as systems get more testing, both in the lab and in theﬁeld, is of interest to many more people than just software engineers; nuclear,electrical and aerospace engineers all depend on reliability models and metrics.
In the simplest possible case – where the tester is trying to ﬁnd a single bugin a system – a reasonable model is the Poisson distribution: the probabilityp that the bug remains undetected after t statistically random tests is givenby p = e�Et where E depends on the proportion of possible inputs that ita↵ects [1175].
 So where the reliability of a system is dominated by a single bug– say when we’re looking for the ﬁrst bug in a system, or the last one – reliabilitygrowth can be exponential.
But extensive empirical investigations have shown that in large and complexsystems, the likelihood that the t-th test fails is not proportional to e�Et but tok/t for some constant k.
 So reliability grows very much more slowly.
 This wasﬁrst documented in the bug history of IBM mainframe operating systems [18],and has been conﬁrmed in many other studies [1198].
 As a failure probabilityof k/t means a mean time between failure (MTBF) of about t/k, reliabilitygrows linearly with testing time.
 This result is often stated by the safety criticalsystems community as ‘If you want a mean time between failure of a millionhours, then you have to test for (at least) a million hours’ [355].
 This has beenone of the main arguments against the development of complex, critical systemsthat can’t be fully tested before use, such as President Reagan’s ‘Star Wars’ballistic missile defence program.
The reason for the k/t behaviour emerged in [249] and was proved undermore general assumptions by observing that the Maxwell-Boltzmann statisticsdeveloped to model ideal gases apply to statistically independent bugs too [312].
This model gives a number of other interesting results.
 If you can assume thatthe bugs are statistically independent, then the k/t reliability growth is the bestpossible: the rule that you need a million hours of testing to get a million hoursMTBF is inescapable, up to some constant multiple which depends on the initialSecurity Engineering930Ross Anderson28.
3.
 METRICS AND DYNAMICS OF DEPENDABILITYquality of the code and the scope of the testing.
 This can be seen as a versionof ‘Murphy’s Law’: that the number of defects which survive a selection processis maximised.
These statistics give a neat link between evolutionary models of software andthe evolution of a biological species under selective pressure, where the ‘bugs’are genes that reduce ﬁtness.
 Just as software testing removes the minimumpossible number of bugs consistent with the tests applied, biological evolutionenables a species to adapt to a changed environment at a minimum cost inearly deaths while preserving as much diversity as possible to help the speciessurvive future environmental shocks.
 For example, if a population of rabbitsis preyed on by snakes, they will be selected for alertness rather than speed.
Their variability in speed will remain, so if foxes arrive in the neighbourhoodthe rabbit population’s average running speed can rise sharply under selectivepredation9.
The evolutionary model also points to fundamental limits on the reliabilitygains to be had from reusable software components such as objects or libraries;well-tested libraries simply mean that overall failure rates will be dominated bynew code.
 It also explains the safety-critical systems community’s observationthat test results are often a poor performance indicator [1175].
 The failure timemeasured by a tester depends only on the initial quality of the program, thescope of the testing and the number of tests, so it gives virtually no furtherinformation about the program’s likely performance in another environment.
There are also some results that are unexpected, but obvious in retrospect: forexample, each bug’s contribution to the overall failure rate is independent ofwhether the code containing it is executed frequently or rarely – intuitively,code that is executed less is also tested less.
 Finally, di↵erent testers shouldwork on a program in parallel rather than in series.
So complex systems only become reliable following prolonged testing by di-verse testers.
 This gives the advantage to tried-and-tested designs for machinery,as we gain statistical knowledge of how it fails.
 Mass-market software startedto be used at su�cient scale to enable thorough testing, especially once crashreports started to be sent to the vendor.
 The use of regression testing by devel-opment teams meant that billions of test cases can be exercised overnight witheach new build.
 Services that move to the cloud can be monitored for failureall the time.
So what are the limits to reliability?First, new bugs are introduced bythe new code in new versions dictated by platform business models, and second,adversarial action brings in a signiﬁcant asymmetry between attack and defence.
Let’s take a simpliﬁed example.
 Suppose a product such as Windows has1,000,000 bugs each with an MTBF of 1,000,000,000 hours.
 Suppose that Ahmedworks for the Iranian Revolutionary Guard to create tools to break into the USArmy’s network, while Brian is the NSA guy whose job is to stop Ahmed.
 Sohe must learn of the bugs before Ahmed does.
Ahmed has only half a dozen people, so he can only do 10,000 hours of testinga year.
 Brian has full Windows source code, dozens of PhDs, oversight of the9More formally, the fundamental theorem of natural selection says that a species with ahigh genic variance can adapt to a changing environment more quickly [695].
Security Engineering931Ross Anderson28.
3.
 METRICS AND DYNAMICS OF DEPENDABILITYcommercial evaluation labs, an inside track on CERT, an information sharingdeal with other Five Eyes member states, and also runs the government’s schemeto send round consultants to critical industries such as power and telecommsto ﬁnd out how to hack them (pardon me, to advise them how to protect theirsystems).
This all adds up to the equivalent of 100,000,000 hours a year oftesting.
After a year, Ahmed ﬁnds 10 bugs, while Brian has found 100,000.
 But theprobability that Brian has found any one of Ahmed’s bugs is only 10%, and theprobability that he’ll have found them all is negligible.
 And Brian’s bug reportswill have become such a ﬁrehose that Microsoft will have found some excuse tostop ﬁxing them.
 In other words, the attacker has thermodynamics on his side.
In real life, vulnerabilities are correlated rather than independent; if 90% ofyour vulnerabilities are stack overﬂows, and you introduce compiler technologysuch as stack canaries and ASLR to trap them, then for modelling purposes therewas perhaps only a single vulnerability.
 However, it’s taken years to sort-of-not-quite ﬁx that one, and new ones come along all the time.
 So if you are actuallyresponsible for Army security, you can’t just rely on some commercial o↵-the-shelf product you bought a few years ago.
 One way to escape the statistical trapis simplicity – which, as we saw in Chapter 9, ends up meaning policies such asmandatory access controls, architecture such as multilevel secure mail guards,and much else besides.
 The more modern approach is a learning system thatobserves what’s broken and ﬁxes it quickly.
 That in turn means vigilant networkmonitoring, breach reporting, vulnerability disclosure and rapid patching – aswe described in section 27.
5.
7.
28.
3.
2Hostile reviewWhen you really want a protection property to hold, it’s vital that the designand implementation be subjected to hostile review.
 It will be eventually, andit’s likely to be cheaper if it’s done before the system is ﬁelded.
 As we’ve seen inone case history after another, the motivation of the attacker is critical; friendlyreviews, by people who want the system to pass, are essentially useless comparedwith contributions by people who are seriously trying to break it.
 That’s thebasic reason evaluations paid for by the vendor from one of a number of compet-ing evaluators, as in the Common Criteria and ISO 27001, are fundamentallybroken.
 (Recall our discussion in section 12.
2.
6 of auditors’ chronic inability todetect fraud by the executives who hired them.
 One hedge fund manager whomade $100M from shorting Wirecard, Jim Chanos, said, “When people ask us,who were the auditors, I always say ‘Who cares?’ Almost every fraud has beenaudited by a major accounting ﬁrm.
” [30].
)To do hostile review, you can motivate attackers with either money or hon-our.
 An example of the ﬁrst was the Independent Validation and Veriﬁcation(IV&V) program used by NASA for manned space ﬂight; contractors were hiredto trawl through the code and paid a bonus for every bug they found.
 An ex-ample of the second was in the evaluation of nuclear command and control,where Sandia National Laboratories and the NSA vied to ﬁnd bugs in eachothers’ designs.
 Another was at IBM, which maintained a leading position incryptography for years by having two teams, one in New York and the other inSecurity Engineering932Ross Anderson28.
3.
 METRICS AND DYNAMICS OF DEPENDABILITYNorth Carolina, who would try to break each others’ work, like Cambridge andOxford trying to win a boat race every year.
 Yet another is Google’s ProjectZero where the company devotes real engineering e↵ort to ﬁnding vulnerabilitiesboth in products that it relies on, such as Linux, and competitor products suchas iOS, and aggressively discloses them after 90 days’ notice in order to forcethem to be ﬁxed.
 This gets over 97% of them ﬁxed [589].
Review by academics is, at its best, in this category.
 We academics winour spurs by breaking stu↵, and get the highest accolades by inventing newtypes of attack.
 We compete with each other – Cambridge against Berkeleyagainst CMU against the Weizmann.
 The established best practice, though, isto motivate hostile review with money, and speciﬁcally via bug bounty programswhere vendors o↵er big rewards for reports of vulnerabilities.
 As we noted insection 27.
5.
7 above, Apple o↵ers $1m for anyone who can hack the iOS kernelwithout requiring any clicks by the user; this is one signiﬁcant metric for iOSsecurity10.
One way to turbocharge either academic review or a bug bounty program isto open your design and implementation, so all the world can look for bugs.
28.
3.
3Free and open-source softwareShould security mechanisms be open to scrutiny? The historical consensus isthat they should be.
 The ﬁrst book in English on cryptography was writtenin 1641 by Oliver Cromwell’s cryptographer John Wilkins.
 In ‘Mercury, or theSecret and Swift Messenger’ he justiﬁed discussing cryptography with the re-mark ‘If all those useful Inventions that are liable to abuse, should thereforebe concealed, there is not any Art or Science which might be lawfully profest’.
The ﬁrst exposition of cryptographic engineering, Auguste Kerckho↵s ‘La Cryp-tographie Militaire’ in 1883, recommended that cryptographic systems shouldbe designed in such a way that they are not compromised if the opponent learnsthe technique being used: security must depend only on the key [1042].
InVictorian times, the debate also touched on whether locksmiths should discussvulnerabilities in locks; as I noted in section 13.
2.
4, one book author pointed outthat both locksmiths and burglars knew how to pick locks and it was only thecustomers who were ignorant.
 In section 15.
8 I discussed the partial opennessfound even in nuclear security.
The free and open-source software (FOSS) movement extends this philosophyof openness from the algorithms and architecture to the implementation detail.
Many security products have publicly-available source code, of which the ﬁrstwas probably the PGP email encryption program.
The Linux and FreeBSDoperating systems and the Apache web server are also open-source and arewidely relied on: Android runs on Linux, which is also dominant in the world’sdata centres, while iOS is based on FreeBSD.
Open-source software is not entirely a recent invention; in the early daysof computing, most system software vendors published their source code.
 Thisstarted to recede in the early 1980s when pressure of litigation led IBM to adopt10On this metric the most secure system on earth might be bitcoin, as anyone who couldbreak the signature mechanism could steal billions.
Security Engineering933Ross Anderson28.
3.
 METRICS AND DYNAMICS OF DEPENDABILITYan ‘object-code-only’ policy for its mainframe software, despite bitter criticismfrom its users.
 The pendulum has swung back since 2000, and IBM is one ofthe stalwarts of open source.
There are a number of strong arguments in favour of open software, and afew against.
 First, while many closed systems are developed in structured wayswith waterfall or spiral models of the initial development and later upgrades, theworld is moving towards more agile development styles, a tension described byEric Raymond as “The Cathedral and the Bazaar” in an inﬂuential 1999 bookof that name [1584].
 Second, systems are getting so complex and toolchains solong that often the bug you’re trying to bust isn’t in the code you wrote but inan operating system or even a compiler on which you rely, so you want to beable to ﬁnd bugs there quickly too, and either get them ﬁxed or contribute a ﬁxyourself.
 Third, if everyone in the world can inspect and play with the software,then bugs are more likely to be found and ﬁxed; in Raymond’s famous phrase,“To many eyes, all bugs are shallow”.
 Fourth, it may also be more di�cult toinsert backdoors into such a product (though people have been caught trying,now that an exploit can sell for seven ﬁgures).
 Finally, for all these reasons,open source is great for conﬁdence.
The proprietary software industry argues that while openness helps the de-fenders ﬁnd bugs so they can ﬁx them, it also helps the attackers ﬁnd bugs sothey can exploit them.
 There may not be enough defenders for many open prod-ucts, as the typical volunteer ﬁnds developing code more rewarding than bughunting (though bug bounties are starting to shift this).
 Second, as I noted insection 28.
3.
4, di↵erent testers ﬁnd di↵erent bugs as their test focus is di↵erent.
As volunteers will look at cool bits of code such as the crypto, smart spooks orbug-bounty hunters will look at the boring bits such as the device drivers.
 Inpractice, major vulnerabilities lurk for years.
 For example, a programming bugin PGP versions 5 and 6 allowed an attacker to add an extra escrow key withoutthe key holder’s knowledge [1700].
So will the attackers or the defenders be helped more? Under the standardmodel of reliability growth, we can show that openness helps attack and defenceequally [74].
Thus whether an open or proprietary approach works best ina given application will depend on whether and how that application departsfrom the standard assumptions, for example, of independent vulnerabilities.
 Inthe end, you have to go out and collect the data; as an example, a study ofsecurity bugs found in the OpenBSD operating system revealed that these bugswere signiﬁcantly correlated, which suggests that openness there was a goodthing [1488].
So where is the balance of beneﬁt? Eric Raymond’s inﬂuential analysis ofthe economics of open source software [1585] suggests ﬁve criteria for whethera product would be likely to beneﬁt from an open source approach: where itis based on common engineering knowledge rather than trade secrets; whereit is sensitive to failure; where it needs peer review for veriﬁcation; where itis su�ciently business-critical that di↵erent users will cooperate in ﬁnding andremoving bugs; and where its economics include strong network e↵ects.
 Securitypasses all these tests.
The law-and-economics scholar Peter Swire has explained why governmentsSecurity Engineering934Ross Anderson28.
3.
 METRICS AND DYNAMICS OF DEPENDABILITYare intrinsically less likely to embrace disclosure: although competitive forcesdrove even Microsoft to open up a lot of its software for interoperability andtrust reasons, government agencies play di↵erent games, such as expanding theirbudgets and avoiding embarrassment [1853].
 Yet even there, the security argu-ments have started to prevail: from tentative beginnings in about 1999, the USDepartment of Defense has started to embrace open source, notably throughthe SELinux project I discussed in section 9.
5.
2.
So while an open design is neither necessary nor su�cient, it is often goingto be helpful.
 The important ﬁrst-order questions are how much e↵ort was ex-pended by capable people in checking and testing what you built – and whetherthey tell you everything they ﬁnd.
 The prudent thing to do here is to have agenerous bug-bounty program.
 And there’s a second-order question of grow-ing importance: if your business depends on Linux, shouldn’t at least a coupleof your engineers be engaged its its developer community, so you know what’sgoing on?28.
3.
4Process assuranceIn recent years less emphasis has come to be placed on assurance measuresfocused on the product, such as testing, and more on process measures such aswho developed it and how.
 As anyone who’s done system development knows,some programmers produce code with an order of magnitude fewer bugs thanothers.
 There are also some organizations that produce much better code thanothers.
 Capable ﬁrms try to hire good people, while good people prefer to workfor ﬁrms that value them and that hire kindred spirits.
While some of the di↵erences between high-quality and low-quality devel-opers are down to talent, many are conditioned by work culture.
 In my ownexperience, some IT departments are slow and bureaucratic while others arelively.
Leadership matters; just as replacing Boeing’s engineering leadershipwith money men contributed to the 737Max disaster, I’ve seen an IT depart-ment’s morale collapse when its CIO was replaced by a bureaucrat.
 Anotherproblem is that engineer quality has a tendency to decline over time.
 One fac-tor is glamour: a lot of bright graduates want to work for startups rather thanthe big tech ﬁrms, or for racy ﬁntechs and hedge funds rather than boring oldmoney-centre banks.
 Another is demographics: the Microsoft of the early 1990swas full of young engineers working long hours, but a decade later many hadcashed their stock options and left, while the rest had mostly acquired familiesand worked o�ce hours.
 Once a company stops growing, promotion is slow;there was a saying in IBM that ‘The only people who ever left were the goodones11.
’ Banks and government agencies have similar problems.
 Some ﬁrmshave tried to counter this by rating systems that require managers to ﬁre theleast productive 10% or so of their team each year, but the damage this doesto morale is dreadful; people spend their time sucking up rather than writingcode.
 Maintaining a productive work culture is one of the really hard problemsand a surprising number of big-name ﬁrms are really bad at it.
 The capabilitymaturity model, which we discussed in section 27.
5.
3, is one of the tools that canhelp good managers keep good teams together and improve them over time.
 But11As a former IBM employee, I liked that one!Security Engineering935Ross Anderson28.
3.
 METRICS AND DYNAMICS OF DEPENDABILITYon its own it’s not enough.
 The whole corporate environment matters, from thewater-cooler chat to the top leadership.
 Is the mission to do great engineering,or just to make money for Wall Street? Of course every ﬁrm pretends to havea mission, but most are bogus and the sta↵ see through them instantly.
Some old-fashioned companies swear by the ISO 9001 standard, which re-quires them to document their processes for design, development, testing, docu-mentation, audit and management control generally.
 For more detail, see [1937];a whole industry of consultants and auditors has got its snouts in this trough.
Like ISO 27001 which we discussed in section 28.
2.
9 above, it’s decorative ratherthan e↵ective.
 At best it can provide a framework for incremental process im-provement; but very often it’s an exercise in box-ticking that merely replaceschaos by more bureaucratic chaos.
Just as agile development methodologiesdisplaced waterfall approaches, so ISO 9001 is being displaced by the capabil-ity maturity model.
 What that comes down to, in assurance terms, is trustedsuppliers.
But trusted suppliers are hard to certify.
 Government certiﬁers cannot beseen to discriminate, so a program degenerates into box-ticking.
 Private certiﬁ-cation schemes have a tendency to reinforce cartels, or to race to the bottom, aswe discussed above in section 28.
2.
8.
 In both cases the consultancies and auditﬁrms industrialise the process to maximise their fee income, and we get backto where we started.
 If you are good at your job, how do you get that across?Small businesses who do high-quality work generally do better when they sell tothe most discriminating customers – to the few big players who’re smart enoughto appreciate what they do.
 In short, you usually have to be an expert yourselfto really understand who the quality providers are.
So what about the dynamics? If quality is hard to measure, and the in-centives for quality are mixed, and improving quality is hard, then what canusefully be said about the assurance level of evolving products? Will they belike milk, or like wine [1488]? Will they get better with age, or go o↵?The simple answer is that you have to do real measurements.
 The qualityof a system may improve, or decline.
 It may even ﬁnd an equilibrium if therate at which new bugs are introduced by product enhancements equals therate at which old bugs are found and removed.
There are several researchcommunities measuring reliability, availability and maintainability of systemsin various applications and contexts.
 Empirically, the reliability of new systemsoften improves for a while as the more energetic bugs are found and ﬁxed, thenstays in equilibrium for a number of years, and then deteriorates as the codegets complex and more di�cult to maintain (which software engineers sometimeseven refer to as senescence).
 However, if the ﬁrms that maintain the code arestill making enough money from it, and are incentivised to care about quality,they can ﬁx this by rewriting the parts that have become too messy – a processknown as refactoring.
 In short, the real world is complicated.
 Models can takeyou only so far, and you have to study how a system behaves in actual use.
Measurement brings its own problems.
 Some vendors collect and analysemasses of data about how their products fail – examples being platform compa-nies like Microsoft, Google and Apple – but make only selected data availableto outsiders, creating a market for specialist third-party evaluators, from theSecurity Engineering936Ross Anderson28.
4.
 THE ENTANGLEMENT OF SAFETY AND SECURITYtech press to academics.
 Other ﬁrms say much less, creating an opportunity forrating ﬁrms such as Bitsight.
 The healthcare sector is notoriously cagey aboutevidence of harm to patients, whose lawyers may have to work for years to builda negligence case.
 But in applications such as medical devices, there is enoughof a public interest for regulators to intervene to increase transparency, and aswe noted in section 28.
2.
3 above, the EU recently changed the law on medicaldevice regulation to compel aftermarket surveillance.
 As most software nowa-days is in applications rather than platforms, and very often in or supportingdevices, this brings us to consider the regulation of safety.
28.
4The Entanglement of Safety and SecurityAs we discussed in 28.
2.
2 governments regulate safety for many types of devicefrom cars to railway signals and from medical devices to toys.
 As software ﬁndsits way into everything and everything gets connected to cloud services, thenature of safety regulation is changing, from simple pre-market safety testingto maintaining security and safety over a service lifetime of years during whichsoftware will be patched regularly.
 We’ve already seen how this is becomingentangled with security.
 We discussed smart grids in section 23.
8.
1, smart metersin section 14.
2 and building alarms in section 13.
3.
I believe that the increasing entanglement of safety and security is so signif-icant for our ﬁeld that since 2017 we’ve merged teaching on safety and securityfor our ﬁrst-year undergraduates, as I mentioned in section 27.
1.
 Safety is amuch more diverse subject than security.
 While security engineering is a fairlycoherent discipline, safety engineering has fragmented over time into separatedisciplines for aircraft, road vehicles, ships, medical devices, railway signals andother applications.
 We can still learn a lot from safety engineers, as I discussedin section 27.
3, and safety engineers are starting to have to learn about securitytoo.
 This will be a long process.
 Thanks to the coronavirus lockdown, theselectures are now publicly available on video [89]; I now wish I’d put my lecturesonline years ago.
What spurred us to unite security and safety teaching was some work wedid for the European Union in 2015–6 looking at what will happen to safetyregulation once computers are embedded invisibly everywhere.
 The EU is theleading safety regulator worldwide for dozens of industries, as it’s the largestmarket and cares more about safety than the US government does.
 O�cialswanted to know how this ecosystem would have to adapt to the ‘Internet ofThings’ where vulnerabilities (whether old or new) may be remotely exploited,and at scale.
 Many regulators who previously thought only in terms of safetywill have to start thinking of security as well.
The problem facing the EU in 2015 was how to modernise safety regulationacross dozens of industries from cars and planes to medical devices, railwaysignals and toys, and to introduce security regulation as appropriate.
 The reg-ulatory goals are di↵erent.
 In this book, we have discussed how security fails ina number of di↵erent sectors and the nature of the underlying market failure.
In di↵erent contexts, security regulators might want to drive up attackers’ costsand reduce their income; to reduce the cost of defence; to reduce the impact ofSecurity Engineering937Ross Anderson28.
4.
 THE ENTANGLEMENT OF SAFETY AND SECURITYsecurity failure; to enable insurers to price cyber-risks e�ciently; and to reduceboth the social cost of attacks and social vulnerability to them.
Safety regulators seem to be more straightforward.
They tend to ignorethe economic subtleties underlying each market failure and focus on injury anddeath, then on direct property damage.
 For deaths, at least, you’d think wehave decent statistics, but priorities are modulated by public concern aboutdi↵erent types of harm.
 As we’ve discussed, the public are much more alarmedat a hundred people dying all at once in a plane crash than a thousand peopledying one at a time in medical device accidents.
 However, when hackers showedthey could go in over wiﬁ and change the dose delivered by several modelsof Hospira Symbiq infusion pump to a potentially fatal level, the FDA issued asafety advisory telling hospitals to stop using it [2066].
 It did not issue advisoriesabout the 300+ models that merely su↵ered from the safety issues we discussedin section 28.
2.
3.
 When you stop to think about it, that’s rather striking.
 Asafety regulator ignores a problem that kills several thousand Americans a yearwhile panicking at a safety-plus-security issue that has so far killed nobody.
Perhaps people intuitively grasp the principle we discussed in section 27.
3.
6:that a one-in-a-million chance of a fatal accident happening by chance doesn’tgive much assurance if an opponent can engineer the combination of inputsneeded to trigger it.
The pattern continued the following year, when the FDA recalled 465,000St Jude pacemakers in the USA for a ﬁrmware update after a report that thedevice could be hacked.
 The update involves a hospital visit because of a smallrisk of device failure.
 The report itself was controversial, as it was promoted byan investment ﬁrm that had shorted St Jude’s stock [1838].
The EU already had work in progress on medical device safety and, thefollowing year, updated its Medical Device Directives to require that medicaldevice software be developed ‘in accordance with the state of the art taking intoaccount the principles of development life cycle, risk management, including in-formation security, veriﬁcation and validation’, and ‘designed and manufacturedin such a way as to protect, as far as possible, against unauthorised access thatcould hamper the device from functioning as intended’ [653].
 This text doesn’tcover all the bases but is a useful ﬁrst step; it comes into force in 2021.
28.
4.
1The electronic safety and security of carsRoad safety helped drive interest in the convergence of security and safety inthe mid-2010s, thanks to the surge of interest in self-driving cars driven byGoogle and Tesla, among others.
 Following the breakthrough in computer visionusing deep neural networks in 2012, there was rapid progress.
 The ﬁrst newsof early accidents with experimental vehicles arrived around 2015 at the sametime as the breakthrough research on adversarial machine learning I describedin section 25.
3 and the high-proﬁle hack of the Jeep Cherokee, which I describedin section 25.
2.
4.
 Autonomous cars suddenly became a hot topic, not just forstock-market investors and security researchers, but for safety.
 Could terroristshack them and drive them into crowds?Could they get the same result byprojecting deceptive images on a building? And if kids could use their phoneto hail a car home from school, could someone hack it to abduct them? AndSecurity Engineering938Ross Anderson28.
4.
 THE ENTANGLEMENT OF SAFETY AND SECURITYwhat about the ethics – if a self-driving car was about to crash and could choosebetween killing its one occupant or two pedestrians, what would it do? Whatshould it do? Let’s take the safety and assurance aspects one step at a time.
Road safety is a major success story for safety regulation.
 Following RalphNader’s book ‘Unsafe at any speed’ [1370], the US Congress created the NationalHighway Tra�c Safety Administration (NHTSA).
 It started from a belief thatcrash testing of new models would be enough, but found it needed to force therecall of vehicles that were discovered later to be unsafe12.
 The e↵ects can beseen starkly in a Consumer Reports video of a crash test between a 2009 ChevyMalibu and a 1959 Chevy Bel Air.
 The Bel Air’s passenger compartment iscrushed and the dummy driver impaled on the steering wheel; a human driverwould have been killed.
 Thanks to 50 years of progress, the passenger compart-ment of the Malibu remains intact; the front crumple zone absorbs much of theenergy, the seatbelt and airbag hold the dummy driver, and a human driverwould have walked away [472].
 I show this video to my ﬁrst-year students toemphasise that safety engineering is not just about making mistakes less likely,but also about mitigating their e↵ects.
 The decades of progress that the videoillustrates involved not just engineering, lobbying and standard setting acrossmultiple countries, but many tussles between safety campaigners and the indus-try.
 Within the industry, some carmakers tried to lead while others dragged theirheels.
 Car safety also involves driver training, laws against drink driving andexcessive driver working hours, changing social norms around such behaviour,steady improvements to road junction design and much else.
 It has grown intoa large and complex ecosystem.
 This now has to evolve as cars become smarterand more connected.
During the 2010s, cars were steadily acquiring more assistive technology,from parking assist through adaptive cruise control to automatic emergencybraking and automatic lane keeping.
 I described in section 25.
2 how compa-nies like Google and Tesla drove a research program to join these systems uptogether, giving autonomous driving.
 The assistive technology features them-selves had various bugs; I discussed the blind spots of adaptive cruise control insection 23.
4.
1.
 Some were also open to exploitation: Charlie Miller and ChrisValasek had hacked the Jeep’s park-assist feature to drive it o↵ the road.
 Com-panies that sold limited autonomous driving features, such as Tesla, experiencedaccidents that began to undermine public conﬁdence.
 I discussed some of thesecurity implications of autonomous vehicles in section 25.
2.
 We discussed theusability aspects of safety too.
 Tesla’s ‘Autopilot’ required the driver to payattention and keep a hand on the steering wheel, in order to remain in controland avoid accidents.
 But as it drove adequately much of the time, many driversdidn’t, with consequences that were occasionally both fatal and newsworthy.
Even in 2020, while the better autopilot systems can drive a car passably wellon the motorway, they can be ﬂaky on smaller roads, getting confused at round-abouts and running over grass verges.
 So how should we test their safety?Testing an anti-lock braking system (ABS) is fairly straightforward as weunderstand the physics of skidding and aquaplaning, and such systems havebeen around long enough for us to have a long accident history.
 We next hademergency brake assist (EBA), which applies full braking force if it thinks you’re12The story is told in ‘The Struggle for Auto Safety’ [1235].
Security Engineering939Ross Anderson28.
4.
 THE ENTANGLEMENT OF SAFETY AND SECURITYtrying to do an emergency stop.
 The usual algorithm is that if you move yourfoot from the accelerator to the brake in under 300ms and then apply at least2kg of force, it activates and stops the car as quickly as possible.
 This is asimple algorithm but is harder to evaluate, as it’s trying to infer the driver’sintent.
 (I once triggered mine unintentionally and thankfully there wasn’t a carclose behind me.
)A recent addition is automatic emergency braking (AEB) which is supposedto stop the car if a child or a dog runs in front of you.
 This is harder still,as you’re trying to understand everything you see on the street ahead, withcomplex processing that uses both traditional logic and machine-vision systemsbased on deep neural networks.
 As we discussed in section 25.
2, the currentproducts are both limited and of variable quality.
 Add lane keeping assist andadaptive cruise control, and your car can pretty well drive itself on the freeway.
But how should you test that? And if we ever move to full autonomy, your riskand threat analysis must include a lot of the bad things that happen in humansocieties.
Tesla says in defence of its Autopilot feature that its cars are safer thanothers; of the 135 fatalities in crashes involving its vehicles up to June 23 2020,only 10 were attributed to Autopilot [1870].
 The actual ﬁgures are controversial,though.
 An insurance forensics company brought a lawsuit against NHTSA toget the raw ﬁgures for accidents up till June 2016, studied them, and claimedthat the analysis o↵ered by Tesla and accepted by NHTSA had considered only13% of the data.
 Rather than a 40% decrease in airbag deployments after theAutosteer feature of the vehicle was activated, as Tesla had claimed, the fulldata showed a 57% increase from 0.
76 deployments per million miles of travelto 1.
21 [1565].
The insurance industry accumulates good data over time across all car mak-ers and worries about the cost of claims.
 It was concerned at AEB, worryingthat if cars brake hard when a rabbit runs in front of them, there might bemore rear-end collisions.
 But once the data started to arrive in 2016, insurersrelaxed.
 When I check online how much it would cost me to insure a Tesla withAutopilot versus a plug-in hybrid Mercedes of similar value, I get about thesame answer (though more insurers bid for the Mercedes).
But actuarial costs are not the only driver of public policy.
 Politicians startedto worry about truck drivers’ jobs.
 Philosophers started to worry about ethics:given a choice between killing a pedestrian and the driver, would an autopilotprotect its driver? The industry worried about updates.
 Progress in machinevision is so rapid that you can imagine having to sell a whole new vision unitevery ﬁve years, as the systems we have now won’t run on the hardware of ﬁveyears ago.
 Would the customers stand for having to pay several thousand Eurosevery few years for a new autopilot?People also worry more about security threats, as we have evolved to besensitive to adversarial activity.
By 2020, we have a ﬂurry of security stan-dardisation, including the draft ISO 21434 standard on cybersecurity, which Imentioned in section 27.
3.
5; proposed amendments to the regulations of theUNECE13 to deal with cybersecurity and software updates for connected vehi-13The UN Economic Commission for Europe was established by a 1958 treaty.
 It includesSecurity Engineering940Ross Anderson28.
4.
 THE ENTANGLEMENT OF SAFETY AND SECURITYcles [1921]; and in Japan, following cyber attacks on Toyota and Honda, baselinerequirements for the whole car industry supply chain [1243].
 That’s all great,but the target is moving faster all the time.
In Brussels, o�cials started to worry about how the regulatory ecosystemcould cope.
 Over 20 agencies are involved one way or another in vehicle safety(unlike in the USA, where NHTSA covers everything from car design to speedlimits).
 Would each agency have to hire a security engineer? Some of them don’thave any engineers at all, just lawyers and economists.
 How should the ecosys-tem evolve to cope? O�cials were suddenly less willing to trust the industry’sassurances after the Dieselgate emissions scandal in 2015, when it turned outthat Volkswagen had installed software in its cars to cheat on emissions tests.
The Volkswagen and Audi CEOs lost their jobs and face criminal charges, alongwith about a dozen other executives; the companies paid billions in legal settle-ments.
 The threat model was no longer just the external hacker, but includedthe vendors themselves.
 Regulators wanted to get back in control.
 What didthey need to do?28.
4.
2Modernising safety and security regulationOur brief was to consider the policy problem generally across all sectors.
 Itwas clear that European institutions needed cybersecurity expertise to supportsafety, privacy, consumer protection and competition.
But what would thismean in practice? In order to ﬂesh this out, ´Eireann Leverett, Richard Claytonand I studied three industries of which we had some knowledge: medical devices,cars and electricity distribution.
 Our full report [157] was presented in 2016 andpublished the following year, along with a summary version for academic audi-ences [1148].
 The full report has an extensive analysis of the existing patchworkof safety / security standards for embedded devices from ISO, IEC, NIST andothers.
This exercise taught us a huge amount about subjects we didn’t expect wouldbe on the agenda.
 Usability is critical in a number of ways.
 The dominant safetyparadigm used to be to analyse how limited or erratic human performance coulddegrade an otherwise well-designed system, and then work out how to mitigatethe consequences.
 Some countries demand that drivers over 67 get a medicalor re-sit their driving test, as well as insisting on seat belts and airbags.
 Insecurity, malice comes into the equation: you worry about the widow in hereighties who’s called up and persuaded to install an ‘upgrade’ on her PC.
 Carsecurity is not just about whether a terrorist can take over your car remotelyand drive it into some pedestrians.
 If a child can use her mobile phone to directa car to take her to school, what new threats do we have to worry about? Mightshe be abducted, whether by a stranger or (more likely) in a custody dispute?And whose engineers need to worry about her safety – the car company’s, theride-hailing company’s, or the government’s?The security engineer’s task is to enable even vulnerable users to enjoy rea-sonable protection against a capable motivated opponent.
 How do you embedthe car-making countries in Europe and Africa plus Japan, Korea and Australasia and ise↵ectively one of three standardisation zones for cars, the others being the Americas andChina.
Security Engineering941Ross Anderson28.
4.
 THE ENTANGLEMENT OF SAFETY AND SECURITYgood practice in industries that have never had to think of distant adversariesbefore?That’s not just a matter of setting minimum standards but also ofembedding security thinking into standards bodies, regulatory agencies, testingfacilities and many other places in the ecosystem.
 That will be a long and ardu-ous process, just as car safety was.
 Getting test engineers who work by checkingcarefully whether the ‘British standard ﬁnger’ can be accidentally poked intoan electrical appliance to think in terms of creative malice instead will be hard.
Where do we start?We came up with a number of recommendations.
 Some were considered bythe Commission to be in the ‘too hard’ category, including extending productliability law to services, and requiring the reporting of breaches and vulnerabili-ties not just to security agencies and privacy regulators but to other stakeholderstoo.
 Eventually we’ll need laws regulating the use of car data in investigatingaccidents, particularly if there are disputes over liability when car autopilotscause fatal crashes.
 (At present the vendors hold the data close and it takesvigorous litigation to get hold of it.
) Without data we won’t be able to build alearning system.
One of our recommendations was that vendors should have to self-certify, fortheir CE mark, that products can be patched if need be.
 This looks set to bepartly achieved by means of a technical standard, ETSI EN 303 645 V2.
1 [646],as I discussed in section 28.
2.
9 above.
 ETSI is a membership organisation ofsome 800 ﬁrms; it can move more quickly than governments but still has someclout; for example, it set up the standards bodies for mobile telephony.
 Failureto comply with an ETSI standard does not however empower a customs o�cerin Rotterdam to send a container of toys back to China.
 For that, we need toendow standards with the force of law.
28.
4.
3The Cybersecurity Act 2019Another recommendation was that Europe should create a European SecurityEngineering Agency to support policymakers.
 Europe already had the EuropeanNetwork and Information Security Agency (ENISA) which coordinated securitybreach reporting among EU government agencies, but it had been exiled toCrete as a result of lobbying by the UK and French intelligence agencies, whodid not want a peer competitor among the European institutions.
 The Brexitvote shifted the politics and made it feasible for ENISA to open a proper Brusselso�ce so it could take on the security engineering advisory role.
The Cybersecurity Act 2019 formalised this [655].
It empowered ENISAto be the central agency for regulating security standards, as we described insection 28.
2.
9, and also to be the main agency for cybersecurity advice to otherEuropean bodies.
 It is to be hoped that ENISA will build its competence andclout over time, and see to it that new safety standards pay appropriate attentionto security too, including at a minimum an appropriate development lifecycle(which was another of our recommendations).
For a security technology to really work, functionality isn’t enough, and thesame goes for testing and even incentives for learning.
 The right people have totrust it and it has to become embedded in social and organisational processes,Security Engineering942Ross Anderson28.
5.
 SUSTAINABILITYwhich means alignment with wider systems and stable persistence over a longenough period of time.
The implication is that regulators should shift fromthe testing of products to the assurance of whole systems (this was our ﬁnalrecommendation).
28.
5SustainabilityThe problem our report identiﬁed as the most serious in the long term was thatproducts are becoming much less static.
 As security and safety vulnerabilitiesare patched, regulators will have to deal with a moving target.
Automobilemechanisms will need security testing as well as safety testing, and also meansof dealing with updates.
 As we saw from the Volkswagen debacle, many legacymanufacturers haven’t caught up with coordinated disclosure.
Most two-year old phones don’t get patched because the OEM and the mobilenetwork operator can’t get their act together.
 So how on earth are we going topatch a 25-year-old Land Rover that spent 10 years in the Danish countrysideand was then exported to Romania? This kicked o↵ a political ﬁght, as the carindustry did not want to be liable for software patching for more than six years.
(The typical European car dealer will sell you a 3-year lease on a new car ifyou’re rich, and on an approved used car if you’re not quite so rich.
) However,the embedded carbon cost of a new car – the amount of CO2 emitted duringits manufacture – is about equal to its lifetime fuel burn.
 And it’s predictablethat, sooner or later, a car whose software isn’t up-to-date won’t be allowed onthe roads.
 At present, the average age of a car at scrappage is about 15 years;if that were reduced to six, the environmental cost would be unacceptable.
 Wewould not even save CO2 by moving from internal combustion engines to electricvehicles, because of the higher embedded carbon cost of electric vehicles; thewhole energy transition is based on the assumption that they will last at leastas long as the 150,000km average of our legacy ﬂeet [614].
We found a very ready audience in European institutions.
A number ofother stakeholders had been complaining about the e↵ects of software on thedurability of consumer goods, with updates available only for a short periodof time or not at all.
 Right-to-repair activists were campaigning for consumerelectronic devices to be reusable in a circular economy, annoyed that tech ﬁrmstry to prevent repair using ‘security’ mechanisms, or even abuse them in anattempt to make repair illegal.
 The self-regulation of the IoT market has beenlargely unsuccessful, thanks to a complex interplay of economic incentives andconsumer expectations [1954].
 Consumer-rights organisations were starting towarn of the shockingly short lifespan of smart devices: you could spend extra ona ‘smart fridge’ only to ﬁnd that it turned into a frosty brick a year later whenthe vendor stopped maintaining the server [933].
Planned obsolescence wasalready a hot political topic as green parties increased their vote share acrossEurope.
 Lightbulbs used to last longer; the bicentennial light has been burningat Livermore since 1901.
 In 1924 a cartel of GE, Osram and Philips agreedto reduce average bulb lifetimes from 2500h to 1000h, and this behaviour hasbeen followed by many industries since.
 Governments have pushed back; Francemade it illegal to shorten product life in 2015, and after Apple admitted in 2017Security Engineering943Ross Anderson28.
5.
 SUSTAINABILITYthat it had used a software update to slow down older iPhones, prompting usersto buy newer ones, it was prosecuted.
 In 2020 it received the highest-ever ﬁne,e1.
2B, for anti-competitive practices, although this also related to its treatmentof its French distributors [1193].
 (It settled a US class action for $500m [966].
)Security agencies were already warning us about the risks of the ‘Internetof Things’, including network-connected devices with default passwords andunpatchable software.
 In fact, I learned of the Mirai botnet taking down Twitteras I was on the Eurostar train back to London from giving the ﬁrst presentationof our work, to an audience of about 100 security and IT policy people inBrussels.
 We soon found out that it exploited Xiaomi CCTV cameras that haddefault passwords and whose software could not be patched.
 It was a perfectillustration of the need for action.
Over the ensuing three years there was more than one initiative to try tocreate a legal means to push back on tech companies that failed like Xiaomito support their products by patching vulnerabilities (or even making patchingpossible).
 The tech lobbyists blocked the ﬁrst couple of attempts, but eventu-ally in 2019 the European Parliament updated consumer law to cover softwaremaintenance.
28.
5.
1The Sales of Goods DirectiveThis Directive passed the European Parliament in May 2019 [656] and will takee↵ect from 2021.
 Thereafter, ﬁrms selling goods ‘with digital elements’ mustmaintain those elements for a reasonable service life.
 The wording is designedto cover software in the goods themselves, online services to which the goodsare connected, and apps which may communicate with the goods either via theservices or directly.
 They must be maintained for a minimum of two years aftersale, and for a longer period if that is a reasonable expectation of the customer.
What might that mean in practice?Existing regulations require vendors of durables such as cars and washingmachines to keep supplying spares for at least ten years, so we can hope thatthe new regulatory regime will require at least as long.
 Indeed, the preamble tothe Directive notes that “A consumer would normally expect to receive updatesfor at least as long as the period during which the seller is liable for a lack ofconformity, while in some cases the consumer’s reasonable expectation couldextend beyond that period, as might be the case particularly with regard tosecurity updates.
” Given that in many countries cars have to pass an annualroadworthiness test to remain in use, and that such a test is likely to includea check that software is patched up to date in the foreseeable future, we couldwell see a requirement for security patches to extend beyond ten years.
No doubt there will be all sorts of arguments as the lobbyists try to cut thecosts of this, but it’s a huge step in the right direction.
 American practice oftenfollows Europe on safety matters.
Security Engineering944Ross Anderson28.
5.
 SUSTAINABILITY28.
5.
2New research directionsNow that there is not just a clear social need for long-term maintenance of thesafety and security of software in durable goods, but a clear legal mandate, I urgemy fellow computer scientists to adopt this as a grand challenge for research.
Since the 1960s we have come to see computers almost as consumables,thanks to Moore’s law.
 This has conditioned our thinking from the lowest levelof technical detail up to the highest levels of policy.
We’ve crammed thou-sands, and then millions, more transistors into chips to support more elaboratepipelining and caching.
 We’ve put up with slow and ine�cient software in theknowledge that next year’s PC will run it faster.
 We’ve shrugged o↵ monop-olies, believing that the tech ten years from now will be quite di↵erent fromtoday’s, so we can replace competition in the market with competition for themarket.
 We’ve been like a cruise ship, happily throwing the trash overboard inthe expectation that we’ll leave it far behind us.
Moore’s law is now running out of steam.
 The analysis of CPU performanceby Hennessy and Paterson shows that while this grew by 25% per annum from1978 to 1986 and a whopping 52% from 1986 to 2003, it slowed to 23% in2003–11, 12% in 2013–15 and 3.
5% after that [882].
 As the party winds down,we’ll have to start clearing up the trash.
 That extends from the side-channelattacks like Spectre that were caused by the 12-stage CPU pipelines, throughthe technical debt accumulated in our bloatware, right up to the monopolisticbusiness ecosystem that drives it all.
There is much, much more.
 The root certiﬁcates of a number of popularCAs are starting to expire, and if these are embedded in devices such as TVswhose software can’t be upgraded, then the devices are essentially bricked [117].
(The most popular, Letsencrypt, rolls over in 2021.
) When CA root certs expireyou have to update clients, not servers, to ﬁx them.
 In consumer devices, thetrend is towards shorter lifetimes, to make crypto updateable; as I discussed insection 21.
6, browsers such as Safari and Chrome are starting to enforce 398-daycertiﬁcate expiry, and that’s another strong incentive for frequent updates.
There are many environments with long-lived equipment where updatesaren’t usual, from petrochemical plants to electricity substations.
 Systems inbuildings and civil engineering projects are somewhat of a hybrid; some vendorsare working on versions of Linux that are expected to be as stable as possibleand maintained for 25 years, while others are pushing for more aggressive reg-ular updating of whole systems and telling us to ‘put everything in the cloud’.
This latter approach is associated with the ‘smart buildings’ meme, but hasits own drawbacks.
 Once multiple contractors and subcontractors need onlineaccess to systems that contain full engineering information on buildings – fromthe electricity substations through the air-conditioning to the ﬁre and burglaralarms – there are obvious risks.
 Some of these contractors operate at interna-tional scale, so a subverted employee or rooted machine there may have accessto the critical national infrastructure of dozens of countries.
 Are we comfortablewith that?Adapting to the new normal will take years, as it will require behaviourchange by millions of stakeholders.
 I suspect that the tensions created by thisSecurity Engineering945Ross Anderson28.
5.
 SUSTAINABILITYadaption will become signiﬁcant in policy, entrepreneurship and research overthe next decade.
So what might sustainable security research look like? As a ﬁrst pilot project,Laurent Simon, David Chisnall and I tackled the maintenance of cryptographysoftware.
As I mentioned in section 19.
4.
1, TLS was proven secure twentyyears ago but there’s been about one attack a year on it since, mostly viaside channels.
 One of the problems is that the crypto implementation, such asOpenSSL, typically has code designed to perform cryptographic operations inconstant time, so that the key in use won’t leak to an outside observer, and alsoto zeroise memory locations containing key material or other sensitive data,so that the key can’t be deduced by other users of the same machine either.
But every so often, somebody improves a compiler so that it now understandsthat certain instructions don’t do any real work.
 It optimises them away, andall of a sudden millions of machines have insecure crypto software.
This isextremely annoying; you’re out there ﬁghting the bad guys and all of a suddenyour compiler writer stabs you in the back, like a subversive ﬁfth column in yourrear.
 Our toolsmiths should be our allies rather than our enemies, and so weworked out what would be needed to ﬁx this properly.
 Languages like C have noway of expressing programmer intent, so we ﬁgured out how to do this by meansof code annotations.
 Getting a compiler to do constant-time code and secureobject deletion properly turns out to be surprisingly tricky, but we eventuallygot a working proof of concept in the form of plugins for LLVM [1758].
Much, much more will be needed.
 Moving from the low level of compilerinternals to the medium level of safety systems, a big challenge facing the carindustry is getting accident data to the stakeholders who can learn from it.
 InEurope, some ﬁfty thousand people die in road tra�c accidents each year, andanother half a million are injured.
 Worldwide, there are something like a milliondeaths a year.
 As cars are starting to log both control inputs and sensor data,there are many megabytes of data about a typical accident, but at present theseare mostly not analysed.
 Increasingly, the data are on the vendors’ servers aswell as in the damaged vehicles.
 But when the police investigate major roadaccidents, they do not at present have access to much information from datarecorders or to most of the 100-million-plus lines of software in the vehicle –some of which will be from subsidiary suppliers, and of uncertain provenance,version and patch status.
 Where there is a closely-fought lawsuit, data may bedemanded, but vendors are reluctant to share it and it typically takes a courtorder.
What should happen? We should aim at a learning system.
 We keep hear-ing reports of people getting killed by an autonomous car in a stupid accident– as when an Uber killed Elaine Herzberg in Tempe, Arizona because she waspushing a bike on the road and its software detected pedestrians only on ornear a crosswalk [1264].
 We should expect to be able to push an update to stopthat happening again.
 So what would the patch cycle look like? In aviation,accidents are monitored resulting in feedback not just to operators such as pi-lots and air tra�c controllers but to the designers of aircraft and supportingground systems.
 Work is starting on systems for monitoring accidents involvingmedical devices, though the vendors may well drag their feet.
 There, too, thekey is mandatory systems for monitoring adverse events and collecting data.
Security Engineering946Ross Anderson28.
6.
 SUMMARYAt present, we ﬁx road junctions once there have been several accidents there;that’s all the ‘patch cycle’ we have at present, because the only data availableto the highways department is the location and severity of each accident, plusperhaps a couple of sentences in the report from the attending o�cer.
 A learn-ing system for cars too is inevitable as vehicles become more autonomous, butthey won’t learn on their own.
Learning will involve analysing the causes of failures, accumulating engineer-ing knowledge, and ultimately politics involving multiple stakeholder groups.
For starters, we’ll need the ﬁne-grained data from what the cars sensed, whatthey decided to do, and why.
 The task of writing the laws to get these datafrom vendors to accident investigators, insurance assessors and other stakehold-ers lies ahead.
 At present, EU Member States are responsible for post-marketsurveillance of vehicle standards, so very little gets done, and there have beenproposals to give the European Commission a surveillance power in the wake ofDieselgate.
 Then there will be the task of actually building these systems.
 Theywill be large and complex, because of the need to deal with multiple conﬂictingrights around safety, privacy and jurisdiction.
Moving still further up the stack to the level of policy, there’s a growingconsensus that tech needs to be better regulated.
 We could perhaps tolerate thevarious harms to privacy and competition while the technology was changingrapidly.
 If you didn’t like the IBM monopoly in the 1980s you just had to waituntil Microsoft came along; and by the time Microsoft had become the ‘evilempire’ in the late 1990s, Larry and Sergey were starting Google.
 Was Google+too clunky for you? No matter, try Facebook or Twitter.
 But as Moore’s lawruns out of steam, the dominant ﬁrms we have now may remain dominant forsome time – just like the railways dominated the second half of the nineteenthcentury and the ﬁrst third of the twentieth.
 And there are many other sectorswhere technology has enabled some players to lock in market dominance; as Iwrite in 2020, Amazon is the world’s most valuable company.
 We need to refreshour thinking on antitrust law.
 There are some signs that this is happening [1044].
What would you hope the law to look like twenty years from now? How shouldthe safety, security and antitrust pieces ﬁt together?28.
6SummaryIn the old days, the big question in a security engineering project was how youknow when you’re done.
 All sorts of evaluation and assurance methodologieswere devised to help.
 Now the world is di↵erent.
 We’re never done, and nobodywho says they are done should be trusted.
Security evaluation and assurance schemes grew up in a number of di↵erentecosystems.
 The US military spawned the original Orange Book, and inspiredboth the FIPS 140 standards for cryptographic modules and the Common Cri-teria, both of which attempted to spread the gospel of trustworthy systems tobusinesses and to other countries.
 Safety certiﬁcation schemes evolved sepa-rately in a number of industries – healthcare, aerospace and road vehicles toname just three.
 Vendors game these systems all the time, and work to capturethe regulators where this is possible.
 Now that everything’s acquiring connec-Security Engineering947Ross Anderson28.
6.
 SUMMARYtivity, you can’t have safety without security, and these ecosystems are merging.
In both safety and security, the emphasis will move from pre-market testingto monitoring and response, which will include updating both devices alreadyin the ﬁeld and the services that support them.
 This will move beyond softwarelifecycle standards towards the goal of a learning system that can recover quicklyeven from novel hazards and attacks.
Things are improving, slowly.
 Back in the 20th century, many vendors nevergot information security right.
 By 2010, the better ones were getting it moreor less right at the third or fourth attempt.
 In the future, everyone will beexpected to ﬁx their products reasonably promptly when they break, and to doso for a reasonable period of time.
But the cost of all this, the entanglement of security with safety in all sortsof devices and services, and their interaction with issues from discriminationto globalisation and trade conﬂict, will make these issues increasingly the stu↵of global politics.
 The safety and security costs inﬂicted on us by tech, in itsbroadest sense, will be in increasing tension with national ideas of sovereigntyand, at a more practical level, people’s ability to achieve by collective actionthose goals that cannot be achieved through individual action or market forces.
Just as security economics was a hot topic in the 2000s and security psychologyin the 2010s, I expect that the politics of security will be a growth topic in the2020s and beyond.
Research problemsIn addition to the grand challenge of sustainable security I discuss in sec-tion 28.
5.
2 above, there are many other open problems around assurance.
 Wereally don’t know how to do assurance in complex ecosystems such as wherecars talk to online services and mobile phone apps.
 A second bundle of prob-lems comes from the fact that as the worlds of safety and security are slowlycoming together, like a couple of galaxies slowly merging, we ﬁnd that safetyengineers and security engineers don’t speak each others’ languages, have incom-patible sets of standards and even incompatible approaches to standardisation.
Working this out in one industry after another will take years.
Another big opportunity may be for lightweight mechanisms to improve realdeployed systems.
 Too many researchers take the view that ‘If it’s not perfect,it’s no good.
’ We have large communities of academics writing papers aboutprovable security, formal methods and about obscure attacks that aren’t foundin the wild because they don’t scale.
 We have large numbers of real problemsarising from companies corner-cutting on development.
If programmers aregoing to steal as much code as they can from stackexchange, do we need apublic-interest e↵ort to clean up the examples there to get rid of the bu↵eroverﬂows? And do we have any chance of setting security usability standardsfor tools such as crypto libraries and device permissions, so that (for example)libraries that default to ECB would be forcibly retired, just like MD5 and SHA1?Yet another is likely to be the testing of AI/ML systems, both before deploy-ment and for continuous assessment.
 We already know, for example, that deepSecurity Engineering948Ross Anderson28.
6.
 SUMMARYneural networks and other ML mechanisms inhale prejudice along with theirtraining data; because machine-vision systems are mostly trained on photos ofwhite people, they are uniformly worse at spotting people with darker skin,leading to the concern that autonomous vehicles could be more likely to killblack pedestrians [2026].
 What will a learning system look like when it touchesa contentious social issue? How do you do continuous safety in a world wherenot all lives are valued equally? How do we ensure that the security, privacyand safety engineering decisions that ﬁrms take are open to public scrutiny andlegal challenge?Further ReadingThere’s a whole industry devoted to promoting the security and safety assurancebusiness, supported by mountains of your tax dollars.
 Their enthusiasm caneven have the ﬂavour of religion.
 Unfortunately, there are nowhere near enoughpeople writing heresy.
Security Engineering949Ross Anderson