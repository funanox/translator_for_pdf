Chapter 20Advanced CryptographicEngineeringGive me a rock on which to stand, and I will move the world.
– ArchimedesWhoever thinks his problem can be solved using cryptography,doesn’t understand his problem and doesn’t understandcryptography– Attributed by Roger Needham and Butler Lampson to each other20.
1IntroductionCryptography is often used to build a trustworthy component on which morecomplex designs can rely.
 Such designs come from three rather di↵erent back-grounds.
 The ﬁrst is the government systems world we described in Chapter9, where the philosophy is to minimise the trusted computing base using mech-anisms like data diodes and multilevel secure encryption devices.
 The secondis the world of banking described in Chapter 12 where smartcards are usedas authentication tokens while HSMs are used to protect PINs and keys.
 Thethird is the world of cryptography research in the 1980s and 1990s where peopledreamed of solving social problems using mathematics: of creating anonymouscommunications so that oppressed groups could evade state surveillance, lead-ing to censorship-resistant publishing, untraceable digital cash and electronicelections that would be impossible to rig.
 In all these cases, real life turned outto be somewhat messier than we anticipated.
There are even more complex cryptographic components that we use asplatforms.
 But the engineering isn’t just about reducing the attack surface, orsimplifying our fault tree analysis.
 In most cases there’s a signiﬁcant interactionwith policy, liability and other complicating factors.
In this chapter I’m going to discuss six examples of cryptographic engineering60620.
2.
 FULL-DISK ENCRYPTION– full disk encryption, the Signal protocol, Tor, hardware security modules,enclaves and blockchains.
 The ﬁrst is a simple example to set the scene; the otherﬁve use crypto in more complex ways to support a wide range of applications,including payments in the case of the last three.
 All but HSMs are used bycybercriminals.
Hard disk encryption has been around since the 1980s and is one of thesimplest security products, at least conceptually.
 By encrypting the data onyour hard disk when the machine’s in use, you ensure that a thief can only stealthe hardware, not the data.
Signal is a protocol for secure messaging between phones.
 It is perhaps thenext level up in complexity and is about enabling people to manage a socialnetwork as securely as possible in the face of equipment compromise.
 Signaldoes private contact discovery by means of enclaves.
Tor takes this to the next level by providing anonymity, when you don’t wantsomeone observing your tra�c to know who you’re talking to or which websitesyou’re visiting.
HSMs have provided a trust platform for payment services since the 1980s.
But the crypto apps that run on them can su↵er from attacks on their applicationprogramming interfaces that are so deeply entangled with payment applicationsthat they are very hard to ﬁx.
Enclaves are an attempt by CPU vendors to provide a general purpose cryptoplatform: we’ve had Arm’s TrustZone since 2004 and Intel’s SGX since 2015.
They are starting to replace HSMs in payment applications, and also supportprivate contact discovery in Signal.
 But they have been plagued with problemsfrom side-channel attacks to class breaks.
 For example, if you can extract themaster secret key from an SGX chip, you can break the whole ecosystem.
Finally, for a quite di↵erent kind of trusted computer, we look at Bitcoin.
This is a project, since 2009, to create a digital currency based on a shared ledgerthat emerges using cryptographic mechanisms from the cooperation of mutuallymistrustful parties.
 Many of the stakeholders are far from trustworthy, and thereare dominant players at several levels in the technology stack.
 Yet a trustedcomputer has somehow emerged, thanks to a combination of cryptography andeconomic incentives, and has kept going despite the huge amounts of moneythat could be taken in a successful attack.
It may be useful to bring together in one chapter the trusted platformsof both bankers and gangsters, so we can contrast them.
 Some striking factsemerge.
 For example, the best attempts of the top technology companies toproduce trusted computers have produced ﬂawed products, while the gangstersseem to have created something that works – at least for now.
20.
2Full-disk encryptionThe idea behind full-disk encryption (FDE) is simple.
You encrypt data asit’s written to disk, and do decryption as it’s read again.
The key dependson an initial authentication step such as a password, which is forgotten whenSecurity Engineering607Ross Anderson20.
2.
 FULL-DISK ENCRYPTIONthe machine sleeps or is switched o↵.
 So if a doctor leaves their laptop on atrain, only the hardware is lost; the medical records are not.
 FDE has becomea regulatory requirement in many industries.
In Europe, privacy regulatorsgenerally see the loss of machines with FDE as not serious enough to attracta ﬁne or to need mandatory notiﬁcation of data subjects.
 Many phones andlaptops come with FDE; with some it’s enabled by default (Android) while withothers it just takes a click (Mac).
Scratch a little under the surface, though, and there’s a wide variance inquality.
 From the early days of hard disks in 1980s, software FDE productswere available but imposed a performance penalty, while hardware productscost more and were export-controlled.
The engineering isn’t trivial, as youneed a platform on which to run the initial authentication step.
 Early productso↵ered an extra encrypted volume but did not protect the host operating systemand could be defeated by malware.
 The initial authentication is tricky in otherways.
 If you derive the disk key from a user password, then a thief can tryzillions of them o✏ine, as we discussed in 3.
4.
4.
1, and guess anything a normaluser sets up.
 A hardware TPM chip can limit password guessing, and from2007 this became available for Windows with Bitlocker.
 Integrating FDE into aplatform enables the vendor to design coherent mechanisms for trusted boot ofan authentic copy of the operating system, setting up and managing recoverykeys, and coping with quite complex interactions with software upgrade, swapspace, device repairs, the backup and recovery of user data, and factory resetwhen the device is sold.
Third-party o↵erings started to o↵er some extra features: TrueCrypt, forexample, o↵ered a steganographic ﬁle system where the very existence of a diskvolume would remain hidden unless the user knew the right password [114]1.
A crypto phone sold to criminals, EncroChat, had a whole hidden partitioncontaining encrypted chat and VOIP apps; I’ll discuss such products in moredetail in section 25.
4.
1.
 However most people now use the FDE facility providedby the vendor of their phone or laptop, as proper integration involves quite a lotof the platform.
 Since 2010 we’ve had a special mode of operation, XTS-AES,designed for FDE; it encrypts each block salted with the sector number, and hasa mechanism to ﬁt disc blocks to block ciphers.
 O↵erings such as Microsoft’sBitLocker and Apple’s FileVault have an overhead of only a few percent, whenrun on CPUs with AES support.
Yet attacks continue.
 In 2008, Alex Halderman and colleagues at Princetoncame up with cold boot attacks, which defeated the principal FDE products thenon the market and can still present a problem for many machines [854].
 As Idescribed in section 18.
3, you freeze a computer’s DRAM in which the transientencryption key is stored, then reboot the device with a lightweight operatingsystem and acquire a memory image, from which the key can be read.
 In 2015,we found that most Androids were insecure: the factory reset function was sobadly engineered by most OEMs that credentials, including FDE keys, could berecovered from second-hand devices [1757].
 And most Android phones don’t get1That product was suddenly discontinued and its anonymous developers recommendedthat users migrate to other products because of an unspeciﬁed vulnerability; some suspectthat this was a ‘warrant canary’, a pre-planned warning message whose transmission thedevelopers suppress by certifying regularly that they are not subject to coercion, but whichﬁres o↵ a warning once they’re served with a subpoena or warrant [61].
Security Engineering608Ross Anderson20.
3.
 SIGNALpatched once they’re no longer on sale.
 And in 2019, Carlo Meijer and Bernardvan Gastel found that the three third-party FDE products that held 60% of themarket were insecure, that open-source software encryption would have beenbetter, and that Bitlocker turned itself o↵ if one of these hardware productsappeared to be present; thanks to their work, it no longer does so [1285].
 Andthen there’s the collateral damage.
 Now that lots of sensitive data are kept noton hard disks but in Amazon S3 buckets, auditors routinely demand that thesebuckets are encrypted; but as the failure mode of an S3 bucket isn’t a burglarin Amazon’s data centre but negligence over access controls, it’s unclear thatS3 bucket encryption achieves anything other than tick-box compliance.
And ﬁnally one has to consider abusability, of which there are at least twosigniﬁcant kinds.
 First, the wide availability of FDE code is one of the twocomponents that led to the recent wave of ransomware attacks, where a gangpenetrates your systems, installs FDE, lets it run until you’ve encrypted enoughbackups to make recovery painful, then demands a ransom for the key.
 (Theother component is cryptocurrency which I’ll discuss later in this chapter.
) Sec-ond, many people consider FDE to be magic insurance against compromise, andwon’t report a laptop left on a train if it had FDE enabled (or was supposedto), even if the ﬁnder might have seen the password, or be able to easily guessit.
So even the simplest of encryption products has a signiﬁcant entanglementwith compliance, is much more complex under the hood than you might think atﬁrst glance, usually imposes some performance penalty, and can be vulnerable toa capable opponent – even years after the relevant attacks have been published.
20.
3SignalAs smartphones spread round the world, people switched from SMS to messagingapps such as WhatsApp, Telegram and Signal as they’re cheaper and moreﬂexible, allowing you to create groups of families and friends.
 Pretty soon theystarted supporting voice and video calls too, and o↵ering end-to-end encryption.
It had previously been possible to encrypt email using programs like PGP, butit was rather ﬁddly (as we discussed in section 3.
2.
1) and remained a nicheactivity.
 The arrival of new platforms meant that message encryption couldbe made universal, shipped as a default with the app; the Snowden disclosureshelped stoke the public demand.
Signal is a free messaging app, initially developed by a man who uses thename of Moxie Marlinspike.
 It set the standard for end-to-end encryption ofmessaging, and its mechanisms have been adopted by competing products in-cluding WhatsApp.
 Mobile messages can be highly sensitive, with everythingfrom lovers’ assignations through business deals to political intrigues at diplo-matic summits; yet mobile phones are often lost or stolen, or sent in for repairwhen the screens break.
 So key material in phones is frequently exposed to com-promise, and it’s not enough to just have a single long-lived private key in anapp.
 The Signal protocol therefore provides the properties of forward secrecy,that a key compromise today won’t expose any future tra�c, and backward se-crecy, which means that it won’t expose previous tra�c either.
 These are nowSecurity Engineering609Ross Anderson20.
3.
 SIGNALformalised as post-compromise security [451].
The protocol has three main components: the Extended Triple Di�e-Hellman(X3DH) protocol to set up keys between Alice, Bob and the server; a ratchetprotocol to derive message keys once a secret key is established; and mechanismsfor ﬁnding the Signal keys of other people in your address book.
We can’t use vanilla Di�e-Hellman to establish a fresh key between Aliceand Bob, as they might not be online at the same time.
So in the X3DHprotocol [1227], each user U publishes an identity key IKU and a prekey SKUto a server, together with a signature on the latter that can be veriﬁed usingthe former.
 The algorithms are elliptic-curve Di�e-Hellman and elliptic-curveDSA.
 When Alice wants to send a message to Bob, she fetches Bob’s keys IKBand SKB from the server, generates an ephemeral Di�e-Hellman key EKA,and combines them with Bob’s keys in all the feasible ways: DH(IKA, SPKB),DH(EKA, IKB), and DH(EKA, SPKB).
 These are hashed together to givea fresh key KAB.
 Alice then sends Bob an initial message containing her keysIKA and EKA, a note of which of Bob’s prekeys she used, and a ciphertextencrypted using KAB so that he can check he’s got it too.
 Optionally, Bob canupload a one-time ephemeral key that Alice will combine with EKA and hashinto the mix.
Given an initial Di�e-Hellman key KAB, Alice and Bob then use the dou-ble ratchet algorithm to derive message keys for individual texts and calls.
 Itspurpose is to recover security if one of their phones is compromised.
 It usestwo mechanisms: a key derivation function (KDF) or one-way hash function toupdate stored secret keys, and further Di�e-Hellman key exchanges.
 Alice andBob each maintain separate KDF chains for sending and for receiving, each witha shared-secret key and a Di�e-Hellman key.
 Each message carries a new Di�eHellman key part which is combined with the key for the relevant chain, whilethe shared-secret key is passed through the KDF.
 The actual details are slightlymore ﬁddly, because of the need to deal with out-of-order messages [1512].
 Thegoal is that an opponent must compromise either Alice’s phone or Bob’s con-tinuously in order to get access to the tra�c between them.
The really tricky part is the initial authentication step.
 If Charlie could takeover the server and send Alice his own IK instead of Bob’s, all bets are o↵.
 Thisis the attack being mounted on messaging apps by some intelligence agencies.
Systems such as Apple’s iMessage don’t just send a single identity key KI toyour counterparty but a whole keyring of device keys – one for each of yourMacBooks, iPhones and other Apple devices.
 Ian Levy and Crispin Robinson ofGCHQ propose that laws such as the UK’s Investigatory Powers Bill be used tocompel providers to add an extra law-enforcement key to the keyring of any useragainst whom they get a warrant [1153].
 This has led to policy tussles in theUSA, the UK and elsewhere, to which I return in section 26.
2.
8.
 Keeping suchsurveillance covert will depend on the phone app software remaining opaqueto users; otherwise the double ratchet algorithm will prevent Alice and Bob’sprivate conversation being joined by Charlie as a silent conference call partner,or ‘ghost user’.
 Signal attempts to forestall this by being open source.
The upshot is that if Charlie wants to exchange Signal messages with Alicewhile pretending to be Bob, he has to either compromise Bob’s phone or stealSecurity Engineering610Ross Anderson20.
3.
 SIGNALBob’s phone number.
 The options are much the same as if he wanted to stealmoney from Bob’s bank account.
 They include hacking and stealing the phone;using SS7 exploits to steal Bob’s SMS messages; and a SIM swap attack to takeover Bob’s phone number.
The easiest attack for an individual to mount isprobably SIM swapping, which we discussed in section 12.
7.
4.
 Signal now o↵ersan additional PIN that you need to enter when recovering service on a phonenumber on which a di↵erent handset was previously active.
 Nation states havesophisticated hacking tools, and have SS7 access – so if the FSB’s in your threatmodel, it’s best to use a phone whose number they don’t know, and don’t carryit around switched on at the same time as a phone they do know is yours, orthey might correlate the traces – as I described in section 2.
2.
1.
10.
As we will discuss in section 26.
2.
2, much of the beneﬁt of signals intelligencecomes from metadata, from knowing who called whom and when (or who trav-eled with whom and when).
 So for a whistleblower, the game depends on howmany other people will become suspects as well as you – the anonymity set.
 Ifyou’re a senior civil servant thinking of leaking an illegal policy to a newspaper,and you’re one of ten people who knows the story, then you might be the onlyone of the ten who has ever used Signal.
However, if you’re one of hundreds of low-level suspects (say you’re a unionorganiser or NGO sta↵er) and might be on a long list of targets for thematiccollection, then you may want to block the local police from systematicallyrecording your patterns of contacts, and here Signal can indeed help.
 It o↵ersthe interesting innovation of private contact discovery.
Previous attempts to help ordinary people use end-to-end encryption, suchas the email encryption program PGP, never got much traction outside specialistniches because key management was too much bother.
 Messaging apps solvedthe usability problem by demanding access to your address book, looking upall your contacts on their servers to see who else was a user and then ﬂaggingthem so you know you can message them.
However, giving service ﬁrms acopy of your address book is already a privacy compromise, and if you also letthem keep a plaintext record of your social graph, proﬁle name, location, groupmemberships and who is messaging whom, then investigators can get all this bysubpoena.
 The original version of Signal compared hashes of the phone numbersin people’s address books to discover who was using it; however, Christof Hagenand colleagues used 100 accounts over 25 days to scan all 505m phone numbersin the USA, discovering 2.
5m Signal users [848].
 Signal has now implementedprivate contact discovery; I will discuss it later in section 20.
6 which discussesSGX, the mechanism it uses.
 However, when you set up a Signal account onyour phone, even private contact discovery makes this fact immediately apparentto everyone in your address book who’s also on Signal (and they’ll say – ‘Hey,Fred’s about to leak something’ – so a careful leaker would buy a burner phonefor cash.
)A critical but less visible part of the system is the message server.
 This hasto store encrypted messages that have not yet been delivered but how muchelse is kept and for how long2? Signal keeps records of group memberships,but there’s now a proposal for anonymous group messaging, which would make2There was a debate about how to handle undelivered messages when keys change, andthe WhatsApp implementation was criticised for prioritising delivery over failing closed.
Security Engineering611Ross Anderson20.
4.
 TORgroup members known to each other but not to Signal’s servers [409].
 Again,technology can only do so much; if one member of your group is disloyal, theycan betray others.
 However Signal has got real traction as the leading commu-nications security tool available to the public.
 There was a signiﬁcant uptick inusage in the USA after the 2016 election, and in 2020 the European Commission(Europe’s civil service) ordered its sta↵ to switch to Signal after the compromiseof a server containing thousands of diplomatic cables [399].
There was an upset in July 2020, when a Signal update forced users to selecta PIN, with a view to keeping each user’s contact data encrypted in an enclave,so it could be recovered if the user got a new phone, and so that there could besome other way to make a Signal contact other than by sharing a phone number.
This created a storm of protest as users assumed that Signal would also keepmessage content; other users didn’t think a PIN gave enough protection, ordidn’t want to give Signal a PIN they used for banking, or just didn’t like theidea of any centralised data at all.
 People started questioning the wisdom ofrelying on a secure communications app whose chief maintainer is someone whouses a pseudonym, who can hold millions of users hostage on a whim, and whosebacking was partly from the government and partly from a billionaire3.
 Whatshould the governance of public-interest critical infrastructure look like?Signal claims to keep no records of tra�c, but what if a FISA warrant fromthe NSA had forced them to do so and lie about it? This brings us to the harderquestion of how communications can be made anonymous.
20.
4TorThe Onion Router (Tor) is the main system people use to get serious anonymityonline, with about 2 million concurrent users in 2020.
 It began its life in 1998at the US Naval Research Laboratory, and was called Onion Routing becausemessages in it are nested like the layers of an onion [1590].
 If Alice wants tovisit Eve’s website without Eve or anyone else being able to identify her, shesets up a TLS connection to a Tor relay operated by Bob, which sets up a TLSconnection to a Tor relay operated by Carol, which in turn a TLS connection toa Tor relay operated by David – from whose ‘exit node’ Alice can now establish aconnection to Eve’s website [1360].
 The idea is to separate routing from identity– anyone wanting to link Alice to Eve has to subvert Bob, Carol and Dave, ormonitor the tra�c in and out of Bob’s and David’s systems.
The inspiration had been a 1981 idea of David Chaum’s, the mix or anony-mous remailer [410].
 This accepts encrypted messages, strips o↵ the encryption,and then remails them to the address that it ﬁnds inside.
 Various people ex-perimented with these in the 1990s and found that you need three more thingsto make it work properly.
 First, you need more than one mix; an opponentcould compromise a single mix by coercing the operator, or simply correlatingthe tra�c in and out.
 Second, you need to engineer it for the tra�c you wantto protect, be that email, web or messaging.
 Third, and hardest of all, you needscale.
3Brian Acton, one of the founders of WhatsApp.
Security Engineering612Ross Anderson20.
4.
 TORThe Navy opened Tor up to the world in 2003 because you can only beanonymous in a crowd.
 If Tor had been restricted to US intelligence agents, thenanyone using it would be a target.
 It is now maintained by the Tor Project, aUS nonproﬁt that maintains the Tor Browser, which has become the default Torclient.
 This not only handles circuit setup and encryption but manages cookies,javascript and other browser features that are hazardous to privacy.
 Similarfunctionality is also built into some other browsers, such as Brave.
There’salso software for Tor relays, which are run by volunteers with high-bandwidthconnections; in 2020, about 6,000 active relays serve about 2 million users.
 Whenyou turn on a Tor-enabled browser, it opens a circuit by ﬁnding three Tor relaysthrough which it connects to the outside world.
Tor’s cryptographic and software design has evolved over 20 years in the faceof a variety of threats and abuse, and it is now used as a component in manyapplications.
 It’s used to defeat censorship in countries like Iran and Pakistan soyou can connect to Facebook and read American and European newspapers.
 TheUS State Department supports it, and Facebook is the biggest Tor destination.
It can also be used to connect to underground dark markets where you can buydrugs and malware.
 It can be used to leak classiﬁed documents.
 It can be usedto visit child sex abuse websites.
 The police also use it to visit such sites, so theoperators don’t know they’re police.
The principal vulnerabilities were known from day one and documented inthe 1998 paper that introduced onion routing to the world, six years beforeTor itself appeared [1590].
 But they have frequently been overlooked by care-less users.
 First, a malicious exit node can monitor the tra�c if Eve’s websitedoesn’t use encryption, or if she uses it in such a way that the exit node can doa man-in-the-middle attack.
 In September 2007, someone set up ﬁve Tor exitnodes, monitored the tra�c that went through them, and published the inter-esting stu↵ [1359].
 This included logons and passwords for a number of webmailaccounts used by embassies, including missions from Iran, India, Japan andRussia4.
 Yet the Tor documentation made clear that exit tra�c can be read,so more careful diplomats would have used a mail service that supported TLSencryption, as Gmail already did by then.
The second problem is the many tricks that web pages employ to track users.
This was the main reason for the introduction in 2008 of the Tor Browser, whichlimits the tracking ability of cookies and other ﬁngerprinting mechanisms.
 Butmany applications get users to identify themselves explicitly, or leak informationwithout realising it.
 In section 11.
2.
3 I discussed how supposedly anonymoussearch histories from AOL identiﬁed users: a few local searches (that tell whereyou live) and a few special-interest searches (that reveal your hobbies) can beenough.
Third, low-latency, high-bandwidth systems such as Tor have some intrinsicexposure to tra�c analysis [1363].
 A global adversary such as the NSA, thattaps tra�c at many points in the Internet, need only tap a small number ofexchange points to get a good enough sample to reconstruct circuits [1365].
 Inpractice this is harder than it looks5.
 Tor has made clear since the start that4This gave an insight into password choice: Uzbekistan came top with passwords like‘s1e7u0l7c’ while Tunisia just used ‘Tunisia’ and an Indian embassy ‘1234’.
5The intelligence community paid a compliment to Tor, on a GCHQ slide deck leaked bySecurity Engineering613Ross Anderson20.
4.
 TORit does not protect against tra�c conﬁrmation attacks, where the opponentcontrols both the entry and exit relays and correlates the timing, volume orother characteristics of the tra�c to identify a particular circuit.
 Indeed, in 2014it was discovered that someone (presumably an intelligence agency) had beendoing just this, volunteering relays into the system that tinkered with protocolheaders in order to make it easier [561].
 Tor relays now have countermeasuresagainst such tweaks, but tra�c conﬁrmation is still a threat.
Fourth, as Tor connects through a pool of some 6,000 relays, a ﬁrewall cansimply block their IP addresses.
 This is done by some companies and also bysome countries, most notably China.
 To circumvent such blocking, volunteersmake available Tor bridges – Tor entry nodes not listed in the public directory.
Various games are played as Chinese and other censors try to ﬁnd and blockthese too, and to characterise Tor tra�c.
 China appears to prefer that peoplecircumventing its national ﬁrewall use VPNs instead; these are not only morescalable but easier to shut down completely at times of crisis (such as in theearly stages of the 2020 coronavirus outbreak).
Law-enforcement agencies have on a number of occasions managed to ﬁndand close down Tor onion services, websites that are available only through theTor network; rather than a normal URL, they have a ‘.
onion’ address that isessentially a cryptographic key.
 The most famous such service was Silk Road,an underground marketplace where people bought and sold drugs; its operatorwas arrested because of poor operational security (the email address he used toannounce his new service could be traced back to him).
 Other onion serviceshave had their servers hacked, or supply chains traced.
Many of them usecryptocurrencies, which we’ll describe later and which can also be traced invarious ways.
 There have also been attacks on the browsers of Tor users withtechniques such as zero-days and sandbox escapes.
 And even in the absence oftechnical failures, anonymity is intrinsically hard; real-world transactions (andindeed real-world web tra�c) can be very dirty, so unexpected inferences canoften be drawn.
As with FDE, Tor has a signiﬁcant entanglement with compliance, helpinga variety of actors to evade surveillance and circumvent laws both good andbad.
 The engineering has become a lot more complex under the hood than itlooks.
 It deﬁnitely imposes a performance penalty – websites can take a secondto load rather than a few hundred milliseconds.
 And despite the robustness ofthe Tor system itself, it has intrinsic limitations that are not intuitively obviousand make anonymity systems built on it hazardous to use.
 Anonymity systemsrequire careful operational security as well as just the right software.
The governance aspects are of interest.
 Tor is maintained by the Tor Project,a US nonproﬁt set up in 2006 to formalise a volunteer project that had startedin 2002.
 Although it has many volunteers, a growing core of permanent sta↵have been funded from various sources over the years, from the EFF to the USState Department.
 It remains at heart an international community of peoplemotivated by human rights.
 An ethnographic study by Ben Collier describesit as made up of three overlapping groups: a group of engineers who see Toras a structure, and believe that political problems can be solved by doing engi-Ed Snowden, saying “Tor stinks!”Security Engineering614Ross Anderson20.
5.
 HSMSneering; a group of activists see it as a struggle, and are committed to speciﬁcpolitical values such as anti-racism; while a third group of people largely main-tain the Tor relays, are generally politically agnostic, and see what they do asproviding infrastructure – “privacy as a service” [458].
 Security at scale requiresinfrastructure, and to provide this largely by volunteer e↵ort requires leaderswho can translate between the di↵erent stakeholders’ agendas and negotiatevalues rather than just contracts.
20.
5HSMsIn the chapter on Banking and Bookkeeping, we described how banks use HSMsto enforce a separation-of-duty policy: no single person at the bank should beable to get their hands on a customer’s card details and PIN.
 HSMs are alsoused to protect the SSL/TLS keys for many websites; you don’t want importantlive keys to be sitting on a developerˆa˘A´Zs laptop, or to be easily extractableby a cloud provider through a memory dump.
 In the cryptocurrency industry,HSMs are used to protect keys that could sign away substantial assets.
 In thechapter on Tamper Resistance, we described the mechanisms used to make theHSM tamper-proof.
 But this isn’t enough.
 You also have to ensure that whenyou split a computation between a more trusted component such as an HSMand a less trusted component, an attacker can’t exploit the split.
Whenever a trusted computer talks to a less trusted one, you have to expectthat the less trusted device will lie and cheat, and probe the boundaries by usingunexpected combinations of commands, to trick the more trusted one.
 How canwe analyse this systematically?Banking HSMs have a lot to teach.
 In 1988, Longley and Rigby identiﬁed theimportance of separating key types while doing work for security module vendorEracom [1184].
 In 1993, we reported a security ﬂaw that arose from a customtransaction added to a security module [107].
 However we hit paydirt in 2000when Mike Bond, Jolyon Clulow and I observed that HSM APIs had becomeimmensely complex, with hundreds of di↵erent transactions involving complexcombinations of cryptographic operations to support dozens of payment protocolvariants, and started to think systematically about whether there might be aseries of HSM transactions that would break it [71].
 We asked: “How can yoube sure that there isn’t some chain of 17 transactions which will leak a clearkey?’ After we spent some time staring at the manuals, we started to discoverlots of vulnerabilities of this kind.
20.
5.
1The xor-to-null-key attackHSMs are driven by transactions sent to them by servers at a bank or ATMs inthe ﬁeld.
 The HSM contains a number of master keys that are kept in tamper-responding memory.
 Most keys are stored outside the device, encrypted underone or more master keys.
 It’s convenient to manage keys for ATMs and otherterminals in the databases used to manage them; and nowadays many HSMsare located in the Azure and Amazon clouds where they serve multiple tenants.
Security Engineering615Ross Anderson20.
5.
 HSMSThe encrypted working keys have a type system which classiﬁes them byfunction.
 For example, in the PCI standard for security modules, a PIN deriva-tion key – the master key used to derive a PIN from an account number asdescribed in section 12.
4.
1 – is stored encrypted under a particular pair of mas-ter DES keys to mark it as a non-exportable working key.
 The Terminal MasterKey for an ATM is of the same type, and you’ll recall from section 12.
4.
1 thatATM security policy is dual control, so the bank generates separate keys fortwo ATM custodians, say the branch manager and the branch accountant, whoenter them at a keypad when the device is commissioned, or following a servicevisit.
 The HSM thus has a transaction to generate a key component and printit out on an attached security printer.
 It also returns its encrypted value to thecalling program.
 There was another transaction that combines two componentsto produce the terminal master key: given two encrypted keys, it would decryptthem, exclusive-or them together, and return the result – encrypted in such away as to mark it as a non-exportable working key.
The attack was to combine a key with itself, yielding a known key – thekey of all zeros – marked as a non-exportable working key.
As there was afurther transaction, which would encrypt any non-exportable working key withany other, you were now home and dry.
 You could extract the crown jewels –the PIN derivation key – by encrypting it with your all-zero key.
 You can nowdecrypt the PIN derivation key and work out the PIN for any customer account.
The HSM has been defeated.
The above attack went undiscovered for years.
 The documentation did notspell out what the various types of key in the device were supposed to do; non-exportable working keys were just described as ‘keys supplied encrypted undermaster keys 14 and 15’, and the implications of a transaction to encrypt one suchkey under another were not immediately obvious.
 In fact, the HSMs had simplyevolved from earlier, simpler designs as ATM networking was introduced in the1980s and banks asked for lots more features so they could make heterogeneousnetworks talk to each other.
So Mike Bond built a formal model of the key types used in the deviceand immediately discovered another ﬂaw.
 You could supply the HSM with anaccount number, pretend it’s a MAC key, and get it encrypted with the PINveriﬁcation key – which also gives you the customer PIN directly.
 Confused?Initially everyone was – modern APIs are way too complicated for bugs to beevident on casual inspection.
 Anyway, the full details are at [100].
 The latestHSMs have strong typing to make it easier to reason formally about keys.
20.
5.
2Attacks using backwards compatibility and time-memory tradeo↵sWe worked with an HSM vendor, nCipher, who supplied us with samples of theircompetitors’ products, so we could break them – not just to help their marketing,but to enable them to migrate customer key material to their own products.
 Thetop target at the time was the IBM product, the 4758 [951].
 This was the onlydevice certiﬁed to FIPS 140-1 level 4; in e↵ect the US government had said it wasunbreakable.
 It turned out to be vulnerable to an attack exploiting backwardsSecurity Engineering616Ross Anderson20.
5.
 HSMScompatibility [279].
As DES became vulnerable to keysearch during the 1980s, banks startedmigrating to two-key triple-DES: each block was encrypted with the left key,decrypted with the right key and then encrypted with the left key once more.
This bright idea gave backward compatibility: if you set the left key equal tothe right key, the encryption reverts to single-DES.
 The 4758 stored left keysand right keys separately, and encrypted them di↵erently, giving them di↵erenttypes – but failed to bind together the two halves of a triple-DES key.
 You couldtake the ‘left half’ of a single-DES key plus the ‘right half’ of another, put themtogether into a true triple-DES key, and then use this to export other keys.
So all you had to do to break the 4758 was a single-DES keysearch.
 That’snot too hard now, but was still a fair bit of work back in 2002.
 Fortunately therewas another vulnerability – a time-memory tradeo↵ attack.
 That generation ofHSMs had ‘check values’ for keys – one-way hashes of each key, calculated byencrypting a string of zeroes.
 Suppose you want a single DES key of a speciﬁctype.
 You precompute a table of (say) 240 keys and their hashes.
 You get theHSM to generate keys of the desired type and output the hashes until you seea hash that’s already in the table.
 This takes about 216 hashes, which takesan hour or so [447].
 The backwards-compatibility and time-memory tradeo↵attacks are examples of an API attack on the HSM platform itself rather thanon the PCI PIN management app.
20.
5.
3Di↵erential protocol attacksThe 4758 bugs got ﬁxed, and recent models of ATM o↵er public-key mechanismsfor automatic enrolment.
 But legacy key-management and PIN-managementmechanisms persist at the app layer, as it’s hard to change the architecture of adistributed system with hundreds of vendors and thousands of banks.
 And therewas much more to come.
 The next wave of attacks on HSM APIs was initiatedby Jolyon Clulow in 2003; they perform active manipulation of the applicationlogic to leak information.
 Many HSMs support transactions tailored for speciﬁcapplications; the largest market segment is to support card payments, thoughthere are also HSMs for prepayment utility meters, for certiﬁcation authoritiesand even for nuclear command and control.
Clulow’s ﬁrst attack exploited error messages [449].
I described in sec-tion 12.
4.
2 how banks who just wrote a customer’s encrypted PIN to their bankcard got attacked, as a customer could change the account number to anotherone and use their PIN to loot that account.
 In order to stop such attacks, Visaintroduced an optional PIN block format that exclusive-ors the PIN with theaccount number before encrypting them.
 But if the wrong account number wassent along with the PIN block, the HSM would decrypt it, xor in the accountnumber, and when the result was not a decimal number, it would return an errormessage.
 So by sending a few dozen transactions to the HSM with a variety ofwrong account numbers, you could work out the PIN6.
 There are now special6There are now four di↵erent PIN block formats for PIN transmission, three of whichinclude the PAN as well; and there’s a further format, the PIN Veriﬁcation Value (PVV),which is a one-way encryption of the PIN and PAN that’s sent by banks to switches such asVISA and Mastercard if they want the switch to do stand-in PIN veriﬁcation when their ownSecurity Engineering617Ross Anderson20.
5.
 HSMSPCI rules for HSMs on PIN translation [977].
 Complexity opens up new attacks,which need yet more complexity to patch them.
A further class of attacks was then found by Mike Bond and Piotr Zielinski.
Recall the method used by IBM (and most of the industry) to generate PINs,as shown in Chapter 12, ﬁgure 12.
3.
 The primary account number is encryptedusing the PIN veriﬁcation key, giving 16 hex digits.
 The ﬁrst four are convertedto decimal, and while most banks do this by taking the hex digits modulo 10,not all do.
 HSM vendors parametrised the operation by having a decimalisationtable, of which the default is 0123456789012345, which just reduces the hexoutput modulo 10.
 This was a big mistake.
If we set the decimalisation table to all zeros (i.
e.
, 0000000000000000) thenthe HSM will return a PIN of ’0000’, albeit in encrypted form.
 We then repeatthe call using the table 1000000000000000.
 If the encrypted result changes, weknow that the DES output contained a 0 in its ﬁrst four digits.
 Given a few dozenqueries, the PIN can be deduced.
 Attacks that compare repeated, but slightlymodiﬁed, runs of the same protocol, we call di↵erential protocol analysis.
 Theonly real solution was to pay your HSM vendor extra for a machine with yourown bank’s decimalisation table hard-coded.
 That may cause more problemswhen you want to move your bank to the cloud, and share HSMs maintainedby Amazon or Azure7.
At a philosophical level, this illustrates the di�culty of designing a robustsecure multiparty computation – a computation that uses secret informationfrom one party, but also some inputs that can be manipulated by a hostileparty [99].
 Even in this extremely simple case, it’s so hard that you end uphaving to abandon the IBM method of PIN generation, or at least nail downits parameters so hard that you might as well not have made them tweakable inthe ﬁrst place.
At a practical level, it illustrates one of the main reasons APIs fail over time.
They get made more and more complex, to accommodate the needs of more andmore customers, until suddenly there’s an attack.
20.
5.
4The EMV attackYou’d have thought that after the initial wave of API attacks were publishedin the early 2000s, HSM designers would have been more careful about addingnew transactions.
 However, just as security researchers and HSM vendors foundand ﬁxed bugs, the banking industry mandated new ones.
For example, an HSM feature ordered by EMVCo to support secure mes-saging between a smartcard and a bank HSM introduced an exploitable vul-nerability in all EMV compliant HSMs [22].
 The goal was to enable a bank toorder any EMV card it had issued to change some parameter, such as a key,the next time it did an online transaction.
 So EMVCo deﬁned a transactionSecure Messaging For Keys whereby a server can command an HSM to encryptsystem is down.
7One vendor decreed that a table must have at least eight di↵erent values, with novalue occurring more than four times.
But this doesn’t work:0123456789012345, then1123456789012345, and so on.
Security Engineering618Ross Anderson20.
5.
 HSMSa text message, followed by a key of a type for sharing with bank smartcards.
The encryption can be in CBC or ECB mode, and the text message can be ofvariable length.
 The attack is to choose the message length so that just one byteof the target key crosses the boundary of an encryption block.
 That byte canthen be determined by sending a series of messages that are one byte longer,and where the extra byte cycles through all 256 possible values until the keybyte is found.
20.
5.
5Hacking the HSMs in CAs and cloudsThe most recent HSM break, in 2019, was by Jean-Baptiste B´edrune and GabrielCampana, on a Gemalto HSM whose application supported the PKCS#11 stan-dard for public-key cryptography so it could be used in certiﬁcation authoritiesand as a TLS accelerator.
 (This standard is notoriously obscure and di�cult toimplement.
) They got a software development kit for the HSM, which containedan emulator for the device, and fuzzed it until they found several vulnerabili-ties.
 They managed to patch the authentication function so they could login asadmin into the HSM and install tools that read out the keys [203].
 This is justone example of many where sophisticated cryptography was fatally underminedby careless software engineering.
20.
5.
6Managing HSM risksAt one time or another, someone had found an attack on at least one versionof every security module on the market.
 The root cause, as so often in securityengineering, is featuritis.
 People make APIs more complex until they break.
Banks still have to use HSMs for compliance with PCI rules, but the cryptokeys in them are not protected by the tamper responding enclosures alone.
 Theconﬁguration management has to be tight and vendor software patches have tobe applied promptly, just like in other systems.
 But while most banks of anysize have people who understand software security and the patching lifecycle,they are less likely to have serious HSM expertise.
Specialist ﬁrms o↵er HSM management systems, and we’ll have to see ifthese get subsumed eventually by the big cloud service providers.
 Managementof cloud HSMs is still a work in progress, and products such as Microsoft CloudKey Vault allow keys to be moved back and forth between HSMs and enclavesthat o↵er similar functionality.
 Of course, if a PIN management app has in-trinsic API vulnerabilities, these will be independent of whether it’s running ona traditional on-premises HSM, an HSM in a cloud data centre, or an enclave.
Indeed, one selling point of the Microsoft o↵ering is ‘Removing the need forin-house knowledge of Hardware Security Modules’ [1309].
With that warning, it’s time to look at enclaves.
Security Engineering619Ross Anderson20.
6.
 ENCLAVES20.
6EnclavesEnclaves are like HSMs in that they aim to provide a platform on which youcan do some computation securely on a machine operated by someone you don’tentirely trust.
 Early attempts involved mechanisms for digital rights manage-ment (DRM) which obfuscated code to make it hard to interfere with8, and werefollowed by the ‘trusted computing’ initiative of the early 2000s.
 This proposedan architecture in which CPUs would execute encrypted code, with the keysstored in a separate Trusted Platform Module (TPM) chip.
 Arm duly producedTrustZone in 2004, as I described in section 6.
3.
2.
TrustZone is typically implemented in the System-on-Chip (SoC) at the heartof a modern Android phone, although its trust boundary is typically the wholemotherboard; enclave data may be available in clear on the bus and in DRAMchips.
 The main application has been mobile phones, whose vendors wantedmechanisms to protect the baseband against user tampering (for regulatoryreasons) and to enable the phone itself to be locked (so that mobile networkoperators who subsidise phones could tie them to a contract).
 In neither caseare hardware attacks a real concern.
Could an enclave mechanism such as TrustZone be used to harden a phone-banking system against the kind of attacks we discussed in section 12.
7.
4? At-tempts were made to market it for this purpose, but even ﬁrms that writebanking apps were reluctant to adopt it.
 Up until 2015, it was a closed system,and you could only run code in TrustZone if you had it signed by the OEM.
So a developer of a banking app who wanted a ‘more secure’ authenticationcomponent would have to get that signed by Samsung for Samsung phones, byHuawei for their products, and so on.
 What’s more, the code would be di↵erentdepending on which SoC the product used.
 Now it’s hard enough to make anapp run robustly on enough versions of Android without also having to copewith multiple customised versions of TrustZone running on di↵erent SoC o↵er-ings.
 It’s also hard to assess security claims that vendors make about closedplatforms.
 For the gory details, see Sandro Pinto and Nuno Santos [1529].
In 2015, Intel launched SGX, whose access-control aspects I discussed insection 6.
3.
1.
 SGX enclaves have aimed at a more ambitious use case, namelycloud computing.
 It’s become cheaper to run systems on services such as AWS,Azure and Google: virtualisation lets resources be shared e�ciently, so thecosts of data centres, sysadmins and so on can be amortised over thousands ofcustomers.
 But this raises many questions.
 How can you be sure that sensitivedata isn’t leaked to other tenants of the cloud service, for example via technicalexploits of the hypervisor software? Such products have dozens of bugs patchedevery year [479].
 And what protection do you have against a nation state usinga warrant to get access to your data – in e↵ect a legal exploit of the hypervisor?The cloud service providers themselves long for a technical mechanism thatwould save them the trouble of dealing with such warrants.
 Because of theseconcerns, the security perimeter of SGX is the boundary of the chip itself.
 Codeand data are encrypted as they leave the chip, and decrypted as they’re importedinto the cache.
 The CPU’s hardware protects both conﬁdentiality and integrity.
8For an introduction, see the chapter on ‘Copyright and DRM’ in the second edition ofthis book, available free online.
Security Engineering620Ross Anderson20.
6.
 ENCLAVESThe key cryptographic mechanism is software attestation which enables theCPU to certify to the owner of the software that it is running without modi-ﬁcation on top of trustworthy hardware.
 SGX enclaves run as applications, atring 3, and the CPU machinery isolates their code and data from everythingunderneath, including both operating system and hypervisor9.
 The full detailsof enclave initialisation, address translation, page eviction, exception handlingand so on are extremely complicated; for an explanation and analysis, see Vic-tor Costan and Srini Devadas [479].
 One concern they raise is that with theexception of memory encryption, SGX is implemented in microcode, which canbe updated; the whole system is therefore changeable.
 There are also multi-ple side-channel attacks, particularly since Meltdown and Spectre introducedthe transient execution family of side-channel attacks, which I discussed in sec-tion 19.
4.
5.
 Some have been patched, but the real scandal may be that Intelhas said it won’t ﬁx the Membuster attack as a matter of policy10.
Here my concern is the cryptography used to support the enclave and attestto the software running on it, and its suitability as a platform for other cryptoor crypto support for applications.
As the silicon processes used in high-end CPUs don’t support nonvolatilememory, the ﬁrst problem is to provide unique and persistent chip keys.
 Eachchip has fuses into which the fab burns a seal secret and a provisioning secret,of which the former is not known to Intel but the latter is.
 This is used togenerate the master derivation key (MDK) which in turn generates key materialdependably across power cycles.
 Provisioning seal keys are persistent, so whena computer changes owners, Intel doesn’t need to know.
 These keys enable theCPU to prove its authenticity to Intel which supplies it with an attestation key –a member private key in Intel’s Enhanced Privacy ID (EPID), a group signaturescheme intended to preserve signer anonymity.
These operations are done in a privileged launch enclave (LE).
 Originally allSGX code had to be signed by Intel, but recent versions allow code signed bythird parties.
 Each enclave author is now a CA and certiﬁes each enclave, whichhas a public key, a product ID and a version number (migration of secrets isallowed only to higher version numbers to support patching but not rollback).
The same ratchet applies to updates of the CPU microcode.
One issue is that the compromise of one chip’s MDK – in any CPU, anywhere– breaks the attestation security of every CPU in the same group.
 This happenedin 2019 for AMD’s equivalent of SGX, when a bug in the microcode enabledsuch a key to be extracted [337].
 Intel is vulnerable in the same way: given aclear value of MDK you can create an SGX enclave outside of SGX’s protectionmechanisms.
 If such a break were discovered, Intel would have to blacklist allthe CPUs in the same EPID group.
 We have no idea how large these groupsare, as all attestations are done opaquely by Intel and users must simply trustthe results.
9The earlier proposals of the Trusted Computing Group required that the whole softwarestack underneath the enclave be attested and trustworthy, which is incompatible with anuntrusted hypervisor.
10SGX doesn’t defend against cache timing attacks, so when writing enclave code, you can’tuse data-dependent jumps.
 More generally, it does not protect against software side-channelattacks that rely on performance counters, but doesn’t give enough information for developersto model the possible leakage.
Security Engineering621Ross Anderson20.
6.
 ENCLAVESThere are now some SGX systems doing real work.
 An example I mentionedearlier in this chapter is the messaging app Signal, which uses an enclave forprivate contact discovery.
 Its developers published the source code along with anextensive discussion of the di�culties of developing it on the Signal blog [1226].
The goal is to enable Signal clients to determine whether the contacts in theiraddress book are also Signal users without revealing their address book to theSignal service.
 How can you build a large social graph without having any insightinto it? The idea is that clients can contact the enclave, verify it’s running theright software, and send their contacts in to see who’s also a user.
 However,doing this within the memory limit of an SGX enclave (128Mb) needs carefulorganisation of hash tables of an inverted ﬁle of users’ phone numbers.
There are many more things you have to do to prevent information leakagethrough memory access patterns: as branches might be observed through suchpatterns, critical sections of code must not contain branches.
 In short, blockingside channels is much like organising crypto code to run in constant time: ﬁddly,ad hoc, manual and prone to error.
 SGX is also slow: while the memory encryp-tion itself adds little overhead, context switching is a killer.
 Checking contactsagainst others is really slow, so the process has to be batched for multiple joinersto make it acceptable.
Another example of an SGX app is Microsoft’s Cloud Key Vault, whichenables Azure tenants to store secrets such as keys, passwords and tokens sep-arately from their code [1309].
 There’s an app to help you create and managecertiﬁcates for TLS; secrets and keys can also be stored in cloud HSMs at the topend, while routine applications can be both more secure and more manageableif you don’t have to store database passwords inline in your code.
In short, writing good SGX code is hard.
 The toolchain is restricted, andthings like antivirus are excluded.
 If you’re smart, you can write trusted mal-ware.
 You can even write malware that will run in one SGX enclave and dotiming attacks on code in other enclaves in the same machine, using the SGXmechanisms to hide itself from detection [1689].
And even if you trust Intel completely; even if you believe that the NSAwon’t use a FISA warrant to force Intel to attest to an enclave in debug mode;even if you’re not worried about an MDK compromise or the exploitation of sidechannels – then there’s still the risk of app-layer exposure, just as with HSMs.
If you write your enclave code in such a way that it can be used as an oracle byless trusted code, you’re in trouble.
Intel (and Arm) are talking about successor versions of their enclave tech-nology.
 Meantime Intel points crypto developers at their management engine(ME), a separate microcontroller shipped in the CPU chipset that starts theCPU and contains a ﬁrmware TPM to do secure boot.
 It can brick a CPU byerasing keys if the machine is reported stolen.
 Its code is proprietary, basedon Minix, and is signed by Intel.
 It supports yet another enclave with a Javatrusted execution environment, in which developers can do crypto; for example,in payment terminals you can engineer a hardware trusted path from the MEto a PIN pad [1698].
 This enables crypto code to be shielded from malware onthe CPU but brings issues of its own, such as attacks involving physical access.
The ME has also had a whole series of vulnerabilities and exploits.
 It is consid-Security Engineering622Ross Anderson20.
7.
 BLOCKCHAINSered by the EFF to be a backdoor, and at least one vendor has made machinesavailable to governments where it is switched o↵ after boot.
20.
7BlockchainsThe previous sections on the uses and limits of cryptography, on how cryptog-raphy can be used to support anonymity, and how crypto apps can su↵er ﬂawsat various levels in the stack, set us up to discuss cryptocurrencies and smartcontracts.
 During 2016–7, cryptocurrencies were ‘the’ thing, taking their placein the hype cycle after Big Data and the Internet of Things, alongside AI andquantum.
 To many people, the word ‘crypto’ now refers to bitcoins rather thanto ciphers.
In 2008, Bitcoin was released quietly by someone using the pseudonym ofSatoshi Nakamoto, with a white paper and an implementation [1375].
Thissystem of anonymous digital cash circulated initially among hobbyists and ac-tivists on the cypherpunks mailing list, but within two years it had gone viral.
In February 2011, a young libertarian called Ross Ulbricht set up Silk Road,an online marketplace outside government control.
 Buyers and sellers met ona Tor onion service and could pay for goods and services using Bitcoin.
 Theycould rate each other, as on eBay, and there was an escrow service so that abuyer could deposit bitcoins for release when goods were delivered.
 Silk Roadrapidly became the market for the mail-order supply of controlled drugs, andover $1bn worth of trades went through it before the FBI arrested Ulbricht inOctober 2013 [421].
 Other underground markets adopted Bitcoin too.
 WhileSilk Road was trading, the price had risen from about a dollar to over a hundreddollars, and the rising price attracted investors11.
 Further transaction demandcame from people wanting to get their money out of countries with exchangecontrols, leading to investment demand from people seeing Bitcoin as an assetto be bought in times of crisis, like gold.
 By 2017 we had a bubble – with theprice of a bitcoin rising steeply through the thousand-dollar mark to a peak inDecember 2017 of almost $20k.
Bitcoin has spawned multiple imitators – most of them scams, but some realinnovations too.
 Boosters claimed that cryptocurrency would enable a new waveof innovation and automation as machines could negotiate smart contracts witheach other without humans or banks getting in the way.
 At the time of writing(2020), the peak of enthusiasm has passed, but cryptocurrencies have becomea new asset class for investors, as well as posing multiple problems for ﬁnancialregulators and law enforcement.
All that said, Bitcoin is a fascinating construct of cryptography and eco-nomics which has led to the emergence of a payment system that is also atrusted computer, out of the distributed e↵ort of millions of machines that at-tempt to mine bitcoins.
 There are no trusted parties other than the peoplewho write the software, and no pre-assumed identities of participants.
Themechanisms provide a new way of achieving consensus in distributed systems,quite distinct from the Byzantine fault-tolerance mechanisms we discussed in11When Ulbricht was busted, the Bitcoin price fell from $145.
70 to $109.
76, but as otherdrug markets got going, it quickly recovered.
Security Engineering623Ross Anderson20.
7.
 BLOCKCHAINSsection 7.
3.
1.
 That is one reason to include cryptocurrencies as an example ofadvanced cryptographic engineering; another is the smart contracts and othersecond-layer protocols built on top of them, which are of technical interest al-though they have had little impact so far on business (the total capital of digitalexchanges may be only about $1bn).
Here is a brief summary of the basic mechanisms.
1.
 The Bitcoin blockchain is an append-only ﬁle containing a series of trans-actions.
2.
 Users appear on the blockchain as addresses – pseudonyms which arehashes of public keys.
3.
 Most transactions transfer currency from one address to another by takingan unspent transaction output (UTXO) from a previous transaction andtransferring it to one or more addresses.
 Such a transaction must be signedby the private key corresponding to the UTXO address.
4.
 To make a payment, you sign a transaction and broadcast it via a peer-to-peer network to other users.
Other users are free to select a set ofrequested transactions, check that they’re valid, and mine them into anew block for the blockchain.
5.
 Each block of transactions is authenticated by a miner by means of aSHA256 hash of the block contents and a random salt.
 Miners try di↵erentsalts until the hash output has enough leading zeros to make it a hardenough puzzle.
 Such a hash constitutes a proof of work, and ﬁnding themis a random process, so it’s hard to predict which miner will ﬁnd the nextone.
 The blockchain consists of a chain of hashes and the blocks theyauthenticate.
The di�culty of the puzzle is adjusted automatically sothat a new block is mined about every ten minutes.
6.
 Miners are paid a block reward for each block they mine; at the time ofwriting, this is 12.
5 bitcoins, or over $100,00012.
7.
 Miners also get transaction fees, which are the amount by which the inputsof each transaction exceed the outputs.
 Users bid transaction fees to getpriority for their transactions; they are usually in the tens of cents butcan rise into the tens of dollars at times of congestion.
8.
 If two competing next blocks are mined then the conﬂict is resolved by therule that miners mine the longest chain.
 As a result, transactions aren’treally considered ﬁnal until about half a dozen further blocks have beenmined – about an hour for classic Bitcoin.
 Even so, a majority of minerscould rewrite history by constructing a chain that reached even furtherback – a so-called chain reorganisation.
12In early 2020 a miner who could buy electricity for 5c per kWh could expect to mineBitcoin worth about half what the coins would fetch on the market, if you disregard the costsof the equipment.
 However the reward halves from time to time to limit the total supply ofbitcoin, and the reward is due to drop to 6.
25 bitcoin in mid-2020.
 People investing in miningrigs are therefore gambling that the Bitcoin price will rise and that regulators will not bee↵ective in suppressing demand.
Security Engineering624Ross Anderson20.
7.
 BLOCKCHAINS9.
 If the conﬂict isn’t resolved then you can end up with a fork – the systemspawns two incompatible successors.
 Bitcoin split in 2017 into Bitcoinand Bitcoin Cash over a policy dispute about block length, and users whoowned bitcoins before the fork ended up owning bitcoins in both.
 Butsome forks have been deliberate, and on top of that entrepreneurs havestarted several thousand Bitcoin clones – most of which were scams.
10.
 Transactions can also contain scripts, which make payments programmable.
For a detailed description, there are three standard references.
The ﬁrsttwo are technical expositions by a group of Princeton computer scientists: an18-page systematisation-of-knowledge paper in 2015 by Joe Bonneau, AndrewMiller, Jeremy Clark, Arvind Narayanan, Joshua Kroll and Ed Felten [293] whileat 308 pages there’s a 2016 book by Arvind Narayanan, Joe Bonneau, Ed Felten,Andrew Miller and Steven Goldfeder [1383].
 The third is a 2015 paper in theJournal of Economic Perspectives by Rainer B¨ohme, Nicolas Christin, BenjaminEdelman, and Tyler Moore [274].
 At the time of writing, these are getting outof date, so in what follows I will concentrate on developments since then.
 I’llassume you know the detail, or can look it up, or are not too bothered.
To understand what can go wrong with cryptocurrencies, we have to look ata lot more than just the cryptomathematics.
 A common pattern has been thatelegant cryptographic ideas are let down by shoddy software engineering, a lackof systems thinking and a near-total lack of concern for users.
20.
7.
1WalletsIn the beginning, all Bitcoin users were peers: the full client software wouldmine Bitcoin and let you spend the coins you mined.
 But things soon started tospecialise with custom rigs for miners, and light clients for ordinary users whichdon’t do mining or store the whole blockchain, but make the process of buyingand selling more manageable.
 There is no intrinsic concept of an account, asyou own Bitcoin by knowing a private key that will unlock one or more UTXOs.
Wallets initially stored one or more private keys and provided an interface so theuser could see the UTXOs that these keys could spend (‘my bitcoins’).
 Walletsecurity rapidly became a big deal.
 So-called ‘brain wallets’ which generatedprivate keys from a user-selected passphrase were broken by attackers doingexhaustive search over the public keys visible on the blockchain; brain walletswith guessable passwords were typically emptied within 24 hours [1947].
Software wallets that keep your signing keys on your hard disk, protected bya passphrase, are an improvement, but vulnerable to malware and other attacks.
Serious operators use hardware wallets, which are essentially small HSMs andwhich may be kept o✏ine (so-called cold wallets).
 Even so it is not unknownfor people who are known to own millions of dollars worth of Bitcoin to be heldup by armed robbers in their homes and forced to transfer it.
 If you have solephysical custody of a Bitcoin wallet then you’re just as vulnerable as when,centuries ago, people kept their savings in gold coins.
 By 2013 we’d seen theemergence of hosted wallets where an exchange or other online service providerdoes everything for you.
 That doesn’t really solve the robbery problem, as theSecurity Engineering625Ross Anderson20.
7.
 BLOCKCHAINSrobber will just force you to log on and pay him.
 But hosted wallets have ledto widespread other fraud and abuse as I’ll describe below.
20.
7.
2MinersAs bitcoins grew in popularity and value, more people joined in to mine them.
Mining rigs appeared using FPGAs and then ASICs that run so much fasterthan software on general-purpose machines that within a few years they hadtaken over.
 Miners operate where electricity is naturally cheap, such as Icelandand Quebec, but are mostly in places like Russia or China where they cando deals with local o�cials.
 The total energy consumption of cryptocurrencymining during 2019 was about 75TWh, and the CO2 emissions were over 35Mt– comparable to the carbon footprint of New Zealand.
 As of 2020, each bitcointransaction consumes over half a MWh and emits over a quarter ton of CO2.
Miners have organised themselves into a small number of mining pools thataverage their earnings.
 The control of these pools is opaque.
 Capacity can berented and is sometimes used to attack cryptocurrencies in so-called 51% attacks.
The whole point of the blockchain is to prevent double spending by creatinga tamper-proof, public, append-only log of transactions; but if a majority ofminers collude then they can rewrite history and spend coins multiple times.
 Inthe early days, people thought that such an attack would be instantly fatal to acurrency’s credibility, but reality turned out to be more complex.
 For example,in January 2019, attackers used this technique to steal over $1m from EthereumClassic, a cryptocurrency with a market capitalisation of over $500m, with chainreorganisations dozens of blocks in length [1428].
 Yet its market value was notsigniﬁcantly a↵ected.
 Had they stolen most of it, the price would have collapsedand their loot would have been worthless.
 There were two furthers attack inAugust 2020, in one of which the attackers spent $192,000 to buy the hash powerrequired to steal $5.
6m [1519].
 So we need to think carefully about the gametheory as well as the cryptography when reasoning blockchains; the simplisticarguments don’t always align with reality.
20.
7.
3Smart contractsThe scripting language in Bitcoin is simple, but a later cryptocurrency system,Ethereum, has a Turing-complete VM whose bytecode is usually compiled froma language called Solidity.
 Ethereum has become the second cryptocurrency bymarket cap as it holds out the prospect of smart contracts that can performcomplex transactions automatically.
 During the bubble, many startups talkedof using smart contracts to animate the Internet of Things, and to create newservices such as distributed storage, where people might pay others for the useof their spare hard disk space for backup.
 The idea of such a distributed au-tonomous organisation was heavily promoted during the bubble.
 This is linkedto the ‘redecentralize’ movement which seeks to move the online world awayfrom the large service ﬁrms that came to dominate it during the 2000s; andwhile we have good tools to decentralize the distribution of static, read-onlycontent, we lacked a good way to decentralize transactions [509].
 As of 2020,the main applications seem to be around trading, where distributed exchangesSecurity Engineering626Ross Anderson20.
7.
 BLOCKCHAINS(DEXs) enable people to trade one cryptocurrency for another without humanintervention.
 (They still account for only a tiny fraction of the total tradingvolume.
)This has led to interesting new failure modes.
 Although the consensus mech-anisms of the original Bitcoin blockchain are believed to be incentive compati-ble, this is not the case when the transactions on a blockchain represent extravalue that a miner can extract by manipulating the consensus.
There havenow appeared arbitrage bots that exploit ine�ciencies in DEXs by frontrunning(anticipating and exploiting) trades.
 The bots bid up transaction fees, calledgas in Ethereum; there have been hundreds of millions of these priority gasauctions where traders hustle to get priority for their trades [508].
 Bots mightin theory take over the governance of a market and loot it if they could raiseenough money [869]; they already make large proﬁts by exploiting bugs in smartcontracts [1507].
Fixing bugs can be expensive.
In 2016, an investment fund called DAOwas set up as a smart contract on the Ethereum blockchain, and attracted over$150m from over 10,000 investors.
 Attackers exploited a ﬂaw in the contract tosteal the money13, and after some discussion the Ethereum software was changedto move the stolen money to a recovery account.
 This resulted in a hard forkof the blockchain, with holders of the original cryptocurrency acquiring units inboth the modiﬁed currency and in ‘Ethereum Classic’, as the unmodiﬁed versionbecame known.
A Danish study illustrates the further problems of using smart contracts ina real-world application context.
 There had been a proposal to use them topay parents who have to take time o↵ work to care for sick children, which hascomplex legal rules that clerks often miss, leading to appeals.
 The idea wasto put hashes of the case documents on the Ethereum blockchain so that bothparents and the appeals board can track them, in the hope that automating theexecution of decisions would cut bureaucratic foot-dragging.
 But what aboutinsiders, hackers and mistakes? Local governments tend to get hacked a lotand end up paying ransomware.
 And who updates the contract when the lawchanges, or a bug is discovered? Blockchains are by design immutable, so can’tbe patched.
 But the real deal-breaker was local government fear of losing controlof the process.
 Two further issues include the fact that people often have to bendthe rules to get stu↵ done, and that programmers are more likely to write bugsin an unfamiliar language such as Solidity rather than a familiar one such asPython or even Cobol – a known problem with new languages, which I discussedin section 7.
3.
1.
2.
20.
7.
4O↵-chain payment mechanismsA standard Bitcoin transaction can take six blocks, or one hour, to becomeﬁnal, and even longer at times of congestion.
This may be fast enough forpaying ransoms or buying drugs online, but it’s unimpressive compared withEMV.
 What’s more, Bitcoin’s throughput of about 5 transactions per second is13An alternative view is that if the contract was to accept the output of the code, then theﬂaw was in the users’ grasp of what the code did, and in that case nobody stole anything!Security Engineering627Ross Anderson20.
7.
 BLOCKCHAINSno match for Visa’s 50,000.
People are trying to ﬁx this using side chains, an example of a layer 2 protocol;such protocols do transactions outside, but tethered to, a layer 1 protocol suchas Bitcoin or Ethereum.
 Alice and Bob open a channel by locking coins ona layer 1 blockchain, and can now do rapid transactions between themselves.
The key idea is that they commit some cryptocurrency to each other using ahashed time-lock contract (HTLC) made of two conditional transfers.
 In such atransfer, Bob sends Alice h(R), where R is a random number, and Alice makesa commitment in the blockchain’s scripting language to the e↵ect that “if youshow me R by time t I’ll give you this coin.
” Bob makes a similar commitment.
This opens a channel for them to trade signed transactions at speed, until theydecide to settle up and close the channel.
Quite a bit more engineering is needed to turn this into a working paymentsystem.
 You need a dispute resolution mechanism in case Alice and Bob dis-agree how much each of them should take from the proceeds.
 Then you buildmechanisms for Alice to pay Charlie via Bob, and routing algorithms so you canget money to anybody.
 In theory this can be peer-to-peer but in practice suchsystems appear to organise themselves into hubs, with channels that are alwaysopen, like a banking network.
 Protocol security involves ensuring that honestusers must not lose money even if others collude.
 Costs include the need forintermediate nodes to have enough liquidity to forward transactions, and theneed for all active players to be online – whose implications range from the theftrisks of hot wallets, to the risk of miners front-running Bob when he broadcastsR, to the risk of mass collapse following a network failure [831].
 The leadingsuch system in 2020 is the Lightning network, which makes payments ﬁnal inseconds, enables people with the right phone app to pay to a QR code as withWeChat Pay, and is now handling 1000 transactions per day.
 The limit here ap-pears to be liquidity: although Lightning chains themselves are trust-free, theytie up capacity at the nodes, and the recipient has to decide whether or not toaccept them.
 So a malicious user can set up hundreds of payments, leave themfor hours and then cancel them at no cost.
 As Lightning’s total capitalisationappears to be only a few million dollars, this may leave it somewhat fragile.
 Italso appears very possible that regulators will crack down on forwarding nodes.
20.
7.
5Exchanges, cryptocrime and regulationMining all your own coins is inconvenient, and by 2010 entrepreneurs had set upexchanges that would trade Bitcoin for conventional money.
 Most went bust,often because they were hacked, or because insiders stole the money and claimedto have been hacked.
 The leader by 2011 was Mt Gox in Japan which survivedone hack in 2011 but went bust in 2014 claiming that it had been hacked for$460m.
The court case continues; news coverage at the time reported thatinternal controls and software development processes were chaotic [1280].
That was not all.
 One of Mt Gox’s innovations was to become a custodial ex-change over the course of 2013.
 Instead of keeping customer bitcoins in separatewallets, for which the exchange might or might not have temporary access tothe private key after the customer entered the correct password, Mt Gox startedto keep all the Bitcoin in its own wallets, showing customers a notional accountSecurity Engineering628Ross Anderson20.
7.
 BLOCKCHAINSbalance when they visited its website.
 It had made the transition we saw ineighteenth-century ﬁnance from being a gold merchant to being a bank: ratherthan owning a speciﬁc bag of gold coins in the vault, the customer now justhad a claim on the bank’s whole assets.
 Victims related how after their walletswere hosted, they started to see outgoing transactions they had not authorised.
Analysis after the collapse of Mt Gox revealed that many of these transactionsdid not even appear on the blockchain.
 From mid-2013, when you bought abitcoin from them, all they did was to show you a web page saying that you hada balance of one bitcoin.
 (And that’s how many exchanges work to this day.
)The Bitcoin world has been full of scams, and it looks like the majorityof victims of cryptocrime were ripped o↵ by exchanges that went bust, or gothacked, or that claimed to have been hacked.
 Even in the ﬁrst three years thatexchanges existed, 2010–13, 18 of the 40 exchanges collapsed [1339].
A report by Chainalysis, a Bitcoin analytics ﬁrm, concluded that exchangeslost about $1bn to hackers in 2018, with most of the thefts perpetrated by twocrime gangs; one of them has since been linked to North Korea.
 In additionto this, turnover on underground markets where drugs and other illicit goodsare bought and sold was $600m, approximately double the value for 2017 [400].
There’s also market manipulation.
 John Gri�n and Amin Shams present evi-dence that Bitcoin’s price was supported by insider trading involving Tether, adigital currency pegged to the U.
S.
 dollar, during the 2017 boom [822], raisingthe prospect that the market price of many cryptocurrencies may often havebeen a result of unlawful manipulation.
 This has been borne out by subsequentstudies showing that much of the spot trading is generated by unregulated ex-changes [1615].
Market manipulation aside, the largest single cryptocurrency scam to dateappears to have been a Ponzi scheme called PlusToken, which netted some $3bnfrom Chinese nationals before the organisers were arrested in 2019 [864].
 ButBitcoin has a↵ected many other crime types too.
 Ransomware went up fromabout $2–3m a year to maybe $8m a year between 2001 and 2015, as Bitcoinsuddenly made ransoms easy to collect [91]; this crime type is growing steadily,although ransoms are also collected via gift cards [1190].
 By 2018, bulletproofhosting sites, which provide services to cybercriminals, were moving to cryp-tocurrency as other payment mechanisms became more di�cult [1452].
 In thatyear, the world’s largest darknet child pornography website, Welcome to Video,was closed down after its operators were traced via ﬂows of Bitcoin on theblockchain, so the pseudonymous nature of cryptocurrency has its limits [551].
In total, scams and other abuse add up to something like 3% of cryptocurrencytransaction volume directly; and in addition to the visible cryptocurrency ex-changes, there are a number of over-the-counter brokers, some 100 of which havebeen identiﬁed as involved in money laundering [401].
 The regular exchangesalso make life di�cult for law enforcement.
 Crime gangs may turn proceeds intoBitcoin through one channel, switch it into a di↵erent coin in a second country,and then send it to a third country where they get it out via bank transfer.
However, although Bitcoin uses pseudonyms, the blockchain contains a per-manent record of all transactions.
 As we’ve discussed in a number of contexts– from our chapter on inference control to the section on Tor in this chapter –anonymity is hard.
 Real-world transactions and data have context and allowSecurity Engineering629Ross Anderson20.
7.
 BLOCKCHAINSinferences to be made.
 Bitcoin users have tried all sorts of tricks to make trans-actions more anonymous, for example by splitting payments into many smallerones, mixing them up, and then recombining them – a so-called ‘tumbler’ or‘mixer’.
 However, if you do that, you taint your bitcoins with attempted moneylaundering; and in total, perhaps 10% of Bitcoin have been stolen, or passedthough a money-laundering service, at least once.
 (For an analysis, see [116].
)As an example, an Ohio man was indicted in 2020 for operating just such amixer that laundered $300m [553].
 There are also cryptocurrencies that o↵ermore privacy using further cryptographic techniques, notably Zcash and Mon-ero.
 At present, Monero o↵ers the strongest privacy and is designed so thatcoins can be mined using software; over 4% of its coins have been mined bymalware running on other people’s machines [1529].
Governments have been trying to push back using ﬁnancial regulation.
 TheUS Treasury’s Financial Crimes Enforcement Network (FinCEN) drives anti-money-laundering (AML) and know-your-customer (KYC) regulations world-wide, which get incorporated into local law, for example via the EU’s 5th Anti-money-laundering Directive.
 Some governments go further.
 For example, Ger-many’s regulator BaFin has used existing ﬁnancial regulations to insist that allexchanges get licenses; as localbitcoins.
com, a peer-to-peer exchange thatenables individuals to buy and sell cryptocurrency from each other for cash,didn’t apply for one, it is blocked there.
 But at the time of writing, the biggestpush comes from a FinCEN advisory in 2019 that required cryptocurrency ex-changes to implement the ‘travel rule’ whereby anyone handling a transactionover $10,000 has to identify both sender and recipient and ﬁle a suspicious ac-tivity report if relevant.
 The exchanges were given until June 2020 to come upwith a solution; at least one individual exchanging sums over $10,000 has beenﬁned [688].
Further regulation is on the agenda in Europe too.
Mt Gox largely hadJapanese clients while most Chinese appear to use Binance and many peoplein the UK and the USA use Coinbase.
 When one British or American usersends Bitcoin to another, there’s a fair chance that the transaction never goesnear the blockchain: if they’re both Coinbase customers, then Coinbase cansimply adjust the balances displayed in their Bitcoin wallet webpages.
Thisimmediately raises the question of why the exchanges are not regulated likeany other money service business.
In the EU, the E-money Directive mightseem to apply, yet regulators in the UK and Germany only enforce it in respectof the traditional currency balances that customers have with the exchanges;the exchanges argued that as transaction demand is much less than investmentdemand, virtual currencies should be treated as assets rather than as paymentmechanisms.
 But in that case, why does the regulator not require the exchangesto operate under the same rules as stockbrokers, so that a customer’s bitcoinscan’t be used for transactions, but merely sold back to market with the proceedsbeing sent to the bank account used to purchase them?In an analysis that colleagues and I produced of exchange operations andof the mechanics of tracing stolen Bitcoin, we also recommended applying thePayment Services Directive, which would give exchange customers consumerprotection comparable to that with banks [116].
 It is notable, for example, thatwhile banks have shown a lot of interest in how to block SIM swap attacks onSecurity Engineering630Ross Anderson20.
8.
 CRYPTO DREAMS THAT FAILEDtheir customers’ phones, most cryptocurrency exchanges have shown no interestat all – despite the fact that exchange credentials are one of the main targets ofthe SIM swap gangs [1449].
 Consumer protection in the world of cryptocurrencyis unﬁnished business, and regulatory agencies in Europe and elsewhere areworking on it.
20.
7.
6Permissioned blockchainsThe hype around cryptocurrencies and blockchains piqued commercial interest,and from about 2015, CEOs coming back from Davos told their IT departmentsthey needed a blockchain.
 The CIOs then had to explore whether blockchainscould be created that could do useful work, without Bitcoin’s environmentalwaste, illegal content and illegal actors.
 This led to initiatives such as Hyper-ledger and the Enterprise Ethereum Alliance, with corporate supporters devel-oping a variety of blockchain tools and standards.
 Many involve a permissionedblockchain fabric that is based on Byzantine fault tolerance rather than proof-of-work and can still support smart contracts.
 A number of them use SGX aspart of their consensus mechanism, such as Intel’s own proof of elapsed time(PoET) proposal.
 There are many other proposed consensus mechanisms; for asurvey, see Bano et al [165].
As an application example, JP Morgan worked on a system from 2015 thatwould enable participating banks to enter mortgages on a blockchain, so thatits scripting language would allow traders to create futures and options of arbi-trary complexity.
 They explored a number of design tradeo↵s, such as betweenlow latency and security in adversarial settings, and how transaction privacycan be extended to keep business logic private as well as the names of indi-vidual participants [1421].
 One conclusion was that for the vast majority ofapplications, you don’t need a blockchain; a forward-secure sealed log will do.
And where a blockchain might help, you can’t use a public one.
 Above all,blockchain apps must talk to legacy systems and must be no more likely to cre-ate application security mistakes or usability hazards.
 There have been enoughscrew-ups: for example, Argentina published its o�cial gazette (Boletin O�cial)on a blockchain, and decreed it to be legally valid, whereupon someone hackedit to publish fake news about the coronavirus [499].
 Such real-world experienceappears to be taming the initial exuberance of the bubble.
Perhaps the most controversial project is Libra, a Facebook proposal tocreate a payment system with its value pegged against a basket of currencies.
This was supposed to be run by a consortium of ﬁnancial, tech and other ﬁrms,but has run into signiﬁcant opposition from central banks, resulting in keyﬁnancial players such as Visa, MasterCard and PayPal pulling out.
20.
8Crypto dreams that failedA number of people have proposed electronic voting systems based on blockchainsbecause they’re supposedly immutable and you can build functionality on themusing crypto.
 These proposals follow over thirty years of research into the pos-sible use of cryptography in electronic elections to provide a system that isSecurity Engineering631Ross Anderson20.
9.
 SUMMARYsimultaneously anonymous and provably accurate.
 In fact, during the Bitcoinboom of 2017–8, a common student project proposal was ‘solving world peaceby putting elections on the blockchain’.
Election systems claiming to use a blockchain have now been deployed inboth Russia and America, with less than impressive results.
 In 2018 a systemfor three wards in the city of Moscow used an Ethereum blockchain for votetallying, but the link between vote tallying and the blockchain was broken whentwo crypto vulnerabilities were ﬁxed just before the election – and the blockchainvanished just afterwards [782].
 Also in 2018, West Virginia became the ﬁrst USstate to allow some voters to cast their ballot using a mobile phone app.
 MichaelSpecter, James Koppel and Danny Weitzner from MIT reverse engineered it andfound a number of vulnerabilities that would let an attacker expose or alter votes,despite the app’s use of a blockchain, which was irrelevant to the attacks [1810].
According to the researchers, an attacker could create a tainted paper trail,making a reliable audit impossible – despite the selling points of blockchainsincluding transparency and accountability.
The idea that blockchains can solve the problems of elections makes the expe-rienced security engineer despair.
 You can’t ﬁx elections with this technology,because it doesn’t tackle how they’re stolen.
 Parties in power are constantlychanging the rules and subverting the technology at all levels in the stack, fromvoter registration through campaign funding and advertising rules through me-dia censorship, voter intimidation and voting schemes that can be manipulated.
We’ll discuss this at greater length in section 25.
5.
20.
9SummaryStarting in the 1980s, many people have tried to use cryptography as a trustedplatform for some aspect of system security.
 The original killer app for com-mercial cryptography was the protection of PINs in ATMs and then of cardpayments more generally, as we described in Chapter 12.
 Many cryptographyresearchers (including me) then started to hope that we could solve other eco-nomic and social problems with cryptography.
Anonymous communicationswould stop censorship; anonymous digital cash would protect our privacy; digi-tal voting would make elections harder to rig; threshold signatures would help usbuild robust internal control systems; and electronic auctions would push backon corruption.
 The research papers at the Crypto and Eurocrypt conferences ofthe period are brimming with ideas like these.
 A generation later, and with atechlash of scepticism about the e↵ects of globalised technology, it may be timeto take stock.
Our case studies teach a technical point, an economic one, and a policy one.
The technical point is that cryptographic systems aren’t magic; they havebugs and have to be patched like anything else.
 Even the simplest applications,like FDE, get complex as they mature as products, and vary widely in imple-mentation quality.
 HSMs are another example of cryptosystems that acquiredever more features until the features broke them, and now require other com-ponents to block targeted attacks.
 SGX runs on processors so complex thatSecurity Engineering632Ross Anderson20.
9.
 SUMMARYit’s vulnerable to multiple side-channel attacks, and Intel doesn’t even considersome of them to be within its threat model: if a capable motivated opponentcan run their code on the same machine as you, you’re basically toast.
 Much thesame holds for blockchains, which have developed the most complex ecosystemof all.
 Even the basic assumption that rational miners are not motivated torewrite history starts to fail when applications create the necessary incentives.
Again, a cryptocurrency can go on acquiring features until they break it, andsmart contracts can help the process along.
The economic point is that the advanced crypto mechanisms we’ve seendeployed all come with a signiﬁcant cost.
 HSMs cost more than servers.
 SGX hasmemory limits and a real performance overhead on context switching.
 Bitcoinminers emit as much CO2 as New Zealand.
 Smart contracts may be able to dosome clever things but in practice are very restricted in size and scope comparedwith other software.
 There is a ﬁne calculation about whether the cost is worthit; and this calculation may become more adverse over time as the maintenancecosts mount and the system gets into technical debt.
The policy point is that advanced cryptographic mechanisms all get tangledup with liability.
 If successful they seem to acquire, as part of their core purpose,either the desire to satisfy some regulation or the desire to avoid regulation.
 Sothe decision to deploy them, or maintain them, may involve subtle externalities.
Hardware security modules are mandatory in card payment systems becauseof card scheme rules based, ultimately, on banks’ desire to not be liable forfraud.
 SGX is seen as a way to assure customers of cloud computing servicesthat they protect their most valuable assets against rogue sysadmins and againstintelligence agencies.
 Bitcoin and its many clones have become a mechanismfor circumventing everything from securities and payment law to anti-money-laundering regulations.
 Real systems get built for strategic reasons, and thattends to mean creating or entrenching power for their creators – be it marketpower or political power.
As for cryptocurrencies, they have so far had extreme volatility, limitedcapacity, unpredictable transaction costs, no governance, and limited trans-parency.
 The proof-of-work mechanisms used by most of them cause CO2 emis-sions that reasonable people might consider unacceptable, and their use in prac-tice is entangled with all sorts of criminality.
 While the law should defend theright of private ﬁrms and individuals to create value tokens such as couponsand air miles, once these start being used as currency and institutions emergethat behave like banks, it is reasonable for the lawgiver to treat them as such.
It is also reasonable for the lawgiver to think about carbon taxes, or to requireorganisations that use blockchains to account for the CO2 they produce.
If we had to sum up the experience of forty years of trying to apply the magicof mathematics to solve real-world problems, it would probably be TANSTAAFL:there ain’t no such thing as a free lunch.
Security Engineering633Ross Anderson20.
9.
 SUMMARYResearch ProblemsThere are deep problems around decentralisation that cross the boundary be-tween cryptography and system security.
 Decentralised protocols tend to fos-silise; we’re still using email, DNS and BGP mechanisms from the early 1990sbecause of the di�culty of changing anything.
 End-to-end crypto could not belayered on top of SMTP email, despite the e↵orts of PGP, but needed to waitfor a new platform like Signal that could impose it by ﬁat.
Bitcoin provides another example.
 The original cypherpunks ideal was afully decentralised payment system providing a means of exchange and a storeof value without the involvement of governments or other dominant playerssuch as banks.
Yet the production of mining rigs has become a monopoly,controlled by Bitmain, while the ASICs all come from TSMC.
 The great majorityof Bitcoin users rely on custodial exchanges to hold their cryptocurrency, andthese exchanges do most of the trading – DEXs are only 0.
01% of it.
Thecustodial exchanges have in e↵ect become unregulated banks.
In systems such as Signal, Tor and Bitcoin, the real consensus is not crypto-graphic but social; it’s the consensus of the developers.
 In Tor this is a commu-nity while in the world of cryptocurrency there are competing developer teamsworking for proﬁt.
 The security economics may be expected be more importantthan the cryptography, and we’ve already seen how smart contracts can createapplication-layer incentives that could break the underlying consensus layer.
What about the dependability of smart contracts in general? The computerscience approach to the API security problem has been to try to adapt formal-methods tools to prove that interfaces are safe.
 There is a growing literature onthis, and even a series of workshops, but the methods can still only tackle fairlysimple APIs.
 Smart contracts are running into similar problems, complicatedby the di�culty of changing them to ﬁx bugs or to respond to changing circum-stances.
 It is unsurprising that many of the smart contracts used to set up DEXshave hard-coded admin keys that enable human intervention if need be.
 This isjust prudent engineering, but calls into question the ideological justiﬁcation ofsuch exchanges as ‘trustless’.
Further ReadingTo get up to speed on Tor, a good starting point is the Tor Project’s docu-mentation page.
For more detail on how Bitcoin works, read the Princetonbook [1383] or the JEP paper [274], while for our more detailed view on tracingstolen Bitcoin and on cryptocurrency regulation, see [116].
 For a discussion ofthe interaction between centralisation and privacy, see Carmela Troncoso andcolleagues [1910].
 A survey of the state of play in messaging apps in 2015 (thetime when Signal came together from previous apps for messaging and VOIP)can be found at [1917].
Security Engineering634Ross Anderson