Chapter 29Beyond “Computer SaysNo”At the start of this century, security technology was an archipelago of mutuallysuspicious islands – the cryptologists, the operating system protection people,the burglar alarm industry, right through to the chemists who did banknote inks.
We all thought the world ended at our shore.
 By 2010, security engineeringwas an established and growing discipline; the islands were being joined upby bridges as practitioners realised we had to look beyond our comfort zones.
The banknote ink chemist who didn’t want to understand digital watermarks,and the cryptologist who could only talk about conﬁdentiality, were steadilymarginalised.
Now, in 2020, everyone needs to have a systems perspective in order to designcomponents that can be integrated usefully into real products and services.
 Andas these are used by real people, and often at global scale, our ﬁeld is embracingthe humanities and social sciences too.
Security engineering is about ensuring that systems are predictably depend-able in the face of all sorts of malice, from bombers to botnets.
 And as attacksshift from the hard technology to the people who use it, systems must also beresilient to error, mischance and even coercion.
 So a realistic understandingof people – sta↵, customers, users and bystanders – is essential; human, insti-tutional and economic factors are as important as technical ones.
 The waysin which real systems provide dependability are becoming ever more diverse,and protection goals are not just closer to the application, they can be subtleand complex.
 Conﬂicts between goals are common: where one principal wantsaccountability and another wants deniability, it’s hard to please them both.
Starting in 2001, we began to realise that many persistent security failuresare incentive failures at heart; if Alice guards a system while Bob pays the costof failure, you can expect trouble.
 This led to the growth of security economics,which the ﬁrst edition of this book helped to catalyse.
 The second edition in2008 documented how failures were also increasingly about usability, and thedecade after that saw a lot of research into security psychology.
950So what next? By way of a conclusion to this book, I’d like to highlightthree things.
First, complexity.
Computer science has spent seventy years devising animpressive array of tools to manage technical complexity, but we’re now comingup hard against social complexity.
 We can program cars to drive themselvesfairly well on the freeway or in the desert, but we can’t cope with clutteredcity streets with all those unpredictable people.
 We can encrypt messages orstrip people’s names from databases but we can’t stop social structure showingthrough.
 And bullying people has its limits; “computer says no” is a fast way tolose customers.
 It’s not enough to study how a computer system can interactwith a human; we need to ﬁgure out how it can work with many interactinghumans.
Second, sustainability.
 As we put software in everything and connect ev-erything online, we have to patch the software and maintain the servers.
 Withdurable goods like cars, pacemakers and electricity substations, we may have tomaintain software for twenty or even forty years.
 We have no real idea how todo that, and if we don’t crack it then our automation will be bad news for ourplanet’s future.
 So-called ‘smart’ devices are often just things that have to bethrown away sooner, when “computer says no”.
Third, politics.
 Security is not a scalar, but a relationship.
 It’s not somekind of magic fairy dust you sprinkle on systems, but about how these systemsexercise power.
 Who loses and who gains when “computer says no”? Does thesocial-network user get privacy, or does the advertiser get access? How is it usedto turn money into political power? And if people want public goods such as adependable Internet or a low rate of cybercrime, how can these be provided ina global world?The stability of cybercrime over a decade in which the technology has changedcompletely suggests that it’s not fundamentally about technology.
 The persis-tence of tech monopolies raises other questions about how tech and society canco-evolve, and about the nature of power.
 When Facebook becomes the arbiterof political speech, when Apple and Google can dictate policy on coronaviruscontact tracing, and when Amazon, Microsoft and Google dictate policy on fa-cial recognition (outside China), then I suspect that technology people shouldstart reading up on political science, as well as on economics and psychology.
The most intractable problems of the next ten years may be around governance.
Just as individuals can learn through experience, so our societies learn andadapt too.
 Democracy is the key mechanism for that.
 So a crucial way in whichengineers can contribute is by taking part in the policy debate.
 The more weengage in the problems that technology poses around complexity, sustainabilityand the nature of power, the faster our societies will adapt to deal with them.
Security Engineering951Ross Anderson