Chapter 27Secure SystemsDevelopmentMy own experience is that developers with a clean, expressive set ofspeciﬁc security requirements can build a very tight machine.
 Theydon’t have to be security gurus, but they have to understand whatthey’re trying to build and how it should work.
– Rick SmithWhen it comes to being slaves to fashion, American managers makeadolescent girls look like rugged individualists.
– Geo↵ NunbergThe fox knows many things; the hedgehog one big thing.
– Archilochus27.
1IntroductionSo far we’ve discussed a great variety of security applications, technologies andconcerns.
 If you’re a working engineer, manager or consultant, paid to build ormaintain a system with some security assurance requirements, you will by nowbe looking for a systematic way to go about it.
 This brings us to such topicsas risk analysis, system engineering methodology, and, ﬁnally, the secret sauce:how you manage a team to write secure code.
The secret is that there isn’t actually a secret, whether sauce or anythingelse.
 Lots of people claim there is one and get religious fervour for the passionof the moment, from the Orange Book in the 1980s to Agile Development now.
But the ﬁrst take o↵ered on this was the right one.
 In the 1960s Fred Brooksled the team on the world’s ﬁrst really large software project, the operatingsystem for the IBM S/360 mainframe.
 In his classic book “The Mythical Man-Month” he describes all the problems they struggled with, and his conclusionis that “there is no silver bullet” [328].
 There’s no magic formula that makes86827.
2.
 RISK MANAGEMENTan intrinsically hard job easy.
 There’s also the famous line from Archilochusat the head of this chapter: the fox knows many things, while the hedgehogknows one big thing.
 Managing secure development is fox knowledge ratherthan hedgehog knowledge.
 An experienced security engineering manager hasto know thousands of little things; that’s why this book is so fat!And thesecurity engineering manager’s job is getting harder all the time as softwaregets everywhere and starts to interact with safety.
In 2017, I changed the way I teach undergraduates at Cambridge.
 Up tillthen we’d taught security courses separately from software engineering, with thelatter focusing on safety.
 But most real-world systems require both, and they’reentangled in complex ways.
 Both safety and security are emergent propertiesthat really have to be baked in from the beginning.
 Both involve systematicthinking about what can go wrong, whether by accident or as a result of mal-ice.
 Accidents can expose systems to attacks, and attacks can degrade systemsso they become dangerous.
 The course was developed further by my colleagueAlastair Beresford while I was on sabbatical in 2019, and the 2020 course onSoftware and Security Engineering is now online as ten video lectures, thanksto the pandemic [89].
 That course is designed to give our ﬁrst-year undergrad-uates a solid foundation for later work in security, cryptography and softwareengineering.
 Like this book, it introduces the basics, from deﬁnitions throughthe basics of protocols and crypto, then the importance of human and organiza-tional issues as well as technical ones, illustrated with case histories.
 It discusseshow you set goals for safety and security, how you manage them as a systemevolves, and how you instil suitable ways of thinking and working into yourteam.
 Success is about attitudes and work practices as well as skills.
The two questions you have to ask are, “Are we building the right system?”and “Are we building it right?” In the rest of this chapter I’m going to startwith how we assess and manage risks – to both safety and security; and thengo on to discuss how we build systems, once we’ve got a speciﬁcation to workto.
 I’ll then discuss some of the hazards that arise as a result of organisationalbehaviour – a real but often ignored kind of insider threat.
27.
2Risk ManagementAt the heart of both safety engineering and security engineering lie decisionsabout priorities: how much to spend on protection against what.
 Risk man-agement must be done within a broader framework of managing all the risksto an enterprise or indeed to a nation.
 That is often done badly.
 The coron-avirus crisis should have made it obvious to everyone that although pandemicswere at the top of the risk register of many countries, including the UK, mostgovernments spent much more of their resilience budget on terrorism, whichwas several places down the list.
 Countries with recent experience of SARS orMERS, such as Taiwan and South Korea, did better: they were ready to testresidents and trace contacts at scale, and responded quickly.
 Britain wasted twomonths before realising the disease was serious, at a cost of tens of thousandsof lives.
So what actually is a risk register? A common methodology, as used bySecurity Engineering869Ross Anderson27.
2.
 RISK MANAGEMENTthe governing body of my university, is to draw up a list of things that couldgo wrong, giving them scores of 1 to 5 for seriousness and for probability ofoccurrence, and multiplying these together to get a number between 1 and 25.
For example, a university might rate ‘loss of research contract income due toeconomic downturn’ at 5/5 for seriousness if 20% of its income is from thatsource, and rate ‘probability’ at 4/5 as downturns happen frequently but notevery year, giving a raw product of 20.
 You then write down the measures youtake to mitigate each of these risks, and have an argument in a risk committeeabout how well each risk is mitigated.
 For example, you control the risk ofvariable research contract income by making a rule that it can be used to hireonly contract sta↵, not tenured faculty; you might then agree that this rule cutsthat risk from 20 to 16.
 You then rank all the risks in order and assign onesenior o�cer to be the owner of each of them.
National risk assessments are somewhat similar: you rate each possible badevent (pandemic, earthquake, forest ﬁre, terrorist attack, .
.
.
) by how many peo-ple it might kill (millions? thousands? dozens?) and then rate it for probabilityby how many you expect each century.
 The UK national risk register, for ex-ample, put pandemic inﬂuenza at the top, with a 5 for severity (could kill up to750,000) and a 4 for likelihood, saying in 2017: “one or more major hazards canbe expected to materialise in the UK in every ﬁve-year period.
 The most seri-ous are pandemic inﬂuenza, national blackout and severe ﬂooding” [361].
 Youthen work out what’s reasonably practical by way of mitigation, be it quaran-tine plans and PPE stockpiles for a pandemic, or building codes and zoning tolimit the damage from ﬂoods and earthquakes.
 You do the cost-beneﬁt analysisand turn priorities into policy.
 You can get things wrong in various ways.
 TheUK largely ignored pandemics because the National Security Council had beencaptured by the security and intelligence agencies; they prioritised terrorism,and the health secretary was not a regular attendee [1848].
 I already discussedterrorism in section 26.
3; here I’ll just add that another aspect of the failurewas policy overshoot.
 When 9/11 taught the world that terrorist attacks cankill thousands rather than just dozens, and the agencies got a lot more of theresilience budget, it made them greedy: they started talking up the risk of ter-rorists getting hold of a nuke so they’d have an even scarier threat on the registerto justify their budgets.
In business too you can ﬁnd that both political behaviour and organisationalbehaviour get in the way of rational risk management.
 But you often have realdata on the more common losses, so you can attempt a more quantitative ap-proach.
 The standard method is to calculate the annual loss expectancy (ALE)for each possible loss scenario, as the expected loss multiplied by the number ofincidents expected in an average year.
 A typical ALE analysis for a bank’s ITsystems might have several hundred entries, including items such as we see inFigure 27.
1.
Note that while accurate ﬁgures are likely to be available for common losses(such as ‘teller takes cash’), the incidence of low-probability high-risk losses suchas a large money-transfer fraud is largely guesswork – though you can sometimesget a rough sanity check by asking for insurance quotes.
ALEs have long been standardized by NIST as the technique to use in USgovernment procurements.
 The UK government uses a tool called CRAMM forSecurity Engineering870Ross Anderson27.
2.
 RISK MANAGEMENTLoss typeAmountIncidenceALESWIFT fraud$50,000,000.
005$250,000ATM fraud (large)$250,000.
2$100,000ATM fraud (small)$20,000.
5$10,000Teller takes cash$3,240200$648,000Figure 27.
1: – items of annualized loss expectancy (ALE)systematic analysis of information security risks, and the modern audit cultureis spreading such tools everywhere.
 But the process of producing such a tablefor low-probability threats tends to be just iterative guesswork.
 The consultantslist all the threats they can think of, attach notional probabilities, work out theALEs, add them up, and ﬁnd that the bank’s ALE exceeds its income.
 Theythen tweak the total down to whatever will justify the largest security budgetthat their client the CISO has said is politically possible.
 I’m sorry if this soundsa bit cynical; but it’s what often seems to happen.
 The point is, ALEs may beof some value, but you need to understand what parts are based on data, whatparts on guesswork and what parts on o�ce politics.
Product risks are di↵erent.
 Di↵erent industries do things di↵erently becauseof the way they evolved and the history of regulation.
 The rules for each sector,whether cars or aircraft or medical devices or railway signals, have evolvedin response to accidents and industry lobbying.
Increasingly, the EuropeanUnion is becoming the world’s safety regulator as it’s the biggest market, asWashington cares less about safety than Brussels does, and as it’s simpler forOEMs to engineer to EU safety speciﬁcations than to have multiple products.
I’ll discuss safety and security certiﬁcation in more detail in the next chapter.
For present purposes, software for cars, planes and medical devices must bedeveloped according to approved procedures, subjected to analyses we’ll discusslater, and tested in speciﬁc ways.
Insurance can be of some help in managing large but unlikely risks.
 But theinsurance business is not completely scientiﬁc either.
 Your insurance premiumsused to give some signal of the risk your business was running, especially if youbought cover for losses of eight ﬁgures or above.
 But insurance is a cyclicalindustry, and since about 2017 a host of new companies have started o↵eringinsurance against cybercrime, squeezing the proﬁts out of the market.
 As aresult, customers will no longer put up with intrusive questionnaires, let alonesite visits from assessors.
 So most insurers’ ability to assess risk is now limited;I will discuss the mechanics of what they do further in section 28.
2.
9.
 They arealso wary of correlated risks that give rise to many claims at once, as that wouldforce them to hold greater reserves; as some cyber risks are correlated, policiestend to either exclude them or be relatively expensive [275].
 (The coronaviruscrisis is teaching ﬁrms about correlated risk as some insurers refuse to pay upon business-interruption risk policies – even those that explicitly mention therisk of sta↵ not being able to get to the o�ce because of epidemics; businessesare asking insurers in turn what the point of insurance is.
)Actuarial risks aside, a very important reason for large companies to takeout insurance cover – and for much other corporate behaviour – is to protectSecurity Engineering871Ross Anderson27.
3.
 LESSONS FROM SAFETY-CRITICAL SYSTEMSexecutives, rather than shareholders.
 The risks that are being tackled may seemon the surface to be operational but are actually legal, regulatory and PR risks.
Directors demand liability insurance, and under UK and US law, professionalnegligence occurs when a professional fails to perform their responsibilities to thelevel required of a reasonably competent person in their profession.
 So negligenceclaims are assessed by the current standards of the industry or profession, givinga strong incentive to follow the herd.
 This is one reason why management issuch a fashion-driven business (as per the quote at the head of this chapter).
This spills over into the discourse used to justify security budgets.
 During themid 1980’s, everyone talked about hackers (even if their numbers were tiny).
From the late 80’s, viruses took over the corporate imagination, and people gotrich selling antivirus software.
 In the mid-1990s, the ﬁrewall became the starproduct.
 The late 1990s saw a frenzy over PKI.
 By 2017 it was blockchains.
Amidst all this hoopla, the security professional must keep a level head andstrive to understand what the real threats are.
We will return to organisational behaviour in a later section.
 First, let’s seewhat we can learn from safety engineering.
27.
3Lessons from safety-critical systemsCritical computer systems are those in which a certain class of failure is tobe avoided if at all possible.
 Depending on the class of failure, they may besafety-critical, business-critical, security-critical, or critical to the environment.
Obvious examples of the safety-critical variety include ﬂight controls and auto-matic braking systems.
 There’s a large literature on this subject, and a lot ofmethodologies have been developed to help manage risk intelligently.
27.
3.
1Safety engineering methodologiesSafety engineering methodologies, like classical security engineering, tend towork systematically from a safety analysis to a speciﬁcation through to a prod-uct, and assume you’re building safety in from the start rather than trying toretroﬁt it.
 The usual procedure is to identify hazards and assess risks; decideon a strategy to cope with them (avoidance, constraint, redundancy .
.
.
); tracethe hazards to hardware and software components which are thereby identiﬁedas critical; identify the operator procedures which are also critical and studythe various applied psychology and operations research issues; set out the safetyfunctional requirements which specify what the safety mechanisms must do, andsafety integrity requirements that specify the likelihood of a safety function be-ing performed satisfactorily; and ﬁnally decide on a test plan.
 The outcome oftesting is not just a system you’re conﬁdent to run live, but an integrated part ofa safety case to justify running it.
 The basic framework is set out in standardssuch as ISO 61508, a basic safety framework for relatively simple programmableelectronics such as the control systems for chemical plants.
 This has been ex-tended with more specialised standards for particular industries, such as ISO26262 for road vehicles.
This safety case will provide the evidence, if something does go wrong, thatSecurity Engineering872Ross Anderson27.
3.
 LESSONS FROM SAFETY-CRITICAL SYSTEMS•�•�•�•�M�(a)�(b)�M�Figure 27.
2: – hazard elimination in motor reversing circuityou exercised due care.
It will typically consist of the hazard analysis, thesafety functional and integrity requirements, and the results of tests (both atcomponent level and system level) which show that the required failure rateshave been achieved.
 The testing may have to be done by an accredited thirdparty; with motor vehicles ﬁrms get away with the safety case being done bya di↵erent department in the same company, with independent management.
Vehicles are a more complex case because of their supply chains.
 At the top isthe brand, whose badge you see on the front of the car.
 Then there’s the originalequipment manufacturer (OEM) which in the case of cars is usually the samecompany, but not always; in other industries the brand and the OEM are quiteseparate.
 A modern car will have components from dozens of manufacturers,of which the Tier 1 suppliers who deal directly with the brand do much of theresearch and development work but get components from other ﬁrms in turn.
In the car industry, the brand puts the car through type approval and carriesthe primary liability, but demands indemnities from component suppliers in casethings go wrong (the law in most countries does not allow you to disclaim liabilityfor death and injury).
 The brand relies on the supply chain for signiﬁcant partsof the safety functionality and integrity and thus for the safety case.
 There arealso tensions: as we already noted, safety certiﬁcation can prevent the timelyapplication of security patches.
 Let’s now look at common safety engineeringmethods and what they can teach us.
27.
3.
2Hazard analysisIn an ideal case, we might be able to design hazards out of a system completely.
As an example, consider the motor reversing circuits in Figure 27.
2.
In thedesign on the left, a double-pole double-throw switch reverses the current passingfrom the battery through the motor.
 However, this has a potential problem: ifonly one of the two poles of the switch moves, the battery will be shorted anda ﬁre may result.
 The solution is to exchange the battery and the motor, as inthe modiﬁed circuit on the right.
 Here, a switch failure will only short out themotor, not the battery.
 Safety engineering is not just about correct operation,but about correct failure too.
Hazard elimination is useful in security engineering too.
 We saw an examplein the early design of SWIFT in section 12.
3.
2: there, the keys used to authen-ticate transactions between one bank and another were exchanged between theSecurity Engineering873Ross Anderson27.
3.
 LESSONS FROM SAFETY-CRITICAL SYSTEMSSuccessful card forgery�Shoulder�surfing�Cryptanalysis of DES�False�terminal�attack�Abuse of�security�module�Trojan�Theft of�keys�Bank insider�Maintenance�  contractor�Bug in�ATM�Encryption�replacement�Falsify�auth�response�Protocol failure�Figure 27.
3: – a threat treebanks directly, so SWIFT did not have the means to forge a valid transactionand its sta↵ and systems had to be trusted less.
 In general, minimizing thetrusted computing base is an exercise in hazard elimination.
 The same appliesin privacy engineering too.
 For example, if you’re designing a contact tracingapp to monitor who might have infected whom in an epidemic, one approach isto have a central database of everyone’s mobile phone location history.
 Howeverthat has obvious privacy hazards, which can be reduced by keeping a Bluetoothcontact history on everyone’s mobile phone instead, and uploading the contacthistory of anyone who calls in sick.
 You then have a policy decision to takebetween better privacy and better tracing.
27.
3.
3Fault trees and threat treesOnce you have eliminated as many hazards as possible, the next step is to iden-tify failures that could cause accidents.
 A common top-down way of identifyingthe things that can go wrong is fault tree analysis where a tree is constructedwhose root is the undesired behavior and whose successive nodes are its possiblecauses.
 This top-down approach is natural where you have a complex systemwith a small number of well-known bad outcomes that you have to avoid.
 Itcarries over in a natural way to security engineering.
 Figure 27.
3 shows an ex-ample of a fault tree (or threat tree, as it’s often called in security engineering)for fraud from automatic teller machines.
Threat trees are used in the US Department of Defense.
 You start out fromeach undesirable outcome, and work backwards by writing down each possibleimmediate cause.
 You then recurse by adding each precursor condition.
 Byworking round the tree’s leaves you should be able to see each combinationof technical attack, operational blunder, physical penetration and so on whichcould break your security policy.
 The other nice thing you get from this is avisualisation of commonality between attack paths, which makes it easier toreason about how to disrupt the most attacks with the least e↵ort.
 In someSecurity Engineering874Ross Anderson27.
3.
 LESSONS FROM SAFETY-CRITICAL SYSTEMSvariants, attack branches have countermeasure sub-branches, which may havecounter-countermeasure attack branches, and so on, in di↵erent colours for em-phasis.
 A threat tree can amount to an attack manual for the system, so it maybe highly classiﬁed, but it’s a DoD requirement – and if the system evaluatorsor accreditors can ﬁnd signiﬁcant extra attacks, they may fail the product.
27.
3.
4Failure modes and e↵ects analysisReturning to the safety-critical world, another way of doing hazard analysisis failure modes and e↵ects analysis (FMEA), pioneered by NASA, which isbottom-up rather than top-down1.
 This involves tracing the consequences of afailure of each of the system’s components all the way up to the e↵ect on themission.
 This is the natural approach in systems with a small number of well-understood critical components or subsystems, such as aircraft.
 For example, ifyou’re going to ﬂy a plane over an ocean or mountains where you can’t glide to anairport in the case of engine failure, then engine power is critical.
 You thereforestudy the mean time to failure of your powerplant and its failure modes, froma broken connecting rod to running out of fuel.
 You insist that single-engineaircraft use reliable engines and you regulate the maintenance schedules; planeshave more than one fuel tank.
 When carrying a lot of passengers, you insist onmulti-engine aircraft and drill the crews to deal with engine failure.
An aerospace example of people missing a failure mode that turned out tobe critical is the 1986 loss of the space shuttle Challenger.
The O-rings inthe booster rockets were known to be a risk by the NASA project manager, anddamage had been found on previous ﬂights; meanwhile the contractor knew thatlow temperatures increased the risk; but the concerns did not come togetheror get through to NASA’s top management.
 An O-ring, made brittle by thecold, failed – causing the loss of the shuttle and seven crew.
 On the resultingboard of inquiry, the physicist Richard Feynman famously demonstrated thison TV by putting a sample of O-ring in a clamp, freezing it in iced water andthen showing that when he released it, it remained dented and did not springback [1612].
 This illustrates that failures are often not just technical but alsoinvolve how people behave in organisations: when protection mechanisms crossinstitutional boundaries, as for example with cars, you need to think of the lawand economics as well as just the engineering.
 Such problems will become muchmore complex as we move towards autonomous vehicles, which will rely on allsorts of third-party services and infrastructure.
27.
3.
5Threat modellingBoth fault trees and FMEA depend on the analyst understanding the systemreally well; they are hard to automate, not fully repeatable and can be up-endedby a subtle change to a subsystem.
 So a thorough analysis of failure modes willoften combine top-down and bottom-up approaches with some methods speciﬁc1FMEA is bottom-up in the technical sense that the analysis works up from individualcomponents, but its actual management often has a top-down ﬂavour as you start work onthe safety case once you have an outline design and reﬁne it progressively as the design isevolved into a product.
Security Engineering875Ross Anderson27.
3.
 LESSONS FROM SAFETY-CRITICAL SYSTEMSto the application that people have learned over time.
 Many industries now haveto rethink their traditional safety analysis methods to incorporate security.
In car safety, complex supply chains mean we have to do multiple interlockinganalyses of vehicles and their subsystems.
A traditional subsystem analysismight work through the failure modes of headlamps, since losing them whiledriving at night can lead to an accident.
 As well as mitigating the risk of alamp failure by having two or more lamps, you worry about switch failure, andwhen the switch becomes electronic you build a fault tree of possible hardwareand software faults.
When we extend this from safety to security, we thinkabout whether an attacker might take over the entertainment system in a car,and use it to send a malicious ‘lamp o↵’ message on the CAN bus once the caris moving quickly enough for this to be dangerous.
 This analysis may lead to adesign decision to have a ﬁrewall between the cabin CAN bus and the powertrainCAN bus.
 (This is the worked example in the new draft ISO 21434 standardfor cybersecurity in road vehicles [962].
)More generally, the shift from safety to security means having to think sys-tematically about insiders.
 Just as double-entry bookkeeping was designed tobe resilient against a single dishonest clerk and has been re-engineered againstthe similar threat of a clerk with malware on their PC, so modern large-scalesystems are typically designed to limit the damage if a single component is com-promised.
 So how can you incorporate malicious insiders into a threat model? Ifyou’re using FMEA, you can just add an opponent at various locations, as withour malicious ‘lamp o↵’ message.
 As for more complex systems, the methodol-ogy adopted by Microsoft following its big push in 2003 to make Windows andO�ce more secure is described by Frank Swiderski and Window Snyder [1851].
Rather than being purely top-down or bottom-up, this is a meet-in-the-middleapproach.
 The basic idea is that you list not just the assets you’re trying toprotect (ability to do transactions, access to conﬁdential data, whatever) butalso the assets available to an attacker (perhaps the ability to subscribe to yoursystem, or to manipulate inputs to the smartcard you supply him, or to get ajob at your call center).
 You then trace the attack paths through the system,from one module to another.
 You try to ﬁgure out what the trust levels mightbe; where the barriers are; and what techniques, such as spooﬁng, tampering,repudiation, information disclosure, service denial and elevation of privilege,might be used to overcome particular barriers.
 The threat model can be usedfor various purposes at di↵erent points in the security development lifecycle,from architecture reviews through targeting code reviews and penetration tests.
There are various ways to manage the resulting mass of data.
 An elemen-tary approach is to construct a matrix of hazards against safety mechanisms,and if the safety policy is that each serious hazard must be constrained by atleast two independent mechanisms, then you can check for two entries in eachof the relevant columns.
 So you can demonstrate graphically that in the pres-ence of the hazard in question, at least two failures will be required to cause anaccident.
 An alternative approach, system theoretic process analysis (STPA),starts o↵ with the hazards and then designs controls in a top-down process,leading to an architectural design for the system; this can be helpful in teasingapart interacting control loops [1150].
 Such methodologies go across to secu-rity engineering [1556].
 One way or another, in order to make the complexitySecurity Engineering876Ross Anderson27.
3.
 LESSONS FROM SAFETY-CRITICAL SYSTEMSmanageable, you may have to organise a hierarchy of safety and security goals.
The security policies discussed in Part II of this book may give you the begin-nings of an answer for the applications we discussed there, and some inspirationfor others.
 This hierarchy can then drive a risk matrix or risk treatment plandepending on the terminology in use in your industry.
27.
3.
6Quantifying risksThe safety-critical systems community has a number of techniques for dealingwith failure and error rates.
 Component failure rates can be measured statisti-cally; the number of bugs in software can be tracked by techniques I’ll discuss inthe next chapter; and there is a lot of experience with the probability of opera-tor error at di↵erent types of activity.
 The bible for human-factors engineeringin safety-critical systems is James Reason’s book ‘Human Error’; I discussed inChapter 3 the rising tide of research in security usability through the 2010s asthe lessons from the safety world have started to percolate into our ﬁeld.
The error rate in a task depends on its familiarity and complexity, theamount of pressure and the number of cues to success.
 Where a task is simple,performed often and there are strong cues to success, the error rate might be1 in 100,000 operations.
 However, when a task is performed for the ﬁrst timein a confusing environment where logical thought is required and the operatoris under pressure, then the odds can be against successful completion.
 ThreeMile Island and Chernobyl taught nuclear engineers that no matter how manydesign walkthroughs you do, it’s when the red lights go on for real that theworst mistakes get made.
 The same lesson has come out of one air accidentinvestigation after another.
 When dozens of alarms go o↵ at once, there’s a fairchance that someone will push the wrong button.
 One guiding principle is todefault to a safe state: to damp down a nuclear reaction, to return an aircraftto straight and level ﬂight, or to bring an autonomous vehicle to a stop at theside of the road.
 No principle is foolproof, and a safe state may be hard tomeasure.
 A vehicle can ﬁnd it hard to tell where the side of the road is if there’sa grass verge; and in the Boeing 737Max crashes (which I describe in detail insection 28.
2.
4) the ﬂight control computer tried to keep the plane level but wasconfused by a faulty angle-of-attack sensor and dived the plane into the groundinstead.
Another principle of safety usability in an emergency is to keep the infor-mation given to operators, and the controls available for them to use, bothsimple and intuitive.
 In the old days, each feed went to a single gauge or dialand there was only so much space for them.
 The temptation nowadays is togive the operator everything, because you can.
 In the old days, designers knewthat an emergency would give the pilots tunnel vision so they put the six in-struments they really needed right in the middle.
 Nowadays there can be ﬁftyalarms rather than two and pilots struggle to work out which screen on whichmenu of the electronic ﬂight information system to look at.
 It is much broaderthan aviation.
 A naval example is the 2017 collision of the USS McCain in theStraits of Singapore, where UI confusion was a major factor.
 Steering controlwas shifted to the wrong helm station and an engine was not throttled back intime, resulting in an uncommanded turn to port across a busy shipping lane,Security Engineering877Ross Anderson27.
3.
 LESSONS FROM SAFETY-CRITICAL SYSTEMSimpact with a chemical tanker, and the death of ten sailors [1929].
So systems that are not fully autonomous must remain controllable, and forthat the likely human errors need to be understood.
 Quite a lot is known aboutthe cognitive biases and other psychological factors that make particular types oferror more common; we discussed them in Chapter 3, and a prudent engineer willstudy how they work out in their ﬁeld.
 Errors are rare in frequently-performedtasks at which the operator has developed some skill, and are more likely whenoperators are stressed and surprised.
 This starts to get us out of the territoryof risk, where the odds are known, and into that of uncertainty, where they’renot.
In security systems, too, the most egregious blunders can be expected inimportant but rarely performed tasks.
 Security usability isn’t just about pre-senting a nice intuitive interface to the end-user.
 It should present the risks ina way that accords with common mental models of threat and protection, andthe likely user reactions to stress should lead to safe outcomes.
It is important to be realistic about the skill level of the people who willperform each critical task and any known estimates of the likelihood of error.
An airplane designer can rely on a predictable skill level from anyone with acommercial pilot’s license, and a shipbuilder knows the strengths and weaknessesof an o�cer in the merchant marine.
 Cars can and do get operated by driverswho are old and frail, young and inexperienced, distracted by passengers, orunder the inﬂuence of alcohol.
 At the professional end of things, usability testingcan be proﬁtably integrated with sta↵ training: when pilots go for their refreshercourses in the simulator, instructors throw all sorts of combinations of equipmentfailure, bad weather, cabin crisis and air-tra�c-control confusion at them.
 Theyobserve what combinations of stress result in fatal accidents, and how these di↵eracross cockpit types.
 Such data are valuable feedback to cockpit designers.
 Inaviation, the incentives for safe operation are su�ciently strong and well aligned,and the scale is large enough, to support a learning system.
 Even so, there areexpensive disasters, such as the Boeing 737Max ﬂight control software This notonly had at least one serious bug, but escaped a proper failure modes and e↵ectsanalysis because the engineers responsible – under pressure from their managersto complete the project on time – wrongly assumed that pilots would be ableto cope with any failure [89].
 As a result, the software relied on a single angle-of-attack sensor rather than using the two sensors with which the aircraft wasﬁtted, and sensor failure led to fatal accidents2.
When testing the usability of redundant systems, you need to pay attentionto fault masking: if the output is determined by majority voting between threeprocessors, and one of them fails, then the system will continue to work ﬁne– but its safety margin will have been eroded, perhaps in ways the operatorswon’t understand properly.
Several air crashes have resulted from ﬂying anairliner with one of the cockpit systems out of action; although pilots maybe intellectually aware that one of the data feeds to the cockpit displays isunreliable, they may rely on it under pressure by reﬂex rather than checkingwith other instruments.
 So you have to think hard about how faults can remain2Aviation safety standards such as DO178 and DO254 generally require diversity in mea-surement type, physics, processing characteristics in addition to redundancy to mitigatecommon-mode failures.
Security Engineering878Ross Anderson27.
4.
 PRIORITISING PROTECTION GOALSvisible and testable even when their immediate e↵ects are mitigated.
Another lesson from safety-critical systems is that although a safety require-ments speciﬁcation and test criteria will be needed as part of the safety case forthe lawyers and regulators, it is good practice to integrate both of them withthe mainstream product documentation.
 If the safety case is separate, then it’seasy to sideline it after approval and fail to maintain it properly.
 (This was afactor in the Boeing 737Max disaster as the usability assumptions underlyingthe safety case for the ﬂight control software were not updated from the pre-vious model of 737.
) The move from project-based software management toagile methodologies, and via DevOps to DevSecOps, is ﬁnally starting to embedsecurity management into the way products evolve.
 We will discuss this in thenext section.
Finally, safety is like security in that it really has to be built in as a systemis developed, rather than retroﬁtted.
 The main di↵erence between the two isin the failure model.
 Safety deals with the e↵ects of random failure, while insecurity we assume a hostile opponent who can cause some of the componentsof our system to fail at the least convenient time and in the most damaging waypossible.
 People are naturally more risk-averse in the presence of an adversary;I will discuss this in section 28.
4.
 A safety engineer will certify a critical ﬂight-control system with an MTBF of 109 hours; a security engineer has to worrywhether an adversary can force the preconditions for that one-in-a-billion failureand crash the plane on demand.
In e↵ect, our task is to program a computer that gives answers which aresubtly and maliciously wrong at the most inconvenient moment possible.
 I’vedescribed this as ‘programming Satan’s computer’ to distinguish it from themore common problem of programming Murphy’s [113].
This is one of thereasons security engineering is hard: Satan’s computer is harder to test [1668].
27.
4Prioritising protection goalsIf you’ve a project to create an entirely new product, or to radically change anexisting one, it’s an idea to spend some time thinking through the protectionpriorities from ﬁrst principles.
A careful safety analysis or threat modellingexercise can provide some numbers to inform this.
 When developing a safetycase or a security policy in detail, it’s essential to understand the context, andmuch of this book has been about the threat models relevant to a wide rangeof applications.
 You should try to reﬁne numerical estimates of risk from theenvironment or context as well.
In the case of a business system, analysis will hinge on the tradeo↵ betweenrisk and reward.
 Security people often focus too much on the former.
 If yourﬁrm has a turnover of $10m, gross proﬁts of $1m and theft losses of $150,000,you might make a loss-reduction pitch about ‘how to increase proﬁts by 15%by stopping theft’; but if you could double the turnover to $20m, then theshareholders would prefer that even if it triples the losses to $450,000.
 Proﬁtis now $1.
55m, up 85%, rather than 15%.
 This is borne out by the experienceof online fraud engines.
 When discussing fraud management strategies with aSecurity Engineering879Ross Anderson27.
4.
 PRIORITISING PROTECTION GOALSnumber of retailers, I noticed that the ﬁrms who got the best results were thosewhere the fraud management team reported to sales rather than ﬁnance.
 Atypical bricks-and-clicks retailer in the UK might decline something like 4% ofo↵ered shopping baskets because the fraud engine alerts at the combination ofgoods, delivery address and payment details.
 So if you can improve the fraudengine and reject only 3%, that’s 1% more sales – a prospect to light up yourChief Marketing O�cer’s eyes.
 But if the fraud team reports instead to theChief Financial O�cer, they’re likely to be seen as a cost rather than as anopportunity.
Similarly, the site reliability engineers of online services have learned notto make a system too reliable.
 If local Internet availability is only 99%, thena service that’s up 99.
9% of the time will be ﬁne; there’s no point spendingmillions more to hit 99.
99% if none of your users will notice the di↵erence.
You’re better o↵ deliberately setting an 0.
1% error budget which you can useproductively, such as by causing occasional deliberate failures to exercise yourresilience mechanisms [236].
This brings me to one of the open debates insecurity management: should one aim at having no CVEs open in any of thesoftware on which one relies? The tick-box approach is to say ‘Of course theremust be no open CVEs’, but that may impose a rather high compliance cost.
If you’re Google, and wrote all your own infrastructure, maybe you can aim atthat; many ﬁrms can’t and have to prioritise.
 I’ll discuss CVEs in more detailin section 27.
5.
7.
1 later.
So don’t trust people who can only talk about ‘tightening security’.
 Oftenit’s too tight already, and what you really need to do is just focus it slightlydi↵erently.
 In the ﬁrst edition of this book, I presented a case study of self-servicecheckout at supermarkets.
 Twenty years ago, a number of supermarkets startedto introduce self-checkout lanes.
 Some started to obsess about losses, and letsecurity get in the way of usability by aggressively challenging customers aboutproduct weight.
 One of the stores that got an advantage started with a moreforgiving approach which they tuned up gradually in the light of experience.
Eventually the industry ﬁgured out how to operate self-checkout lanes, butthe quality of the implementation still varies signiﬁcantly.
 By early 2020, thepioneers are small convenience stores like Lifvs in Sweden which have no sta↵;you open the store’s door with an app, scan your purchases and pay online.
Amazon was also experimenting with fully self-service food stores.
 We saw thenext 20 years of innovation crammed into the few months of the 2020 coronaviruslockdown; by June, other supermarkets have been urging us to download theirscanning app, scan our purchases as we pick them, charge them to a card, andjust go.
Many modern business models were once considered too risky, starting withthe self-service supermarket itself back in the days when grocers kept all thegoods behind the counter.
 Everyone thought Richard Sears would go bust whenhe adopted the slogan ‘Satisfaction guaranteed or your money back’ in the 1880s,yet he invented the modern mail-order business.
 In business, proﬁt is the rewardfor risk.
 But entrepreneurs who succeed may have to improve security quickly.
One recent example is the videoconferencing platform Zoom – which grew from20 million users to 200 million in March 2020, and changed in the process froman enterprise platform into something more like a public utility – forcing themSecurity Engineering880Ross Anderson27.
5.
 METHODOLOGYinto a major security engineering e↵ort [1763].
Trade-o↵s in safety are harder.
 Logically, the value of a human life in a de-veloped country might be a few million dollars, that being an average person’slifetime earnings.
 However our actual valuation of a human life as revealed bysafety behaviour varies from about $50,000 for improvements to road junctions,up to over $500m for train protection systems – and that’s just in the context oftransport policy.
 The variance in health policy is even greater, with costs per lifesaved ranging from a few hundred dollars for ﬂu jabs and some cancer screeningto billions for the least e↵ective interventions [1869]; in other safety contexts, do-mestic smoke alarms cost a few hundred dollars per life saved while the numberfor the “war on terror” is in the billions [1350].
 The reasons for this irrational-ity are fairly well understood – I discussed the psychology in section 3.
2.
5 andthe policy aspects in 26.
3.
3.
 Safety preferences can be changed very sharply bythe threat of hostile action; people may completely ignore a 1-in-10,000 risk ofbeing killed by poorly-designed medical devices until there’s a possibility thatthe devices might be hacked, at which point even a 1-in-10,000,000 risk becomesscary.
 I discuss this phenomenon in section 28.
4.
27.
5MethodologySoftware projects usually take longer than planned, cost more than budgetedand have more bugs than expected3.
 By the 1960s, this had become known asthe software crisis, although the word ‘crisis’ may be inappropriate for a stateof a↵airs that has now lasted, like computer insecurity, for two generations.
Anyway, the term software engineering was proposed by Brian Randall in 1968and deﬁned to be:Software engineering is the establishment and use of sound engineer-ing principles in order to obtain economically software that is reliableand works e�ciently on real machines.
The pioneers hoped that the problem could be solved in the same way webuild ships and aircraft, with a foundation in basic science and a framework ofdesign rules [1420].
 Since then there’s been a lot of progress, but the resultshave been unexpected.
 Back in the late 1960s, people hoped that we’d cut thenumber of large software projects failing from the 30% or so that was observedat the time.
 But we still see about 30% of large projects failing – the di↵erence isthat the failures are much bigger.
 Modern tools get us farther up the complexitymountain before we fall o↵, but the rate of failure is set by company managers’appetite for risk.
We’ll discuss this further in the section on organisationalbehaviour at the end of this chapter.
Software engineering is about managing complexity, of which there are twokinds.
 There is the incidental complexity involved in programming using inap-propriate tools, such as the assembly languages which were all that some earlymachines supported; programming a modern application with a graphical userinterface in such a language would be impossibly tedious and error-prone.
 There3This is sometimes known as “Cheops’ law” after the builder of the Great Pyramid.
Security Engineering881Ross Anderson27.
5.
 METHODOLOGYis also the intrinsic complexity of dealing with large and complicated problems.
A bank’s core systems, for example, may involve tens of millions of lines of codethat implement hundreds of di↵erent products sold through several di↵erentdelivery channels, and are just too much for any one person to understand.
Incidental complexity is largely dealt with using technical tools.
 The mostimportant are high-level languages that hide much of the drudgery of dealingwith machine-speciﬁc detail and enable the programmer to develop code at anappropriate level of abstraction.
 They bring their own costs; many vulnerabili-ties are the result of the properties of the C language, and if we were rerunninghistory we’d surely use something like Rust instead.
 There are also formal meth-ods such as static analysis tools, that enable particularly error-prone design andprogramming tasks to be checked.
Intrinsic complexity requires something subtly di↵erent: methodologies thathelp us divide up a problem into manageable subproblems and restrict the extentto which these subproblems can interact.
 These in turn are supported by theirown sets of tools.
 There are basically two approaches – top-down and iterative.
27.
5.
1Top-down designThe classical model of system development is the waterfall model formalisedby Win Royce in the 1960s for the US Air Force [1628].
 The idea is that youstart from a concise statement of the system’s requirements; elaborate this intoa speciﬁcation; implement and test the system’s components; then integratethem together and test them as a system; then roll out the system for liveoperation.
 From the 1970s until the mid-2000s, this was how all systems for theUS Department of Defense were supposed to be developed, and their lead wasfollowed by many governments worldwide, including not just in defence but inadministration and healthcare.
 When I worked in banking in the 1980s, it wasthe approved process there too, promoted assiduously by IBM, by governmentsand by the big accountancy ﬁrms.
The idea is that the requirements are written in the user language, thespeciﬁcation is written in technical language, the unit testing checks the unitsagainst the speciﬁcation and the system testing checks whether the requirementsare met.
 At the ﬁrst two steps in this chain there is feedback on whether we’rebuilding the right system (validation) and at the next two on whether we’rebuilding it right (veriﬁcation).
 There may be more than four steps: a commonelaboration is to have a sequence of reﬁnement steps as the requirements aredeveloped into ever more detailed speciﬁcations.
 But that’s by the way.
The deﬁning feature of the waterfall model is that development ﬂows inex-orably downwards from the ﬁrst statement of the requirements to the deploy-ment of the system in the ﬁeld.
 Although there is feedback from each stage toits predecessor, there is no system-level feedback from (say) system testing tothe requirements.
There is a version used in safety-critical systems development called the Vmodel, where the system ﬂows down to implementation, then climbs back up ahill of veriﬁcation and validation on the other side, where it’s tested successivelyagainst the implementation, the speciﬁcation and the requirements.
 This is aSecurity Engineering882Ross Anderson27.
5.
 METHODOLOGYRequirements�Validate�Specification�Validate�Implementation�& unit testing�Verify�Integration &�system testing�Verify�Refine�Code�Build�Field�Operations &�maintenance�Figure 27.
4: – the waterfall modelGerman government standard, and also used in the aerospace industry world-wide; it’s found in the ISO 26262 standard for car software safety.
 But althoughit’s written from left to right rather than top-down, it’s still a one-way processwhere the requirements drive the system and the acceptance test ensures thatthe requirements were met, rather than a mechanism for evolving the require-ments in the light of experience.
 It’s more a di↵erent diagram than a di↵erentanimal.
The waterfall model had a precursor in a methodology developed by GerhardPahl and Wolfgang Beitz in Germany just after World War 2 for the design andconstruction of mechanical equipment such as machine tools [1490]; apparentlyone of Pahl’s students later recounted that it was originally designed as a meansof getting the engineering student started, rather than as an accurate descriptionof what experienced designers actually do.
 Win Royce also saw his model asa means of starting to get order out of chaos, rather than as the prescriptivesystem it developed into.
The strengths of the waterfall model are that it compels early clariﬁcationof system goals, architecture, and interfaces; it makes the project manager’stask easier by providing deﬁnite milestones to aim at; it may increase costtransparency by enabling separate charges to be made for each step, and forany late speciﬁcation changes; and it’s compatible with a wide range of tools.
Where it can be made to work, it’s often the best approach.
 The critical questionis whether the requirements are known in detail in advance of any developmentor prototyping work.
 Sometimes this is the case, such as when writing a compileror (in the security world) designing a cryptographic processor to implement aknown transaction set and pass a certain level of evaluation.
 Sometimes a top-down approach is necessary for external reasons, as with an interplanetary spaceprobe where you’ll only get one shot at it.
Security Engineering883Ross Anderson27.
5.
 METHODOLOGYBut very often the detailed requirements aren’t known in advance and aniterative approach is necessary.
 The technology may be changing; the environ-ment could be changing; or a critical part of the project may be the design ofa human-computer interface, which will probably involve testing several proto-types.
 Very often the designer’s most important task is to help the customerdecide what they want, and although this can sometimes be done by discussion,there will often be a need for some prototyping.
Sometimes a formal project is just too slow.
 Reginald Jones attributes muchof the UK’s relative success in electronic warfare in World War 2 to the fact thatBritish scientists hacked stu↵ together quickly, while the Germans used a rigidtop-down development methodology, getting beautifully engineered equipmentbut always six months too late [990].
But the most common reason for using iterative development is that we’restarting from an existing product that we want to improve.
 Even in the earlydays of computing, most programmer e↵ort was always expended on maintain-ing and enhancing existing programs rather than developing new ones; surveyssuggest that 70–80% of the total cost of ownership of a successful IT product isincurred after it ﬁrst goes into service, even when a waterfall methodology wasused [2060].
 Nowadays, as software becomes a matter of embedded code, appsand cloud services which all become ever more complex, the reality in manyﬁrms is that ‘the maintenance is the product’.
Even in the late 1990s, when the most complex human artefacts were soft-ware packages such as Microsoft O�ce, the only way to write such a thing wasto start o↵ from the existing version and enhance it.
 That does not make thewaterfall model obsolete; on the contrary, it is often used to manage a project todevelop a major new feature, or to refactor existing code.
 However, the overallmanagement of a major product nowadays is likely to be based on iteration.
27.
5.
2Iterative design: from spiral to agileThere are di↵erent ﬂavours of iterative development, ranging from a rapid pro-totyping exercise to ﬁrm up the speciﬁcation of a new product, through to amanaged process for ﬁxing or enhancing an existing system.
In the ﬁrst case, one approach is the spiral model in which developmentproceeds through a pre-agreed number of iterations in which a prototype isbuilt and tested, with managers being able to evaluate the risk at each stageso they can decide whether to proceed with the next iteration or to cut theirlosses.
 Devised by Barry Boehm, it’s called the spiral model because the processis often depicted as in Figure 27.
5.
 There are many applications where an initialprototype is the key ﬁrst step; from a startup aiming to produce a demo to showto investors, through a company building a mockup of a new product to showa focus group, to DARPA seedling projects that aim to establish that someproposed technology isn’t completely impossible.
Prototype applications forthe security engineer range from security usability testbeds to proof-of-conceptattack code.
 The key is to solve the worst problem you’re facing, so as to reducethe project risk as much as possible.
The second case we now describe as agile development, which may be summedSecurity Engineering884Ross Anderson27.
5.
 METHODOLOGYProgress�Prototype�#2�Prototype�#1�Development�plan�Risk�analysis�Product�design�Settle final design�Code�Test system�Ship�Commit�Test�Figure 27.
5: – the spiral modelup in the slogan: “Solve your worst problem.
 Repeat”.
An early advocate for an evolutionary approach was Harlan Mills, whotaught that you should build the smallest system that works, try it out onreal users, and then add functionality in small increments.
This is how thepackaged software industry had learned to work by the 1990s: as PCs becamemore capable, software products became so complex that they could not beeconomically developed (or redeveloped) from scratch.
 Indeed, Microsoft triedmore than once to rewrite Word, but gave up each time.
 A landmark earlybook on evolutionary development was ‘Debugging the Development Process’by Steve Maguire of Microsoft in 1994 [1209].
 In this view of the world, productsaren’t the result of a project but of a process that involves continually modi-fying previous versions.
 Microsoft contrasted its approach with that of IBM,then still the largest IT company; in the IBM ecosystem, the waterfall approachwas dominant.
 (IBMers for their part decried Microsoft as a bunch of undis-ciplined hackers who produced buggy, unreliable code; but IBM’s near-deathexperience after Microsoft stole their main business markets has been ascribedto the rigidity of the IBM approach to development [390].
) Professional practicehas evolved in the quarter century since then, and evolutionary development isnow known as ‘agile’, but it is recognisably the same beast.
A key insight about evolutionary development is that just as each genera-tion of a biological species has to be viable for the species to continue, so eachgeneration of an evolving software product must be viable.
 The core technologyis regression testing.
 At regular intervals – typically once a day – all the teamsworking on di↵erent features of a product check in their code, which gets com-piled to a build that is then tested automatically against a large set of inputs.
The regression test checks whether things that used to work still work, and thatold bugs haven’t found their way back.
 It’s always possible that someone’s codebroke the build, so we consider the current ‘generation’ to be the last build thatworked.
 Things are slightly more complex when systems have to work together,as when an app has to talk to a cloud service, or when several electronic com-ponents in a vehicle have to work together, or where a single vehicle componenthas to be customised to work in several di↵erent vehicles.
 You can end up withSecurity Engineering885Ross Anderson27.
5.
 METHODOLOGYa hierarchy of builds and test regimes.
 But one way or another, we always haveviable code that we can ship out for beta testing, or whatever the next stage ofour process might be.
The technology of testing was probably the biggest practical improvementin software engineering during the 1990s and early 2000s.
 Before automatedregression tests were widely used, IBM engineers used to reckon that 15% ofbug ﬁxes either introduced new bugs or reintroduced old ones [18].
 The move toevolutionary development was associated with a number of other changes.
 Forexample, IBM had separated the roles of system analyst, programmer and tester;the analyst spoke to the customer and produced a design, which the programmercoded, and then the tester looked for bugs in the code.
 The incentives weren’tquite right, as the programmer could throw lots of buggy code over the fenceand hope that someone else would ﬁx it.
 This was slow and led to bloated code.
Microsoft abolished the distinction between analysts, programmers and testers;it had only developers, who spoke to the customer and were also responsiblefor ﬁxing their own bugs.
 This held up the bad programmers who wrote lotsof bugs, so that more of the code was produced by the more skilful and carefuldevelopers.
 According to Steve Maguire, this is what enabled Microsoft to winthe battle to rule the world of 32-bit operating systems; their better developmentmethodology let them take a $100bn business-software market from IBM [1209].
27.
5.
3The secure development lifecycleBy the early 2000s, Microsoft had overtaken IBM as the leading tech company,but it was facing ever more criticism for security vulnerabilities in Windows andO�ce that led to more and more malware.
 Servers were moving to Linux andindividual users were starting to buy Macs.
 Eventually in January 2002 BillGates sent all sta↵ a ‘trustworthy computing’ memo ordering them to prioritisesecurity over features, and stopping all development while engineers got securitytraining.
 Their internal training materials became books and papers that helpeddrive change in the broader ecosystem.
 I already discussed their threat modellingin section 27.
3.
5; their ﬁrst take on secure development appeared in 2002 inMichael Howard and David LeBlanc’s ‘Writing Secure Code’ [927], which setsout the early Microsoft approach to managing the security lifecycle, and which Idiscussed in the second edition of this book.
 More appeared over time and theirsecurity development lifecycle (SDL) appeared in 2008, being adopted widely byWindows developers.
The widely used 2010 ‘simpliﬁed implementation’ of SDL is essentially awaterfall process [1308].
 It ‘aims to reduce the number and severity of vulnera-bilities in software’ and ‘introduces security and privacy throughout all phasesof the development process’.
 The ‘pre-SDL’ component is security training; it’sassumed that all the developers get a basic course, the contents of which will de-pend on whether they’re building operating systems, web services or whatever.
There are then ﬁve SDL components.
1.
 Requirements: this involves a risk assessment and the establishment ofquality gates or ‘bug bars’ which will prevent code getting to the nextstage if it contains certain types of ﬂaw.
 The requirements themselvesSecurity Engineering886Ross Anderson27.
5.
 METHODOLOGYare reviewed regularly; at Microsoft, the reviews are never more than sixmonths apart.
2.
 Design: this stage requires threat modelling and establishment of the at-tack surface, to feed into the detailed design of the product.
3.
 Implementation: here, developers have to use approved tools, avoid ordeprecate unsafe functions, and perform static analysis on the code tocheck this has been done.
4.
 Veriﬁcation: this step involves dynamic analysis, fuzz testing, and a reviewof the attack surface.
5.
 Release: this is predicated on an incident response plan and a ﬁnal securityreview.
As well as providing some basic security training to all developers, thereare some further organisational aspects.
 First, security needs a subject-matterexpert (SME) from outside the dev team, and a security or privacy championwithin the team itself to check that everything gets done.
Second, there is a maturity model.
 Starting in 1989, Watts Humphrey devel-oped the Capability Maturity Model (CMM) at the Software Engineering Insti-tute at Carnegie-Mellon University (CMU), based on the idea that competenceis a function of teams rather than just individual developers.
 There’s more toa band than just throwing together half-a-dozen competent musicians, and thesame holds for software.
 Developers start o↵ with di↵erent coding styles, di↵er-ent conventions for commenting and formatting code, di↵erent ways of manag-ing APIs, and even di↵erent workﬂow rhythms.
 The CMU research showed thatnewly-formed teams tended to underestimate the amount of work in a project,and also had a high variance in the amount of time they took; the teams thatworked best together were much better able to predict how long they’d take, interms of the mean development time, but reduced the variance as well [1937].
This requires the self-discipline to sacriﬁce some e�ciency in resource alloca-tion in order to provide continuity for individual engineers and to maintain theteam’s collective expertise.
 Microsoft adapted this and deﬁnes four levels ofsecurity maturity for developer teams.
27.
5.
4Gated developmentIt’s telling that the biggest ﬁrm pushing evolutionary development reverted toa waterfall approach for security.
 Many of the security engineering approachesof the time were tied up with waterfall assumptions, and automated testing onits own is less useful for the security engineer for a number of reasons.
 Secu-rity properties are both emergent and diverse, we security engineers are fewerin number, and there hasn’t been as much investment in tools.
 Speciﬁc attacktypes often need speciﬁc remedies, and many security ﬂaws cross a system’slevels of abstraction, such as when speciﬁcation errors interact with user inter-face features – the sort of problem for which it’s di�cult to devise automatedtests.
 But although regression testing is not su�cient, it is necessary, as it ﬁndsfunctionality that’s been a↵ected by a change.
 It’s particularly important whenSecurity Engineering887Ross Anderson27.
5.
 METHODOLOGYdevelopment sprints add lots of features that can interact with each other.
 Forthis reason, security patches to Windows are an example of gated development:at regular intervals, a pre-release version of the product is pushed through awhole series of additional tests and reviews and prepared for release.
 This isfairly common across systems with safety or security requirements.
 The prepa-ration may involve testing with a wide variety of peripherals and applicationsin the case of Windows, or recertiﬁcation in the case of software for a regulatedproduct.
An issue many neglect is that security requirements evolve, and also have tobe maintained and upgraded.
 They can be driven by changing environments,evolving threats, new dependencies on platforms old and new, and a bundleof other things.
Some changes are implicit; for example, when you upgradeyour static analysis tools you may ﬁnd hundreds of ‘new’ bugs in your existingcodebase, which you have to triage.
 Once more Microsoft was a pioneer here.
When a vulnerability was found in Windows, it’s not enough to just patch it;whoever wrote it might have written a dozen similar ones that are now scatteredthroughout the codebase, and once you publish a patch, the bad guys study itand understand it.
 So rather than just ﬁxing a single bug, you update yourtoolchain so you ﬁnd and eliminate all similar bugs across your products.
 Inorder to manage the costs, both for Microsoft and its customers, the companystarted bundling patches together into a monthly update, the now famous ‘patchTuesday’, in 2003.
 From then until 2015, all customers – from enterprises tothe users of home PCs and tablets – had their software updated on the secondTuesday every month.
 And such patching creates further dependencies.
 Modernquality tools can help you check that no code has a CVE open, so all yourcustomers should have to patch too, if they live by such tools.
 But many don’t:as many as 70% of apps on both phones and desktops have vulnerabilities inthe open-source libraries they use, and which could usually be ﬁxed by a simpleupdate [1695].
 Since 2015, Windows home users receive continuous updates4.
Much the same considerations apply to safety-critical systems, which aresimilar in many respects to secure systems.
 Safety, like security, is an emergentproperty of whole systems, and it doesn’t compose.
 Safety used to depend, inmost applications, on extensive pre-market testing.
 But it’s hard for a connecteddevice to have safety without security, and now that devices such as cars areconnected to the Internet, they are acquiring patch cycles too.
 Yet ensuringthat the latest version of a safety-critical system satisﬁes the safety case mayrequire extensive and expensive testing.
 For example, a car may contain dozensof electronic control units (ECUs) from di↵erent component suppliers, and inaddition to testing the individual ECUs you have to test how they work together.
Firms in the car industry are mutually suspicious and won’t share source codewith each other, even under NDA, so testing can be complex.
 The main testrig may be a ‘lab car’ containing all the electronics from a particular model ofcar, plus extra test systems that let you simulate various maneuvers and evenaccidents.
 These cost real money, and you also need to keep real vehicles for4This also breaks things: we were once about to demonstrate an experiment using a bodymotion-capture suit to a TV crew when the Windows laptop we used to drive it updated itself,and suddenly the capture software wouldn’t work any more.
 There followed frantic phone callsto the software developer in the Netherlands and thankfully we got their update a few hourslater, just in time for the show.
Security Engineering888Ross Anderson27.
5.
 METHODOLOGYroad testing.
 The cost of maintaining ﬂeets of lab cars and real test cars is oneof the reasons car companies dragged their heels when the EU decided to requirethem to patch car software for ten years after the last vehicle left the showroom.
This is one respect in which Tesla has a signiﬁcant advantage; as a tech com-pany with software at the core of its business, Tesla can test and ship changes inweeks which take the legacy car ﬁrms years, as they leave most of the softwaredevelopment to the component suppliers [404].
 Traditionally, automotive soft-ware contracts involved ten years’ support; now you need to support a productfor three years’ development, seven years in the showroom and a further tenafter that.
 I’ll discuss the sustainability aspects of this in the next chapter.
Meanwhile, Tesla is forcing the legacy industry to raise its game, with VW an-nouncing they’ve spent $8bn to create a proper software division, just as theirTesla competitor project runs late [1686].
27.
5.
5Software as a ServiceSince the early 2010s, more and more software has been hosted on central servers,accessed by thin clients and paid for on a subscription basis, rather than beingsold and distributed to users.
 The typical customer has many costs for runningsoftware beyond the license fee, including not just the cost of servers and oper-ators but of deploying it, upgrading it regularly and managing it.
 If the vendorcan take over these tasks from all their customers, many duplicated costs areremoved, and they can manage things better because of their specialised knowl-edge.
 Software can be instrumented so that developers can monitor all aspectsof its performance on a dashboard.
The key technical innovations behind Software as a Service (SaaS) are con-tinuous integration and continuous deployment.
 Rather than having thousandsof customers managing dozens of di↵erent versions of the software, the vendorcan migrate a few customers to a new version to test it, and then migrate therest.
 Upgrades become much more controllable, as they can be tested in a dryrun against a snapshot of the real customer data, called a staging environment.
Some companies now deploy several times a day, as their experience is that fre-quent small changes can be safer and have less risk of breaking something thana larger deployment, such as Microsoft’s Patch Tuesday.
Deployment itself is tentative.
 A SaaS company will typically run its softwareon a number of service instances running on VMs behind a load balancer, whichprovides a point of indirection for managing running services.
The separateinstances also provide separate failure domains to improve robustness.
 To doa rolling deployment we conﬁgure a load balancer to send say 1% of the tra�cto an instance with the new version, often called the ‘canary’ after the cagedbird used by miners to detect carbon monoxide leaks.
 If the canary survives,deployment can be rolled forward progressively to new service instances.
 If thelogging system detects any problems, developers are alerted.
 Some care needs tobe taken that things don’t go wrong if users ﬂap between old and new versionsof a design between transactions.
 If you make a change that breaks backwardscompatibility, you typically build an intermediate stage that will work with bothold and new systems (we were doing this in the world of bank mainframes backin the 1980s anyway).
Security Engineering889Ross Anderson27.
5.
 METHODOLOGYThe ability to manage risks through phased release and rolling deploymentchanges the economics of testing.
 The fact that you can ﬁx bugs extremelyquickly mean that you can achieve a target quality level with much less testing.
You can also see everything the users do, so for the ﬁrst time you can reallyunderstand how usability fails from the point of view of security, safety – andrevenue.
Of course it’s revenue that usually drives the exploitation of this.
Analytics collectors write all behavioural events to a log, which is fed into a datapipeline for metrics, analytics and queries.
 This in turn supports experimentframeworks that can do extensive A/B testing of possible features.
 Ad-drivenservices can optimise by engagement metrics such as active users, time peruser session and use of speciﬁc features.
 Controlled experiments are used toimprove security too; for example, Google has tuned its browser warnings bymeasuring how millions of users react to di↵erent warnings of expired certiﬁcates.
Such improvements are usually fairly small by themselves, so you really needcontrolled experiments to measure them; but when you do lots of them, they addup.
 The investment in building such frameworks into the phased deploymentmechanisms gives an increasing return to scale; the more users you have, thefaster you can achieve statistical signiﬁcance.
 So large ﬁrms can optimise theirproducts more quickly than their smaller competitors; SaaS, like a lot of otherdigital technology, not only cuts costs in the short term, but increases lock-in inthe long term.
 Each time you access a service from a large SaaS ﬁrm, you maybe an unwitting participant in tens or even hundreds of experiments.
 There arelots of ﬁddly details about running multiple concurrent experiments while alsodeploying system enhancements.
Things can get more complex still when you have services put together frommultiple microservices.
 This brings us to the world of infrastructure as code, alsoknown as cloud native development or DevOps, where everything is developedin containers, VMs etc, so all the infrastructure is based on code and can bereplicated quickly.
 You can also use containers to simplify things, packagingas many security dependencies with the code as possible.
New code can bedeployed to a test infrastructure rapidly and tested realistically.
You couldif you wanted manage rolling deployment manually, but this is not scalableand prone to error.
 The solution is to write deployment code, as part of theapplication development process, that uses the cloud platform APIs to allowapplications to deploy themselves and the associated infrastructure, and to hookinto the monitoring mechanisms.
In the last few years, some toolkits havebecome available that allow engineers to do this in a more declarative fashion.
The best guide to this I know is Google’s 2013 book ‘Site Reliability Engi-neering’; SRE is their term for DevOps [236].
 Google led the industry in theart of building large dependable systems out of large ﬂeets of low-cost PCs,building the necessary engineering for load balancing, replication, sharding andredundancy.
 As they operated at a larger scale than anybody else through the2000s and early 2010s, they had to automate more tasks and became good atit.
 The goals of SRE are availability, latency, performance, e�ciency, changemanagement, monitoring, emergency response, and capacity planning.
 The corestrategy is to apply software engineering techniques to automate system admin-istration tasks so as to balance rapid innovation with availability.
As we already noted, there’s no point striving for 99.
9999% availability ifSecurity Engineering890Ross Anderson27.
5.
 METHODOLOGYISPs only let users get to your servers 99% or 99.
9% of the time.
 If you seta realistic error budget, say 0.
1% or 0.
01% unavailability, you can use thatto achieve a number of things.
First, most outages are due to live systemchanges, so you monitor latency, tra�c, errors and saturation well and rollback quickly whenever anything goes wrong.
You use the rest of the errorbudget to support your experimental framework, and doing controlled outages toﬂush dependencies.
 (This was pioneered by Netﬂix whose ‘chaos monkey’ wouldoccasionally take down routers, servers, load balancers and other components,to check that the resilience mechanisms worked as intended; such ‘ﬁre drills’ arenow an industry standard and involve taking down whole data centres.
)In section 12.
2.
6.
2, we mentioned technical debt.
 This concept, due to WardCunningham, encapsulates the observation that development shortcuts are likedebt.
 Whenever we skimp on documentation, ﬁx a problem with a quick-and-dirty kludge, don’t test a ﬁx thoroughly, fail to build in security controls, orfail to work through the consequences of errors, we’re storing up problems thatmay have to be repaid with interest in the future [41].
 Technical debt may makesense for a startup, or a system nearing the end of its life, but it’s more often aproduct of poor management or poorly-aligned incentives.
 Over time, systemscan fall so deeply into debt that they become too hard to maintain or to use;they have to be refactored or replaced.
 For a bank to have to replace its corebanking systems is hugely expensive and disruptive.
 So managing technical debtis really important; this is one of the changes in system management thinkingsince the second edition of this book.
 One important aspect of the philosophyof DevOps is to run debt-free.
27.
5.
6From DevOps to DevSecOpsAs I write, in 2020, the cutting edge is applying agile ideas and methodologynot just to development and operations, but to security too.
 In theory this canmean a strategy of ‘everything as code’; in practice it means not just maintainingan existing security rating (and safety case if relevant) but responding to newthreats, environmental changes, and surprising vulnerabilities.
Bringing thetwo together involves real work, and sometimes things need to be reinvented.
 Imentioned for example in section 12.
2.
2 that DevOps undermines the separationbetween development and production on which banks have relied for years;where separation of duties is necessary, we have to reimagine it.
We see several di↵erent approaches in the companies with which we work.
 Inwhat follows I will give two examples, which we might roughly call the Microsoftworld and the Google world.
 There are of course many others.
27.
5.
6.
1The Azure ecosystemMost of the world’s largest commercial ﬁrms from banks and insurers throughretail to shipping and mining have built their enterprise systems on Windowsover the past 25 years and are now migrating them to Azure, often using systemsintegration and facilities management ﬁrms to do the actual work.
 The typicalclient has a mixture of on-premises and cloud systems with new developmentsmostly migrating from the former to the latter.
 Here policy is largely set bySecurity Engineering891Ross Anderson27.
5.
 METHODOLOGYthe Big Four auditors who, in addition to their standard set of internal controlfeatures, follow Microsoft in requiring a secure development lifecycle.
 The sev-eral dozen tools used to do threat modelling, static analysis, dynamic analysis,fuzz testing, app and network monitoring, security orchestration and incidentresponse impose a signiﬁcant overhead with dozens of people copying data fromone tool to another.
 The DevSecOps task here is to progressively integrate thetools by automating these administrative tasks.
To support this ecosystem, Microsoft has extended its SDL with furthersteps: deﬁning metrics and compliance reporting; threat modelling; cryptog-raphy standards; managing the security risks of third-party components; pen-etration testing; and a standardised incident response.
 The ﬁrm now claimsthat 10% of its engineering investment is in cybersecurity.
 The capable systemintegration and facilities management ﬁrms have worked out ways of buildingthese steps into their workﬂows; much of the actual work involves integratingthe third-party security products that they or their customers have bought.
Appropriate automation is vital for the security team to continue raising theirgame, extending their scope and increasing e↵ectiveness; without it, they fallfurther and further behind, and burn out [1846].
The organising principles for DevSecOps in such a company will be to ‘shiftleft’ which can cover a number of things: the unifying theme is moving security,like software and infrastructure, into the codebase.
 One strategy is to causethings to ‘fail fast’ including engaging security experts early enough in the de-velopment process to avoid delays later: doing pre-commit static analysis of eachdeveloper’s code to minimise failed builds; buying or building specialist tools todetect errors such as incorrect authentication, mistakes in using crypto func-tions, and injection opportunities; both automated and manual security testingof new versions; and automated testing of conﬁguration and deployment includ-ing scanning of the staging network and checks on credentials, encryption keysand so on.
 And while, back in 2010, Microsoft considered operational securityto be separate from software security, a modern Azure shop will close the loopby following up deployment with continuous monitoring, manual penetrationtests and ﬁnally bug bounties for third parties who spot something wrong.
 Wewill discuss these in more detail later.
27.
5.
6.
2The Google ecosystemA second view comes from engineers working on infrastructure, and the bestreference I know is a 2020 book by six Google engineers, ‘Building Secure andReliable Systems’ [23].
 The DevSecOps strategy is somewhat similar at Amazon,but optimised for their product o↵erings; it is described by their CTO WernerVogels at [1966].
 However the Google experience is described in much moredetail.
 This section draws on their book, and on colleagues who have workedrecently at the major service ﬁrms.
When building infrastructure systems on which hundreds of millions of peo-ple will rely, it is critical to automate support functions quickly, and to havereally robust processes for threat identiﬁcation, incident response, damage lim-itation and service recovery.
 So while a facilities-management ﬁrm might workat integrating support functions to save money and reduce errors, the emphasisSecurity Engineering892Ross Anderson27.
5.
 METHODOLOGYat major service ﬁrms is reliability.
 I already mentioned the Google approach tosite reliability engineering: set a realistic target, of say 99.
9% availability, andthen use the residual error budget of 0.
1% downtime by apportioning it betweenfailure recovery, upgrades and experiments.
This in turn drives further principles such as design for recoverability, de-sign for understandability, and a desire to stop humans touching productionsystems wherever possible.
 It’s not enough to have automation for the incre-mental deployment of new binaries; you also want to stop sysadmins havingto type complicated command lines into routers to conﬁgure networks; this iswhere most of the network outages come from, as we noted in section 21.
2.
1.
You manage such risks by building suitable tool proxies.
 This can involve quitea lot of work to align the update of binary and conﬁg ﬁles and work out howto allocate support and recovery e↵ort between SRE and security engineeringteams.
 Further complexity arises with secure testing.
 How do you build testinfrastructures to exercise least privilege? How do you test systems that containlarge amounts of personal information? How do you test the break-glass mech-anisms that give SRE teams emergency human access to live systems? Most ofthese are questions we already had to deal with in the mainframe world of the1980s, but they arose only occasionally and were dealt with by human ingenuityand by trusting some key sta↵.
 Scaling everything up from thousands of usersto billions means that a lot more has to be automated.
There are still tensions.
 In site reliability engineering, alarms should be assimple, predictable and reliable as possible; but in security, some randomisationis often a good idea.
At the application level, systems are increasingly compartmentalised into mi-croservice components with defensible security boundaries and tamper-resistantsecurity contexts, so that if Alice compromises a shopping system’s catalogue,she still can’t spend money as Bob as the payment service is separate.
 Each com-ponent will typically be implemented as a number of parallel copies or shards,giving still smaller failure domains.
 Such domains enable you to limit the blastradius of any compromise; ideally, you want to be able to deal with an intrusionwithout taking your whole system o✏ine.
 Compartmentalised systems can beengineered for resilience too but this is not straightforward.
 When a failure do-main fails, when do you just spin up a new one, and when do you do somethingdi↵erent? What are the dependencies? Which components should fail open,and which should fail secure? What sort of degraded performance is acceptableunder congestion, or under attack? What’s the role of load shedding and throt-tling? And what sort of pain can you rationally inﬂict on users, and on businessmodels? Do you ditch some of the ads, require extra CAPTCHAs for logons, orboth? And how do you test and validate all these resilience mechanisms?Large ﬁrms invest a lot of engineering time in building application frame-works for such services.
There are also standard frameworks for web pages,which should not only prevent SQL injection and cross-site scripting attacksin the ﬁrst place, but also provide support for dozens of di↵erent languages.
Having a single front end to terminate all http(s) and TLS tra�c means that ifyou have to update your certiﬁcate management mechanisms or ciphersuites youonly need to do it once, not in all your di↵erent services.
 A single front end canalso provide a single location for load balancing and DDoS protection, as wellSecurity Engineering893Ross Anderson27.
5.
 METHODOLOGYas for many other functions such as supporting dozens of di↵erent languages.
Using type encapsulation to enforce properties of URLs, SQL and so on canreduce the amount of code you need to verify.
 If you have secure-by-constructionAPIs that are also understandable, that’s best.
 Google has a crypto API calledTink that forces more correct use.
 It requires use of a key management service,whether in the Google cloud, AWS or the Android keystore.
 This ﬁts into anoverall framework for managing crypto termination, code provenance, integrityveriﬁcation and workload isolation, called BeyondProd [998].
27.
5.
6.
3Creating a learning systemWhether you follow the Microsoft approach, the Google approach or your own,to tune such a process you need metrics, and suitable candidates include thenumbers of security tickets opened to dev teams, the number of security-failedbuilds, and the time it takes for a new application to achieve compliance underthe relevant regulation (whether SOX, GDPR or HIPAA).
 As Dev, Sec and Opsconverge, the metrics and management processes converge with the network de-fence mechanisms discussed in section 21.
4, from network monitoring to securityincident and event management.
 But all this needs to be managed intelligently.
A well-run ﬁrm can make the security process more visible to all the dev / opssta↵ via the sprints that you do to work up a privacy impact assessment, im-prove access controls, extend logging or whatever.
 A badly-run ﬁrm will manageto the metrics, which will create tensions: their security sta↵ can end up withconﬂicting goals of keeping the bad guys out, and also of ‘feeding the beast’by hitting all the metrics used to justify the team’s own existence [1846].
 It’simportant to understand where conﬂicts naturally arise as a function of theorganisation’s management structure, and somehow keep them constructive.
One of the big drivers in either case, though, will be the vulnerability lifecy-cle.
 The processes whereby bugs become exploits and then attacks, and theseattacks are noticed leading to vulnerability reports, interim defences using de-vices such as ﬁrewalls, then deﬁnitive patches that are rolled out not just todirect users but along complex supply chains, is ever more central to securitymanagement.
27.
5.
7The vulnerability cycleBack in the 1970s and 1980s, people sometimes described the evolutionary pro-cedure of ﬁnding security bugs in systems and then ﬁxing them dismissively aspenetrate-and-patch.
 It was hoped that some combination of an architecturethat limited the attack surface and the application of formal methods wouldenable us to escape.
 As we’ve seen, that didn’t really work, except in a few edgecases such as cryptographic equipment.
 By the early 2000s, we had come to theconclusion that we just had to manage the patch cycle better, and the modernapproach of security breach disclosure laws, CERTs and responsible disclosurebedded down during this period.
The vulnerability cycle consists of the process whereby someone, the re-searcher, discovers a vulnerability in a system that is maintained by a vendor.
Security Engineering894Ross Anderson27.
5.
 METHODOLOGYThe researcher may be a customer, an academic, a contractor for a nationalintelligence agency or even a criminal.
They may sell it in a market.
Theidea of vulnerability markets was ﬁrst suggested by Jean Camp and CatherineWolfram in 2000 [371]; ﬁrms were set up to buy vulnerabilities, and over timeseveral markets emerged.
 Most of the big software and service ﬁrms now of-fer bug bounties, which can range from thousands to hundreds of thousands ofdollars; at the other extreme are operators who buy up exploits for sale to ex-ploiters such as cyber-arms manufacturers (who sell to military and intelligenceagencies) and forensic ﬁrms (who sell to law enforcement).
 Such operators nowo↵er millions of dollars for persistent remote exploits of Android and iOS.
The researcher may also disclose the bug to the vendor directly – nowadaysmany vendors have a bug bounty program that pays rewards for disclosed vul-nerabilities that attempt to match market prices, at least in order of magnitude.
As market prices for zero-day exploits against popular platforms have headedinto six and even seven ﬁgures, so have bug bounties.
 Apple, for example, o↵ers$1M for anyone who can hack the iOS kernel without requiring any clicks bythe user.
 In 2019, it emerged that at least six hackers have now earned over$1M through the bug bounty platform HackerOne alone [2030].
 A downsideof large bug bounties is that while bugs used to occur naturally, we now seethem being introduced deliberately, for example by contributors to open-sourceprojects whose code ends up in signiﬁcant platforms.
 Such supply-chain attacksused to be the preserve of nation states; now they’re opening up [890].
If an exploit is used in the wild before the vendor issues a patch, it is calleda zero day, and is typically used for targeted attacks.
 If it’s used enough, theneventually someone will notice; the attack gets reported, and then vendor issuesa patch, which may then be reverse engineered so that many other actors nowhave exploit code.
 Customers who fail to patch their systems are now vulnerableto multiple exploits that can be deployed at scale by crime gangs.
Getting the patching cycle right is a problem in the economics of infor-mation security as much as anything else, because the interests of the variousstakeholders can diverge quite radically.
1.
 The vendor would prefer that bugs weren’t found at all, to spare theexpense of patching.
 They’ll patch if they have to but want to minimisethe cost, which may include a lot of testing if their code appears in lotsof product versions.
 Indeed, if their code is used in customer devices thatnow need patching (like cars) they may have to pay an indemnity to covertheir customer’s costs; so in such industries there’s an even more acuteincentive for foot-dragging and denial.
2.
 The average customer might prefer that bugs weren’t found, to avoid thehassle of patching.
 Lazy customers may fail to patch, and get infected as aresult.
 (If all the infected machines do is send a bit of spam, their ownersmay not notice or care.
)3.
 The typical security researcher wants some reward for their discoveries,whether fame, cash or getting a ﬁx for a system they rely on.
4.
 The intelligence agencies want to learn of vulnerabilities quickly, so theycan be used in zero-day exploits before a patch is shipped.
Security Engineering895Ross Anderson27.
5.
 METHODOLOGY5.
 The security software ﬁrms beneﬁt from unpatched vulnerabilities as theirﬁrewalls and AV software can look for their indicators of compromise toblock attacks that exploit them.
6.
 Large companies don’t like patches, and neither do government depart-ments, as the process of testing a new patch against the enterprise’s crit-ical systems and rolling it out is expensive.
 The better ones have builtautomation to deal with regular events like Microsoft’s Patch Tuesday, butupdating or risk-assessing the zillions of IoT devices in their o�ces andfactories will be a headache for years to come.
 Most ﬁrms just don’t havea good enough asset inventory system to cope.
During the 1990s, the debate was driven by people who were frustrated atsoftware vendors for leaving products unpatched for months or even years.
 Thebugtraq mailing list was set up to provide a way for people to disclose bugsanonymously; but this meant that a product might be completely vulnerablefor a month or two until a patch was written, tested and shipped, and untilcustomer ﬁrms had tested it and installed it.
 This led to a debate on ‘respon-sible disclosure’ with various proposals about how long a breathing space theresearcher should give the vendor [1572].
The consensus that emerged was that researchers should disclose vulnerabil-ities to a computer emergency response team (CERT)5 and the global networkof CERTs would inform the vendor, with a delay for a patch to be issued beforethe vulnerability was published.
 The threat of eventual disclosure got vendorso↵ their butts; the delay gave them enough time to test a ﬁx properly beforereleasing it; researchers got credit to put on their CVs; customers got bug ﬁxesat the same time as bug reports; and the big companies organised regular up-dates for which their corporate customers can plan.
 Oh, and the agencies hada hot line into their local CERT, so they learned of naturally occurring exploitsin advance and could exploit them.
This was part of the deal described insection 26.
2.
7.
3 that ended Crypto War 1 back in 2000.
27.
5.
7.
1The CVE systemAn industrial aspect is the Common Vulnerabilities and Exposures (CVE) sys-tem, launched in 1999, which assigns numbers to reported vulnerabilities inpublicly released software packages.
 This is maintained by Mitre, but it dele-gates the assignment of CVEs to large vendors.
 CVE IDs are commonly includedin security advisories, enabling you to search for details of the reporting date,a↵ected products, available remedies and other relevant information.
 There isa Common Vulnerability Scoring System (CVSS) which provides a numericalrepresentation of the severity of a vulnerability.
 The method for calculatingthis has become steadily more complex over time and now depends on whetherthe attack requires local access, its complexity, the e↵ort required, its e↵ects,the availability of exploit code and of patches, the number of targets and thepotential for damage.
5The EU is renaming these CSIRTs – computer security incident response teams.
Security Engineering896Ross Anderson27.
5.
 METHODOLOGYNIST’s National Vulnerability Database (NVD), described as a “comprehen-sive cybersecurity vulnerability database that integrates all publicly availableU.
S.
 Government vulnerability resources and provides references to industry re-sources” is based on the CVE List.
 These resources are critical for automatingthe tracking of vulnerabilities and updates.
 There are now so many thousandsof vulnerabilities reported, and so many hundreds of patches shipped, that au-tomation is essential.
As the system was bedding down, it became a subject of study by secu-rity economists.
 Traditionalists argued that since bugs are many and uncorre-lated, and since most exploits use vulnerabilities reverse-engineered from exist-ing patches, there should be minimal disclosure.
 Pragmatists argued that, fromboth theoretical and empirical perspectives, the threat of disclosure was neededto get vendors to patch.
 I discussed this argument in section 8.
6.
2.
 Since thenwe have seen the introduction of automatic upgrades for mass-market users,the establishment of ﬁrms that make markets in vulnerabilities, and empiricalresearch on the extent to which bugs are correlated.
 Modulo some tuning, thecurrent computer industry way of doing things has been stable for over a decade.
27.
5.
7.
2Coordinated disclosureYet some industries are lagging well behind.
 In section 4.
3.
1 I described howVolkswagen sued academics at Birmingham and Nijmegen universities after theydiscovered, and responsibly disclosed, vulnerabilities in Volkswagen’s remote keyentry system that were already being exploited in car-theft tools that were avail-able online.
 This was a mistake, as it drew attention to the vulnerability, andVolkswagen duly lost in court.
 Companies like Microsoft and Google have hadtwenty years to learn that running bug bounty programs and monthly patchingworks better than threatening to sue people, but a lot of ﬁrms in legacy indus-tries still haven’t worked this out even though their products contain more andmore software.
One of the problems in the Volkswagen case was that the researchers initiallydisclosed the vulnerability to the supplier of its key entry system, which in turntold Volkswagen only at the last minute.
 As a result of supply chain problemslike this, responsible disclosure has given way to coordinated disclosure.
 Fewﬁrms build all their own tools any more, and even a child’s toy may have multiplesoftware dependencies.
 If it does speech and gesture recognition, it probablycontains an Arm chip running some ﬂavour of Linux or FreeBSD, communicateswith a cloud service running another ﬂavour of Linux, and can be controlled byan app that may run on Android or iOS.
 The safety of the toy will depend onsecure communications; for example, it was discovered in February 2019 thatthe communications between Enox’s ‘Safe-KID-One’ toy watch and its back-end server were unencrypted, so that hackers could in theory track and callkids.
 The response was an immediate EU-wide safety recall [654].
 Getting thissort of thing wrong can be sudden death for your product, and your company.
Now what happens when someone discovers an exploitable bug in a platformused in dozens of embedded products?This can be traumatic, as with theShellshock bug in Linux and the Heartbleed bug in OpenSSL (which also a↵ectedLinux).
If Linux gets an emergency patch, coordinating the disclosure is aSecurity Engineering897Ross Anderson27.
5.
 METHODOLOGYnightmare: the Linux maintainers may be able to work in private with themain Linux distributions, and with derivatives like Android whose developerskeep in close contact with them.
But there are the thousands of productsthat incorporate Linux, from alarm clocks to TVs and from kids’ toys to landmines.
 You may suddenly ﬁnd that the CCTV cameras in your building securitysystem have all become hackable, and the vendor can’t ﬁx them quickly or atall.
 Coordinating disclosure on platforms is one of the seriously hard problems.
There is no silver bullet but there are still many things you can do, ranging fromdocumenting your upstream and downstream dependencies, through aggressivetesting of software you depend on so you get to exercise and understand the bugreporting mechanisms, to becoming part of its developer community.
Dealing with such shocks is just one aspect of a process that in the late 2010sbecame a speciality of its own, namely security incident and event management.
27.
5.
7.
3Security incident and event managementYou need an incident response plan for what you’ll do when you learn of avulnerability or an attack.
 In the old days, vendors could take months to respondwith a new version of the product, and would often do nothing at all but issue awarning (or even a denial).
 Nowadays, breach-notiﬁcation laws in both the USAand Europe oblige ﬁrms to disclose attacks where individuals’ privacy could havebeen compromised, and people expect that problems will be ﬁxed quickly.
 Yourplan needs four components: monitoring, repair, distribution and reassurance.
First, make sure you learn of vulnerabilities as soon as you can – and prefer-ably no later than the bad guys (or the press) do.
 This means building a threatintelligence team.
 In some applications you can just acquire threat intelligencedata from specialist ﬁrms, while if you’re an IoT vendor it may be prudent tooperate your own honeypots so you get immediate warning of people attackingyour products.
 Listening to customers is important: you need an e�cient wayfor them to report bugs.
 It may be an idea to provide some incentive, such aspoints towards their next upgrade, lottery tickets or even cash.
 You absolutelyneed to engage with the larger technical ecosystem of bug bounties, vulnerabilitymarkets, CERTs and CVEs described in section 27.
5.
7.
Second, you need to be able to repair the problem.
Twenty years ago,that meant having one member of each product team ‘on call’ with a pagerin case something needed ﬁxing at three in the morning.
 Nowadays it meanspreparing an orchestrated response to anything from a vulnerability report toa major breach.
This will extend from the intrusion-detection and networkmonitoring functions we discussed in section 21.
4.
2.
3 and the threat intelligenceteam through to identifying the dev teams responsible and notifying both yoursuppliers upstream and your customers downstream.
 Responder teams may alsoneed alternative means of communication.
 Did you ever stop to think whetheryou need satellite phones?Third, you need to be able to deploy the patch rapidly: if all the softwareruns on your own servers, then it may be easy, but if it involves patching code inmillions of consumer devices then advance planning is needed.
 It may seem easyto get your customers to visit your website once a day and check for upgrades,Security Engineering898Ross Anderson27.
5.
 METHODOLOGYbut if their own systems depend on your devices and they need to test anydependencies, there’s a tension [195]: pioneers who apply patches quickly candiscover problems that break their systems, while people who take time to testwill be more vulnerable to attack.
 The longer the supply chains get, the harderthe conﬂicts of interest are to manage.
 Operations matter hugely: an emergencypatch process that isn’t tested may do more harm than good, and experienceteaches that in an emergency you just run your normal patch process as fast aspossible [23].
Finally, you need to educate your CEO and main board directors in advanceabout the need to deal quickly and honestly with a security breach in order tokeep conﬁdence and limit damage, by giving them compelling examples of ﬁrmsthat did well and others that did badly.
 You need to have a mechanism to getthrough to your CEO and brief them immediately so they can show the thing’sunder control and reassure your key customers.
 So you need to know the mobileand home phone numbers of everyone who might be needed urgently.
 And youneed a plan to deal with the press.
 The last thing you need is for dozens ofjournalists to phone up and be stonewalled by your PR person or even yourswitchboard operator as you struggle madly to ﬁx the bug.
 Have a set of pressreleases ready for incidents of varying severity, so that your CEO only has topick the right one and ﬁll in the details.
 This can then ship as soon as the ﬁrst(or perhaps the second) journalist calls.
Remind your CEO that both the USA and Europe have security-breachdisclosure laws, so if your systems are hacked and millions of customer cardnumbers compromised, you have to notify all current and former customers,which costs real money.
 You can expect to be sued.
 If you have 10 millioncustomers’ personal data compromised, that might mean 10 million letters at$5 each and 3 million reissued credit cards at $10 each, even if you don’t getclaims from banks for actual fraud losses on those accounts.
 (That may wellhappen; you might expect that of 3 million accounts, a few tens of thousandswould su↵er some fraud in each year anyway, and the banks will sue you for allof it.
) The ﬁnancial loss from a signiﬁcant breach can easily hit nine ﬁgures.
 Ifit happens to you more than once, you can expect to lose customers: customerchurn might only be 2% after one notiﬁed breach, but 30% after two and evenmore after three [2037].
 Since some CEOs have been ﬁred after large breaches,information security has become a CEO issue.
27.
5.
8Organizational mismanagement of riskOrganizational issues are not just a contributory factor in system failure, aswith the loss of organizational memory and the lack of mechanisms for moni-toring changing environments.
 They can often be a primary cause.
 There’s alarge literature on how people behave in organisations, which I touched on insection 8.
6.
7, and I’ve given a number of further examples in various chapters.
However, the importance of organisational factors increases as projects get big-ger.
 Bezos’ law says you can’t run a dev project with more people than canbe fed from two pizzas.
 A team of eight people is just about manageable, butyou can’t go six times as fast by having six such teams in parallel.
 If a projectinvolves multiple teams the members can’t talk to each other at random, or youSecurity Engineering899Ross Anderson27.
5.
 METHODOLOGYget chaos; and they can’t route all their communications through the lowestcommon manager as there isn’t the bandwidth.
 As you scale up, the coordina-tion will start to involve a proliferation of middle managers, sta↵ departmentsand committees.
 The communications complexity of a clean military chain ofcommand, for N people with no lateral interaction, is log N; where everybodyhas to consult everybody else, it’s N 2; and where any subset can form a com-mittee to think about the problem, it can head towards 2N.
 Business schoolpeople have written extensively about this, and their methodology is generallybased on case studies.
Many large development projects have crashed and burned.
 The problemsappear to be much the same whether the disaster is a matter of safety, of securityor of the software simply never working at all; so security people can learn a lotfrom studying project failures documented in the general engineering literature.
A classic study of large software project disasters was written by Bill Curtis,Herb Krasner, and Neil Iscoe [504].
 They found that failure to understand the re-quirements was mostly to blame: a thin spread of application domain knowledgetypically led to ﬂuctuating and conﬂicting requirements which in turn caused abreakdown in communication.
 The example I give in my undergraduate lecturesis the meltdown of a new dispatch system for the London Ambulance Servicewhere a combination of an overly ambitious project, an inadequate speciﬁcationand no real testing led to the city being without ambulance cover for a day.
There are all too many such examples; I use the London Ambulance Servicecase because the subsequent inquiry documented the causes rather well [1805].
I also happened to be in London that day, so I remember it.
 If you haven’t everread the inquiry report, I recommend you do so.
 (In fact I strongly recommendthat you read lots of case studies of project failure.
)The millennium bug gives another useful data point.
 If one accepts thatmany large commercial and government systems needed extensive repair work tochange two-digit dates into four-digit ones in preparation for the year 2000, andthe conventional experience that a signiﬁcant proportion of large developmentprojects are late or never delivered at all, many people naturally assumed thata signiﬁcant number of systems would fail at the end of 1999, and predictedwidespread chaos.
 But this didn’t happen.
 Certainly, the risks to the systemsused by small and medium-sized ﬁrms were overstated; we did a thorough checkof all our systems at the university, and found nothing much that couldn’t beﬁxed fairly easily [69].
Nevertheless, the systems of some large ﬁrms whoseoperations are critical to the economy, such as banks and utilities, did needsubstantial repairs.
 Yet there were no reports of high-consequence failures.
 Thisappears to support Curtis, Krasner, and Iscoe’s thesis.
 The requirement for Y2Kbug ﬁxes was known completely: “I want this system to keep on working, justas it is now, through into 2000 and beyond”.
This is one of the reasons I chose the quote from Rick Smith to head thischapter: “My own experience is that developers with a clean, expressive set ofspeciﬁc security requirements can build a very tight machine.
 They don’t haveto be security gurus, but they have to understand what they’re trying to buildand how it should work.
”Organisations have di�culty dealing with uncertainty, as it gets in the way ofSecurity Engineering900Ross Anderson27.
5.
 METHODOLOGYsetting objectives and planning to meet them.
 So capable teams tackle the hardproblem ﬁrst, to reduce uncertainty; that was DARPA’s mission, and the coreof the spiral model.
 There’s a signiﬁcant business-school literature on how tomanage uncertainty in projects [1179].
 But it’s easy to get this wrong, even in afairly well-deﬁned project.
 Faced with a hard problem, it is common for peopleto furiously attack a related but easier one; we’ve seen a number of examples,such as in section 26.
2.
8.
Risk management can be even worse in security where the problem is open-ended.
 We really have no idea where the next shitstorm will come from.
 In thelate 1990s, we thought we’d got secure smartcards; then along came di↵erentialpower analysis.
 In the mid-2010s we thought we had secure enough CPUs forcompetitor ﬁrms to run their workloads on the same machines in Amazon datacentres; then along came Spectre.
 We also used to think that Apple productscouldn’t get malware and that face recognition would never be good enough tobe a real privacy threat.
 Even though Moore’s law is slowing down, there willbe more surprises.
Middle managers prefer approaches that they can implement by box-tickingtheir way down a checklist, but to deal with uncertainties and open-ended risks,you need a process of open learning, with people paying attention to the alerts,or the frauds, or the safety incidents, or the customer complaints – whateveryou can learn from.
But checklists demand less management attention ande↵ort, and the quality bureaucracy loves them.
 I noted in section 9.
6.
6 thatcertiﬁed processes had a strong tendency to displace critical thought; instead ofconstantly reviewing a system’s protection requirements, designers just reach fortheir checklists.
 The result is often perverse.
 By not tackling the hard problemﬁrst, you hide the uncertainty and it’s worse later6.
 Also, people rapidly learnhow to game checklists.
 There is the eternal tension between us security expertstelling ﬁrms to pay smart people to anticipate what might go wrong, and boardstelling managers to deliver product faster using fewer and cheaper engineers.
When the threat model is politically sensitive, things get more complicated.
The classic question is whether attacks come from insiders or outsiders.
 Insidersare often the biggest security risk, whether because some of them are maliciousor because most of them are careless.
 But you can’t just train all your sta↵ to beunhelpful to each other and to customers, unless perhaps you are a governmentdepartment or other monopoly.
 You have to ﬁnd the sweet spot for control,and that often means working out how to embed it in the culture.
 For example,bank managers know that dual-control safe locks reduce the risk of their familiesbeing taken hostage, and requiring two signatures on large transactions meansextra shoulders to take the burden when something goes wrong.
Getting the risk ecosystem right in an organisation can take both subtletyand persistence.
 The cultural embedding of controls and other protective mea-sures is hard work; if you come into contact with multiple ﬁrms then it’s interest-ing to observe how they manage their rules around everything from code audits(which the tech majors insist on) to tailgating (which semiconductor ﬁrms areat pains to prevent) and whether people are expected to keep one hand on a6I will discuss ISO 27001 in the next chapter.
 The executive summary for now is thatalmost every ﬁrm hit by a big data breach had ISO 27001 certiﬁcation, but it failed becausetheir auditors said something was OK that wasn’t.
Security Engineering901Ross Anderson27.
5.
 METHODOLOGYbanister as they walk up and down the stairs (a favourite of energy companies).
Where do these risk cultures come from, how are they promoted, and why dothey cluster by sector? Their transactional internal control structures may beheavily inﬂuenced by their auditors, as we discussed in section 12.
2.
6.
3, but thebroader security culture varies a lot – and matters.
A further factor is that good CISOs are almost as rare as hens’ teeth.
 Thereare some stars at the top tech and ﬁntech ﬁrms, but being a CISO can be athankless job.
 Good engineers often don’t want it, or don’t have the people skillsto cope, while ambitious managers tend to avoid the job.
 In many organisations,promotions are a matter of seniority and contacts; so if you want to be the CEOyou’ll have to spend 20 years climbing up the hierarchy without o↵ending toomany people on the way.
 Being CISO will mean saying no to people all the time,and a generalist with no tech background can’t hack it anyway.
 The job alsobrings a lot of stress, and the risk of burnout; a CISO’s average tenure is abouttwo years [430].
 In any case, embedding an appropriate culture around risk andsecurity is for the CEO and the board.
 If they don’t think it’s important, theCISO has no chance.
 But breaches have now led to enough CEOs being ﬁred,or losing millions on their stock, that other members of that tribe are startingto pay attention.
One way the risk ecosystem can be skewed is that if a company managesto arrange things so that some of the the risks of the systems it operates getdumped on third parties.
 This creates a moral hazard by removing the incentivesto take care.
 We discussed this in section 12.
5.
2 in the context of banks tryingto shift fraud liability in payment systems to cardholders, merchants or both.
Sta↵ can get lazy or even crooked if they know that customer complaints will bebrushed o↵.
 Another example is Henry Ford, who took the view that if you wereinjured by one of his cars, you should sue the driver, not him; it took decadesfor courts and lawmakers to nail down product liability.
Companies may also swing from being risk takers to being too risk averse,and back again.
 The personality of key executives does matter.
 My own uni-versity has been gung-ho when we hired an engineer to be Vice-Chancellor,timorous when we hired a lawyer, and in the middle when we hired a medic.
Another source of problems is when system design decisions are taken bypeople who are unlikely to be held accountable for them.
 This can happen formany reasons.
 IT sta↵ turnover could be high, with much reliance placed oncontract sta↵; fear of redundancy can turn loyal sta↵ into surreptitious job-seekers.
 This can be a particular problem in big public-sector IT projects: noneof the ministers or civil servants involved expect to be around when the thingis delivered seven years from now.
 So when working on a big system project,don’t forget to look round and ask yourself who’ll take the blame later whenthings go wrong.
Yet another is that when hiring security or safety consultants to help withproduct design, ﬁrms have an incentive to go for a ﬁrm that is ‘good enough’ butwill not be too demanding; a gentle review from a Big Four ﬁrm will be muchmore useful than a detailed review from an expert who might recommend muchmore expensive design changes.
 Indeed, if a ﬁrm was determined to get a com-pletely secure product, then they should hire multiple experts.
 We describedSecurity Engineering902Ross Anderson27.
6.
 MANAGING THE TEAMin section 14.
2.
3 how this helped with the design of prepayment electricity me-ters, and a later experiment with students conﬁrmed that the more people yougot to think about a proposed system design, the more potential hazards andvulnerabilities they could spot [68].
 Of course, this rarely happens.
27.
6Managing the TeamTo develop secure and reliable code, you need to build a team with the rightculture, the right mix of skills, and the right incentives.
Many modern systems are already so complex that few developers can copewith all aspects of them.
 So how do you build strong development teams withcomplementary skills? This has been a subject of vigorous debate for over ﬁftyyears now, with di↵erent writers reﬂecting their personal style or company cul-ture.
 It has long been entangled with cultural issues such as diversity, althoughthese have only got serious attention since the mid-2010s.
27.
6.
1Elite engineersGoing back to the 1960s, Fred Brooks’s famous book, ‘The Mythical Man-Month’, describes the lessons learned from developing the world’s ﬁrst largesoftware product, the operating system for the IBM S/360 mainframe [328].
He describes the ‘chief programmer team’, a concept evolved by his colleagueHarlan Mills, in which a chief programmer – a development lead, in today’slanguage – is supported by a toolsmith, a tester and a language lawyer.
 Thethinking was that some programmers are much more productive than others, sorather than promoting them to management and ‘losing’ them you create postsfor them with the salary and esteem of senior managers.
 The same approachwas found in other tech companies in the 1960s through the 1980s, and even inbank IT departments where I worked in the late 1980s.
The view taken by more modern companies such as Microsoft, Google andFacebook is that you only want to hire the ultra-productive engineers in theﬁrst place – especially if you get a million CVs a year but plan to hire only20,000 new engineers.
 One approach is to hire people as contractors for a fewmonths to see how they do; but that’s harder with fresh graduates, as evenbright students from elite schools can take a few months to become productivein a commercial team.
 Productivity is also a matter of culture; engineers whothrive at one company may do much less well at another.
 A related issue is thatif you have each candidate interviewed by a number of your engineers, that’s notjust a drain on engineer time, but can also perpetuate a culture that’s not verywelcoming to women engineers.
 Elite universities are in a similar situation tothe tech majors, with dozens of applicants for each place; over the years we’velearned to have mechanisms to monitor diversity in hiring and admissions.
The two approaches are not in conﬂict.
 Modern tech ﬁrms employ multipletech superstars from internationally known designers to Turing-award winningcomputer scientists.
 The view at one such ﬁrm is that you cannot expect to writegood software if you don’t have a career structure for programmers.
 People whoSecurity Engineering903Ross Anderson27.
6.
 MANAGING THE TEAMwant to spend their lives writing software, and are good at it, have to get respect,however your organisation signals that – whether it’s salary, bonuses, stock orfripperies like access to the executive dining room.
 Universities get this; weprofessors run the place.
 Tech companies get it too, and one or two banks havestarted to.
 But governments are generally appalling.
 In the UK civil service, themotto is that “scientists should be on tap but not on top.
” And more than onecar company I know of has real problems hiring and retaining decent softwareengineers.
 In one of them, software engineers are expected to become managersafter ﬁve years or remain on a junior pay grade, while in another all engineersare expected to wear business suits to work (and still paid lousy money).
 I’llreturn to this in section 27.
6.
6.
27.
6.
2DiversityAt the beginning of computing, there were plenty of women programmers – theywere the majority until the late 1960s, and included pioneers such as GraceHopper and Dame Stephanie Shirley (who ran her company for years as ‘SteveShirley’).
When I started in the early 1970s there was still a much bettergender balance than today.
 There were minorities too; the orbital calculationsfor the Mercury, Gemini and Apollo missions were led by an African-Americanwoman, Katharine Johnson.
 But things have become male-dominated in theUSA and the UK.
 Since I became an academic in the 1990s, about a sixth oflocal computer science students have been women, despite signiﬁcant e↵orts torecruit more women students.
 However, in the formerly communist countriesof Eastern Europe, the ratio is about a third.
(We’ve improved our genderbalance by admitting lots of students from southern and eastern Europe.
) InIndia there’s close to gender balance.
 So this is a cultural issue, and there’s alot of debate on how it came about.
 Is it a lack of role models, or is it the faultof careers advisers in schools, or are many IT shops just an unpleasant workingenvironment for women?That has certainly been an issue: the Gamergatescandal, which I discussed in section 2.
5.
1, exposed deep misogyny in somegaming communities, while the #MeToo movement has highlighted many casesof sexism in Silicon Valley.
Even within computer science we see a lot of subcultural variation.
 The lasttime I went to a hardware conference – an Arm developer event – I saw about500 men but only three women (all of them Indian).
 In the security ﬁeld, wewere overwhelmingly male in the 1990s when the emphasis was cryptology andoperating system internals, but are much more balanced now we have embracedthe importance of design, usability and psychology.
 Role models and history domatter.
 Research groups with a woman faculty member get more applicationsfrom able women7.
More diverse teams are more e↵ective, and the real change doesn’t comewith the ﬁrst woman you hire, but when you have enough to change the teamculture.
 That might mean three or more.
 It also means getting more enlightenedmanagers.
 Clearly it’s a bad idea to hire misogynistic bullies, though it can behard to spot them in advance.
 More subtly, if you want to attract more women7We have gender balance in our natural language processing group, started in the 1960sby the late Karen Sp¨arck Jones.
Security Engineering904Ross Anderson27.
6.
 MANAGING THE TEAMand retain them, it can be an idea to manage the people rather than the work.
You have to protect your sta↵ and give them space to do what they’re goodat.
 Bullies are often creeps too; as well as bossing the people under them theysuck up to the people above them.
 Very often such people don’t understandwhat’s going on technically so they have no idea who’s productive and have tojudge people by timekeeping or by how much they ingratiate themselves.
 If thismanagement style spreads through an organisation, my advice would be to gosomewhere else.
27.
6.
3Nurturing skills and attitudesModern development has a tension between the desire to keep teams together,so that they get more e�cient and predictable, and moving people around todevelop their skills, stop them going stale, and ensure that there’s more thanone person able to maintain everything that matters.
You will also need a diversity of skills.
 If you’re writing an app, for example,you may want a couple of people to write the Android code, a couple for theApple code and a couple for the server.
 Depending on the task, there may bea user advocate who leads usability testing; advocates for safety or security; anarchitect whose job is to keep the overall design clean and e�cient; a languagelawyer who worries about APIs, a test engineer who runs the regression testingmachinery and a toolsmith who maintains the static and dynamic analysis tools.
If you’re doing continuous integration you’ll have an engineer specialising inA/B testing while if you have a gated approach the test emphasis might beon compatibility with third-party products or with security certiﬁcation.
 You’llneed to give some thought to how many of these skills you try to get in each dev,and how many are subject matter experts who work across teams or come in asconsultants.
 And as you can’t run a project with more people than you can feedfrom two pizzas, you want some of your people to have two or more of theseskills.
 Good tech ﬁrms rotate engineers slowly through the company to acquirea range of skills that maximises their value to the ﬁrm (even though it alsomaximises their value to others, and makes it easier for them to leave) [1209].
But skills are not enough: you need to get people to work together.
 Here, too,working practices have evolved over the years.
 By about 2010, agile developershad adopted the ‘scrum’ where the whole dev team has a stand-up meetingfor ﬁve minutes each day, at which the only people allowed to speak are thedevelopers.
 They describe what they’ve done, what they’re about to do andwhat the problems are.
 Some ﬁrms have moved teams to collaboration toolssuch as Jira.
 In our team we combined daily lunches together with a formalprogress meeting once a week.
(Since the coronavirus lockdown the formalmeeting has become more important and we’ve worked to complement it withother online activities.
)It’s bad practice if people who ﬁnd bugs (even bugs that they coded them-selves) just ﬁx them quietly; as bugs are correlated, there are likely to be more.
Bug tracking matters, and a ticketing system that enables good statistics to bekept is an important tool in improving quality.
 As an example of good prac-tice, in air tra�c control it’s expected that controllers making an error shouldnot only ﬁx it but declare it at once by open outcry: “I have Speedbird 123 atSecurity Engineering905Ross Anderson27.
6.
 MANAGING THE TEAMﬂight level eight zero in the terminal control area by mistake, am instructing todescend to six zero.
” That way any other controller with potentially conﬂictingtra�c can notice, shout out, and coordinate.
 Software is less dramatic, but is nodi↵erent: you need to get your devs comfortable with sharing their experiences,including their errors.
Another factor in team building is the adoption of a standard style.
 Onesignal of a poorly-managed team is that the codebase is in a chaotic mixtureof styles, with everybody doing their own thing.
 When a programmer checksout some code to work on it, they may spend half an hour formatting it andtweaking it into their own style.
 Apart from the wasted time, reformatted codecan trip up your analysis tools.
 You also want comments in the code, as peopletypically spend more time reading code than writing it.
You want to knowwhat a programmer who wrote a vulnerability thought they were doing: was ita design error, or a coding blunder? But teams can easily ﬁght about the ‘right’quantity and style of comments.
 So when you start a project, sit everyone downand let them spend an afternoon hammering out what your house style willbe.
 Provided it’s enough for reading the code later and understanding bugs,it doesn’t matter hugely what the style is: but it does matter that there is aconsistent style that people accept and that is ﬁt for purpose.
 Creating this styleis a better team-building activity than spending the afternoon paintballing, orwhatever the latest corporate team-building fad happens to be.
27.
6.
4Emergent propertiesOne debate is whether you make everyone responsible for securing their owncode, or have a security guru on whom everyone relies.
 The same question ap-plies to safety in ﬁelds such as avionics.
 The answer, as the leading ﬁrms havediscovered, is ‘both’.
 We already noted that Microsoft found it more e↵ectiveto have developers responsible for evolving their own designs and ﬁxing theirown bugs, rather than splitting these functions between analysts, programmersand testers, as IBM did in the last century.
 Both Microsoft and Google nowput rookie engineers through a security ‘boot camp’, so that everyone knows thebasics, and also have subject matter experts at a number of levels.
 These rangefrom working security consultants with a masters degree or the equivalent inter-nal qualiﬁcation, to people with PhDs in the intricate details of cryptographyor virtualisation.
The trick lies in managing the amount of specialisation in the team, and theway in which the specialists (such as the security architect and the testing guru)interact with the other developers.
27.
6.
5Evolving your workﬂowYou also need to think hard about the tools you’ll use.
 Professional developmentteams avoid a large number of the problems described in this book by usingappropriate tools.
 You avoid bu↵er overﬂows by using a modern language suchas Rust, or if you must use C or C++ then have strict coding conventionsand enforce them using static-analysis tools such as SonarQube and Coverity.
Security Engineering906Ross Anderson27.
6.
 MANAGING THE TEAMYou avoid crypto problems, such as timing attacks and weak random numbergenerators, by using well-maintained libraries.
But you need to understandthe limitations of your tools.
 In the case of Coverity, for example, its authorsexplain that while it’s great if you use it from the start of a project, adopting itin midstream imposes real costs, as you suddenly have 20,000 more bug reportsto triage, and your ship date slips by a few months [235].
 Improvements in staticanalysis tools, say in response to a new kind of attack, can also throw up a lotof alarms in an existing codebase.
 In the case of crypto libraries, we discussedin Chapter 6 how they tend to o↵er weak modes of operation such as ECB asdefaults, so you need to ensure your team uses GCM instead.
 (Crypto is one ofthe areas where you need to talk to a subject matter expert.
)You’ll be constantly adding new tools, whether to avoid cross-site scriptingvulnerabilities and SQL injection as you update your website, or to make sureyou don’t leave your client data world-readable in an S3 bucket.
 If you don’tfollow the security news you may not be aware of the latest exploits and attacks,so you may not realise when you have to either grow your own expertise or buyit in.
However you can’t just buy everything in; the security industry haslots of unscrupulous operators who exploit ignorant customers.
 You need tounderstand what you need to buy, and why, and then you will need to integrateit with your existing tools, or your security ops people will spend ever more oftheir time copying IP addresses from one tool to another.
 Doing some of yourown automation helps empower your sta↵ as well as saving time.
Your tools and libraries have to support your architecture.
 One critical thinghere is that you need to be able to evolve APIs safely.
 A system’s architectureis deﬁned more than anything else by its interfaces, and it decays by a thousandsmall cuts: by a programmer needing a ﬁle handling routine that uses twomore parameters than the existing one, and who therefore writes a new routine– which may be dangerous in itself, or may just add to complexity and thuscontribute indirectly to an eventual failure.
 In an ideal world, you’d rely on yourprogramming language to prevent API problems using type safety mechanisms.
But the cross-system fan-out of dependencies is a real hazard to safe APIs.
We saw in section 20.
5 how the APIs of cryptographic hardware security mod-ules were extended to support hundreds of banks’ legacy ATM systems until wesuddenly realised that the resulting feature interactions made them completelyinsecure.
 There are similar tensions in many other application areas, from mo-bile phone baseband software used in over a hundred di↵erent models of phone,to vehicle components used in over a hundred di↵erent cars.
 There must bebetter ways of managing this; I expect that applications with high fan-out willmove in the direction of a microservices architecture with a common core andpluggable proxies for di↵erent calling applications.
27.
6.
6And ﬁnally.
.
.
You also need to understand how to manage people, and the HR departmentcan’t do this for you8.
Tech management cannot be done by generalists as8The main job of HR is damage limitation – stopping leavers from suing you.
Security Engineering907Ross Anderson27.
7.
 SUMMARYthey’re unlikely to win the trust of their sta↵9.
 It also cannot be done well byengineers who are too introverted to engage and motivate others.
 Far too manymanagers went for the job not because they thought they might be good at it,but because it was the only way to get a decent salary.
 Successful managers intech have to love and understand tech; they also have to love and understandpeople.
For your star engineers, you need to create other leadership roles.
Theymay be innovators who will be most productive in an R&D lab.
 They may bethe custodians of your institutional memory: old-timers who know the thirtyyears of history behind your product and can stop people repeating the mistakesof the past.
 They may provide moral leadership to your engineering sta↵ andreassurance to your customers.
 They can help attract bright young recruits whowant to work with them.
 But the key, I feel, is this: that you have one or moreengineering professions in your ﬁrm.
 What’s their shape? Who leads them?How do they compare to those in your competitors? How do you grow anddevelop them? If you realise that all of a sudden you have to unify the safetyengineering and security engineering professions in your company, who is goingto do that, and how?27.
7SummaryManaging a project to build, or enhance, a system that has to meet criticalrequirements for security, safety or both, is a hard problem.
As more andmore devices acquire CPUs and communications, we need to build things thatdo real work while keeping out any vulnerabilities that would make them atarget for attack.
 In other words, you want software security – together withother functionality, and other emergent properties such as safety and real-timeperformance.
If you’re building something entirely new, or a major functional enhancementof an existing system, then understanding the requirements is often the hardestpart of the process.
 More gentle system evolution can involve subtler changesto requirements.
 Larger changes can be forced externally; systems that succeedand get popular, can expect to get attacked.
Writing secure code is hard because of this dynamic context: the ﬁrst prob-lem is to ﬁgure out what you’re trying to do.
However, even given a tightspeciﬁcation, or constant feedback from people hacking your product, you’renot home and dry.
 There are a number of challenges in hiring the right people,giving them the right tools, helping them develop the right ways of working,backing them up with expertise in the right way, and above all creating anenvironment in which they work to improve their security capability.
9As a math geek I always tended to see the MBA types and other corporate politiciansmuch as the Earl of Rochester saw King Charles II: “Here lies our sovereign lord the king,Whose word no man relies on; He never says a foolish thing, Nor ever does a wise one.
”Security Engineering908Ross Anderson27.
7.
 SUMMARYResearch ProblemsThe issues discussed in this chapter are among the hardest and the most impor-tant of any in our ﬁeld.
 However, they receive little attention because they lieat the boundaries with software engineering, applied psychology, economics andmanagement.
 Each of these interfaces could be a productive area of research.
Security economics and security psychology have made great strides in the lastfew years, and we now know we need to do a lot more work on making securitytools easier for developers to use.
 One logical next step is integrating what weknow with safety economics and safe usability.
Yet many failures are due to organisational behaviour.
 Every experienceddeveloper or security consultant has their share of horror stories about ﬁrms withperverse incentives, toxic cultures, high sta↵ turnover, incompetent managementand all the rest of the things we see in the Dilbert cartoons.
 It could be useful ifsomeone were to collect a library of case histories of security failures caused byunsatisfactory incentives in organisations, such as [876, ?].
 What might followgiven a decent empirical foundation?The late Jack Hirshleifer took the view that we should try to design organi-zations in which managers were forced to learn from their mistakes: how couldwe do that? How might you set up institutional structures to monitor changesin the threat environment and feed them through into not just systems devel-opment but into supporting activities such as internal control? Maybe we needsomething like Management as Code? How can you design an organization thatis ‘safety-incentive-compatible’ in the sense that sta↵ behave with an appropri-ate level of care? And what might the cultural anthropology of organisationshave to say? We saw in the last chapter how the response of governments to theapparently novel threats posed by Al-Qaida was maladaptive in many ways: fartoo much of our social resilience budget was spent on anti-terror theatre, at theexpense of preparedness for other societal risks such as pandemics.
 Similarly,far too much of the typical ﬁrm’s resilience budget has been captured by compli-ance, safety theatre and security theatre.
 As a result, too much of the securitydevelopment e↵ort is aimed at compliance rather than managing security andsafety risks properly.
 How can we design feedback mechanisms that will enableus to put the right amount of e↵ort in the right place? Or do we need broaderstructural change, such as the breakup of the Big Four accountancy ﬁrms?Further ReadingManaging the development of information systems has a large, di↵use and mul-tidisciplinary literature.
 There are classics everyone should read, such as FredBrooks’s ‘Mythical Man Month’ [328] and Nancy Leveson’s ‘Safeware’ [1149].
The economics of the software life cycle are discussed by Brooks and by BarryBoehm [272].
 The modern books everyone should read, as of 2020, are proba-bly the Google books on SRE [236] and on‘Building Secure and Reliable Sys-tems’ [23].
 The Microsoft approach to the security development lifecycle hasmany online resources; their doctrine on threat modelling is discussed by FrankSwiderski and Window Snyder [1851]; and their security VP Mike Nash de-Security Engineering909Ross Anderson27.
7.
 SUMMARYscribes the background to the big security push and the adoption of the securitydevelopment lifecycle at [1385].
 The most general set of standards on safetyfunctional and integrity requirements, and the associated engineering processes,is IEC 61508; there are further sets of industry-speciﬁc standards.
 For example,there’s IEC 61511 for process plant control systems, IEC 62061 for safety ofmachinery, and the EN 5012x series for railways.
 In aviation it’s RTCA DO-254for electronic hardware and RTCA DO-178C for software, while in the motorindustry it’s ISO 26262 for safety and ISO 21434 for security – though at thetime of writing this is still just a draft.
 Standards for the Internet of Things arealso a work in progress, and the current draft is ETSI EN 303 645 V2.
1.
We can learn a lot from other engineering disciplines.
 Henry Petroski dis-cusses the history of bridge building, why bridges fall down, and how civil en-gineers learned to learn from the collapses: what tends to happen is that anestablished design paradigm is stretched and stretched until it suddenly failsfor some unforeseen reason [1518].
 IT project failures are another necessarysubject of study; there’s a casebook on how to manage uncertainty in projectsby Christoph Loch, Arnoud DeMeyer and Michael Pich [1179].
 For securityfailures, it’s important to follow the leading security blogs such as Schneier onSecurity, Krebs on Security and SANS, as well as the trade press.
Organizational aspects are discussed at length in the business school liter-ature, but this can be bewildering to the outsider.
 Many business academicspraise business, which is ﬁne for selling airport books, but what we need is a morecritical understanding of how organisations fail.
 If you’re only going to read onebook, make it Lewis Pinault’s ‘Consulting Demons’ – the confessions of a formerinsider about how the big consulting ﬁrms rip o↵ their customers [1527].
 Or-ganisational theorists such as Charles Handy talk of ﬁrms having cultures basedon power, roles, tasks or people, or some combination.
 It’s not just who hasaccess to whom, but who’s prepared to listen to whom and who will just ignoreorders from whom.
 Perhaps such insights might help us design more e↵ectivetools and workﬂows that support how people actually work best.
Security Engineering910Ross Anderson