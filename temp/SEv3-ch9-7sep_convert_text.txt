Chapter 9Multilevel securityMost high assurance work has been done in the area of kineticdevices and infernal machines that are controlled by stupidrobots.
 As information processing technology becomes moreimportant to society, these concerns spread toareas previously thought inherently harmless,like operating systems.
– EARL BOEBERTThe password on the government phone always seemed to drop, andI couldn’t get into it– US diplomat and former CIA o�cer KURT VOLKER, explainingwhy he texted from his personal phoneI brief;you leak;he/she commits a criminal o↵enceby divulging classiﬁed information.
– BRITISH CIVIL SERVICE VERB9.
1IntroductionIn the next few chapters I’m going to explore the concept of a security policyusing case studies.
 A security policy is a succinct description of what we’re tryingto achieve; it’s driven by an understanding of the bad outcomes we wish to avoidand in turn drives the engineering.
 After I’ve ﬂeshed out these ideas a little,I’ll spend the rest of this chapter exploring the multilevel security (MLS) policymodel used in many military and intelligence systems, which hold informationat di↵erent levels of classiﬁcation (Conﬁdential, Secret, Top Secret, .
.
.
), andhave to ensure that data can be read only by a principal whose clearance levelis at least as high.
 Such policies are increasingly also known as information ﬂowcontrol (IFC).
2999.
2.
 WHAT IS A SECURITY POLICY MODEL?They are important for a number of reasons, even if you’re never planningto work for a government contractor:1.
 from about 1980 to about 2005, the US Department of Defense spentseveral billion dollars funding research into multilevel security.
So themodel was worked out in great detail, and we got to understand the second-order e↵ects of pursuing a single policy goal with great zeal;2.
 the mandatory access control (MAC) systems used to implement it havenow appeared in all major operating systems such as Android, iOS andWindows to protect core components against tampering by malware, as Idescribed in chapter 6;3.
 although multilevel security concepts were originally developed to supportconﬁdentiality in military systems, many commercial systems now usemultilevel integrity policies.
For example, safety-critical systems use anumber of safety integrity levels1.
The poet Archilochus famously noted that a fox knows many little things,while a hedgehog knows one big thing.
 Security engineering is usually in foxterritory, but multilevel security is an example of the hedgehog approach.
9.
2What is a Security Policy Model?Where a top-down approach to security engineering is possible, it will typicallytake the form of threat model – security policy – security mechanisms.
Thecritical, and often neglected, part of this process is the security policy.
By a security policy, we mean a document that expresses clearly and con-cisely what the protection mechanisms are to achieve.
 It is driven by our un-derstanding of threats, and in turn drives our system design.
 It will often takethe form of statements about which users may access which data.
 It plays thesame role in specifying the system’s protection requirements, and evaluatingwhether they have been met, that the system speciﬁcation does for functional-ity and the safety case for safety.
 Like the speciﬁcation, its primary function isto communicate.
Many organizations use the phrase ‘security policy’ to mean a collection ofvapid statements, as in Figure 9.
1:1Beware though that terminology varies between di↵erent safety-engineering disciplines.
The safety integrity levels in electricity generation are similar to Biba, while automotive safetyintegrity levels are set in ISO 26262 as a hazard/risk metric that depends on the likelihoodthat a fault will cause an accident, together with the expected severity and controllabilitySecurity Engineering300Ross Anderson9.
2.
 WHAT IS A SECURITY POLICY MODEL?Megacorp Inc security policy1.
 This policy is approved by Management.
2.
 All sta↵ shall obey this security policy.
3.
 Data shall be available only to those with a “need-to-know”.
4.
 All breaches of this policy shall be reported at once to Security.
Figure 9.
1 – typical corporate policy languageThis sort of language is common, but useless – at least to the security engi-neer.
 It dodges the central issue, namely ‘Who determines “need-to-know” andhow?’ Second, it mixes statements at di↵erent levels (organizational approvalof a policy should logically not be part of the policy itself).
 Third, there is amechanism but it’s implied rather than explicit: ‘sta↵ shall obey’ – but whatdoes this mean they actually have to do? Must the obedience be enforced by thesystem, or are users ‘on their honour’? Fourth, how are breaches to be detectedand who has a speciﬁc duty to report them?When you think about it, this is political language.
 A politician’s job is toresolve the tensions in society, and this often requires vague language on whichdi↵erent factions can project their own wishes; corporate executives are oftenoperating politically, to balance di↵erent factions within a company2.
Because the term ‘security policy’ is often abused to mean using security forpolitics, more precise terms have come into use by security engineers.
A security policy model is a succinct statement of the protection propertiesthat a system must have.
 Its key points can typically be written down in apage or less.
 It is the document in which the protection goals of the system areagreed with an entire community, or with the top management of a customer.
It may also be the basis of formal mathematical analysis.
A security target is a more detailed description of the protection mechanismsthat a speciﬁc implementation provides, and how they relate to a list of con-trol objectives (some but not all of which are typically derived from the policymodel).
The security target forms the basis for testing and evaluation of aproduct.
A protection proﬁle is like a security target but expressed in an implementation-independent way to enable comparable evaluations across products and versions.
This can involve the use of a semi-formal language, or at least of suitable se-curity jargon.
 A protection proﬁle is a requirement for products that are to beevaluated under the Common Criteria [1396].
 (I discuss the Common Criteria inPart III; they are used by many governments for mutual recognition of securityevaluations of defense information systems.
)When I don’t have to be so precise, I may use the phrase ‘security policy’ torefer to either a security policy model or a security target.
 I will never use it torefer to a collection of platitudes.
2Big projects often fail in companies when the speciﬁcation becomes political, and theyfail even more often when run by governments – issues I’ll discuss further in Part 3.
Security Engineering301Ross Anderson9.
3.
 MULTILEVEL SECURITY POLICYSometimes, we’re confronted with a completely new application and haveto design a security policy model from scratch.
 More commonly, there alreadyexists a model; we just have to choose the right one, and develop it into asecurity target.
 Neither of these steps is easy.
 In this section of the book, Iprovide a number of security policy models, describe them in the context of realsystems, and examine the engineering mechanisms (and associated constraints)which a security target can use to meet them.
9.
3Multilevel Security PolicyOn March 22, 1940, President Roosevelt signed Executive Order 8381, enablingcertain types of information to be classiﬁed Restricted, Conﬁdential or Se-cret [978].
 President Truman later added a higher level of Top Secret.
 Thisdeveloped into a common protective marking scheme for the sensitivity of doc-uments, and was adopted by NATO governments too in the Cold War.
 Classi-ﬁcations are labels, which run upwards from Unclassiﬁed through Conﬁdential,Secret and Top Secret (see Figure 9.
2).
 The original idea was that informa-tion whose compromise could cost lives was marked ‘Secret’ while informationwhose compromise could cost many lives was ‘Top Secret’.
 Government employ-ees and contractors have clearances depending on the care with which they’vebeen vetted; in the USA, for example, a ‘Secret’ clearance involves checking FBIﬁngerprint ﬁles, while ‘Top Secret’ also involves background checks for the pre-vious ﬁve to ﬁfteen years’ employment plus an interview and often a polygraphtest [548].
 Candidates have to disclose all their sexual partners in recent yearsand all material that might be used to blackmail them, such as teenage druguse or gay a↵airs3.
The access control policy was simple: you can read a document only if yourclearance is at least as high as the document’s classiﬁcation.
So an o�cialcleared to ‘Top Secret’ could read a ‘Secret’ document, but not vice versa.
 Soinformation may only ﬂow upwards, from conﬁdential to secret to top secret,but never downwards – unless an authorized person takes a deliberate decisionto declassify it.
TOP SECRETSECRETCONFIDENTIALUNCLASSIFIEDFigure 9.
2 – multilevel securityThe system rapidly became more complicated.
The damage criteria forclassifying documents were expanded from possible military consequences to3In June 2015, the clearance review data of about 20m Americans was stolen from theO�ce of Personnel Management by the Chinese intelligence services.
 By then, about a millionAmericans had a Top Secret clearance; the OPM data also covered former employees and jobapplicants, as well as their relatives and sexual partners.
 With hindsight, collecting all thedirt on all the citizens with a sensitive job may not have been a great idea.
Security Engineering302Ross Anderson9.
3.
 MULTILEVEL SECURITY POLICYeconomic harm and even political embarrassment.
 Information that is neitherclassiﬁed nor public is known as ‘Controlled Unclassiﬁed Information’ (CUI) inthe USA while Britain uses ‘O�cial’4.
There is also a system of codewords whereby information, especially at Se-cret and above, can be restricted further.
 For example, information that mightreveal intelligence sources or methods – such as the identities of agents or de-cryption capabilities – is typically classiﬁed ‘Top Secret Special CompartmentedIntelligence’ or TS/SCI, which means that so-called need to know restrictionsare imposed as well, with one or more codewords attached to a ﬁle.
 Some code-words relate to a particular military operation or intelligence source and areavailable only to a group of named users.
 To read a document, a user must haveall the codewords that are attached to it.
 A classiﬁcation label, plus a set ofcodewords, makes up a security category or (if there’s at least one codeword)a compartment, which is a set of records with the same access control policy.
Compartmentation is typically implemented nowadays using discretionary ac-cess control mechanisms; I’ll discuss it in the next chapter.
There are also descriptors, caveats and IDO markings.
 Descriptors are wordssuch as ‘Management’, ‘Budget’, and ‘Appointments’: they do not invoke anyspecial handling requirements, so we can deal with a ﬁle marked ‘Conﬁdential –Management’ as if it were simply marked ‘Conﬁdential’.
 Caveats are warningssuch as “UK Eyes Only”, or the US equivalent, “NOFORN”; they do createrestrictions.
 There are also International Defence Organisation markings such asNATO5.
 The lack of obvious di↵erences between codewords, descriptors, caveatsand IDO marking helps make the system confusing.
 A more detailed explanationcan be found in [1562].
9.
3.
1The Anderson reportIn the 1960s, when computers started being widely used, the classiﬁcation sys-tem caused serious friction.
Paul Karger, who worked for the USAF then,described having to log o↵ from a Conﬁdential system, walk across the yardto a di↵erent hut, show a pass to an armed guard, then go in and log on to aSecret system – over a dozen times a day.
 People soon realised they needed away to deal with information at di↵erent levels at the same desk, but how couldthis be done without secrets leaking? As soon as one operating system bug wasﬁxed, some other vulnerability would be discovered.
 The NSA hired an eminentcomputer scientist, Willis Ware, to its scientiﬁc advisory board, and in 1967he brought the extent of the computer security problem to o�cial and publicattention [1985].
 There was the constant worry that even unskilled users would4Prior to adopting the CUI system, the United States had more than 50 di↵erent mark-ings for data that was controlled but not classiﬁed, including For O�cial Use Only (FOUO),Law Enforcement Sensitive (LES), Proprietary (PROPIN), Federal Tax Information (FTI),Sensitive but Unclassiﬁed (SBU), and many, many others.
 Some agencies made up their ownlabels, without any coordination.
 Further problems arose when civilian documents markedConﬁdential ended up at the National Archives and Records Administration, where CONFI-DENTIAL was a national security classiﬁcation.
 Moving from this menagerie of markings toa single centrally-managed government-wide system has taken more than a decade and is stillongoing.
 The UK has its own post-Cold-War simpliﬁcation story.
5Curiously, in the UK ‘NATO Secret’ is less secret than ‘Secret’, so it’s a kind of anti-codeword that moves the content down the lattice rather than up.
Security Engineering303Ross Anderson9.
3.
 MULTILEVEL SECURITY POLICYdiscover loopholes and use them opportunistically; there was also a keen andgrowing awareness of the threat from malicious code.
 (Viruses were not inventeduntil the 1980s; the 70’s concern was Trojans.
) There was then a serious scarewhen it was discovered that the Pentagon’s World Wide Military Command andControl System (WWMCCS) was vulnerable to Trojan Horse attacks; this hadthe e↵ect of restricting its use to people with a ‘Top Secret’ clearance, whichwas inconvenient.
The next step was a 1972 study by James Anderson for the US governmentwhich concluded that a secure system should do one or two things well; andthat these protection properties should be enforced by mechanisms which weresimple enough to verify and that would change only rarely [51].
 It introduced theconcept of a reference monitor – a component of the operating system whichwould mediate access control decisions and be small enough to be subject toanalysis and tests, the completeness of which could be assured.
In modernparlance, such components – together with their associated operating procedures– make up the Trusted Computing Base (TCB).
 More formally, the TCB isdeﬁned as the set of components (hardware, software, human, .
.
.
) whose correctfunctioning is su�cient to ensure that the security policy is enforced, or, morevividly, whose failure could cause a breach of the security policy.
 The Andersonreport’s goal was to make the security policy simple enough for the TCB to beamenable to careful veriﬁcation.
9.
3.
2The Bell-LaPadula modelThe multilevel security policy model that gained wide acceptance was proposedby Dave Bell and Len LaPadula in 1973 [210].
 Its basic property is that infor-mation cannot ﬂow downwards.
 More formally, the Bell-LaPadula (BLP) modelenforces two properties:• The simple security property: no process may read data at a higher level.
This is also known as no read up (NRU);• The *-property: no process may write data to a lower level.
 This is alsoknown as no write down (NWD).
The *-property was Bell and LaPadula’s critical innovation.
 It was drivenby the WWMCCS debacle and the more general fear of Trojan-horse attacks.
An uncleared user might write a Trojan and leave it around where a systemadministrator cleared to ‘Secret’ might execute it; it could then copy itself intothe ‘Secret’ part of the system, read the data there and try to signal it downsomehow.
 It’s also quite possible that an enemy agent could get a job at a com-mercial software house and embed some code in a product that would look forsecret documents to copy.
 If it could then write them down to where its creatorcould read them, the security policy would have been violated.
Informationmight also be leaked as a result of a bug, if applications could write down.
Vulnerabilities such as malicious and buggy code are assumed to be given.
It is also assumed that most sta↵ are careless, and some are dishonest; exten-sive operational security measures have long been used, especially in defenceSecurity Engineering304Ross Anderson9.
3.
 MULTILEVEL SECURITY POLICYenvironments, to prevent people leaking paper documents.
 So the pre-existingculture assumed that security policy was enforced independently of user actions;Bell-LaPadula sets out to enforce it not just independently of users’ direct ac-tions, but of their indirect actions (such as the actions taken by programs theyrun).
So we must prevent programs running at ‘Secret’ from writing to ﬁles at ‘Un-classiﬁed’.
 More generally we must prevent any process at High from signallingto any object at Low.
 Systems that enforce a security policy independently ofuser actions are described as having mandatory access control, as opposed tothe discretionary access control in systems like Unix where users can take theirown access decisions about their ﬁles.
The Bell-LaPadula model enabled designers to prove theorems.
 Given boththe simple security property (no read up), and the star property (no write down),various results can be proved: in particular, if your starting state is secure, thenyour system will remain so.
 To keep things simple, we will generally assumefrom now on that the system has only two levels, High and Low.
9.
3.
3The standard criticisms of Bell-LaPadulaThe introduction of BLP caused a lot of excitement: here was a security policythat did what the defence establishment thought it wanted, was intuitively clear,yet still allowed people to prove theorems.
 Researchers started to beat up on itand reﬁne it.
The ﬁrst big controversy was about John McLean’s System Z, which hedeﬁned as a BLP system with the added feature that a user can ask the systemadministrator to temporarily declassify any ﬁle from High to Low.
 In this way,Low users can read any High ﬁle without breaking the BLP assumptions.
 DaveBell countered that System Z cheats by doing something his model doesn’t allow(changing labels isn’t a valid operation on the state), and John McLean’s retortwas that it didn’t explicitly tell him so: so the BLP rules were not in themselvesenough.
 The issue is dealt with by introducing a tranquility property.
 Strongtranquility says that security labels never change during system operation, whileweak tranquility says that labels never change in such a way as to violate adeﬁned security policy.
Why weak tranquility? In a real system we often want to observe the prin-ciple of least privilege and start o↵ a process at the uncleared level, even ifthe owner of the process were cleared to ‘Top Secret’.
 If they then access aconﬁdential email, their session is automatically upgraded to ‘Conﬁdential’; ingeneral, a process is upgraded each time it accesses data at a higher level (thehigh water mark principle).
 As subjects are usually an abstraction of the mem-ory management sub-system and ﬁle handles, rather than processes, this meansthat state changes when access rights change, rather than when data actuallymoves.
The practical implication is that a process acquires the security labels of allthe ﬁles it reads, and these become the default label set of every ﬁle that itwrites.
 So a process which has read ﬁles at ‘Secret’ and ‘Crypto’ will thereaftercreate ﬁles marked ‘Secret Crypto’.
 This will include temporary copies made ofSecurity Engineering305Ross Anderson9.
3.
 MULTILEVEL SECURITY POLICYother ﬁles.
 If it then reads a ﬁle at ‘Secret Nuclear’ then all ﬁles it creates afterthat will be labelled ‘Secret Crypto Nuclear’, and it will not be able to write toany temporary ﬁles at ‘Secret Crypto’.
The e↵ect this has on applications is one of the serious complexities of mul-tilevel security; most application software needs to be rewritten (or at leastmodiﬁed) to run on MLS platforms.
 Real-time changes in security level meanthat access to resources can be revoked at any time, including in the middle ofa transaction.
 And as the revocation problem is generally unsolvable in mod-ern operating systems, at least in any complete form, the applications have tocope somehow.
 Unless you invest some care and e↵ort, you can easily ﬁnd thateverything ends up in the highest compartment – or that the system fragmentsinto thousands of tiny compartments that don’t communicate at all with eachother.
 In order to prevent this, labels are now generally taken outside the MLSmachinery and dealt with using discretionary access control mechanisms (I’lldiscuss this in the next chapter).
Another problem with BLP, and indeed with all mandatory access controlsystems, is that separating users and processes is the easy part; the hard part iswhen some controlled interaction is needed.
 Most real applications need somekind of trusted subject that can break the security policy; the classic examplewas a trusted word processor that helps an intelligence analyst scrub a TopSecret document when she’s editing it down to Secret [1270].
 BLP is silent onhow the system should protect such an application.
 So it becomes part of theTrusted Computing Base, but a part that can’t be veriﬁed using models basedsolely on BLP.
Finally it’s worth noting that even with the high-water-mark reﬁnement,BLP still doesn’t deal with the creation or destruction of subjects or objects(which is one of the hard problems of building a real MLS system).
9.
3.
4The evolution of MLS policiesMultilevel security policies have evolved in parallel in both the practical andresearch worlds.
The ﬁrst multilevel security policy was a version of high water mark writ-ten in 1967–8 for the ADEPT-50, a mandatory access control system developedfor the IBM S/360 mainframe [2006].
 This used triples of level, compartmentand group, with the groups being ﬁles, users, terminals and jobs.
 As programs(rather than processes) were subjects, it was vulnerable to Trojan horse compro-mises.
 Nonetheless, it laid the foundation for BLP, and also led to the currentIBM S/390 mainframe hardware security architecture [940].
The next big step was Multics.
This had started as an MIT project in1965 and developed into a Honeywell product; it became the template andinspirational example for ‘trusted systems’.
 The evaluation that was carriedout on it by Paul Karger and Roger Schell was hugely inﬂuential and was theﬁrst appearance of the idea that malware could be hidden in the compiler [1019]– and led to Ken Thompson’s famous paper ‘Reﬂections on Trusting Trust’ten years later [1883].
 Multics had a derivative system called SCOMP that I’lldiscuss in section 9.
4.
1 .
Security Engineering306Ross Anderson9.
3.
 MULTILEVEL SECURITY POLICYThe torrent of research money that poured into multilevel security from the1980s led to a number of alternative formulations.
 Noninterference was intro-duced by Joseph Goguen and Jose Meseguer in 1982 [773].
 In a system with thisproperty, High’s actions have no e↵ect on what Low can see.
 Nondeducibility isless restrictive and was introduced by David Sutherland in 1986 [1847] to modelapplications such as a LAN on which there are machines at both Low and High,with the High machines encrypting their LAN tra�c6.
 Nondeducibility turnedout to be too weak, as there’s nothing to stop Low making deductions aboutHigh input with 99% certainty.
 Other theoretical models include GeneralizedNoninterference and restrictiveness [1276]; the Harrison-Ruzzo-Ullman modeltackles the problem of how to deal with the creation and deletion of ﬁles, onwhich BLP is silent [868]; and the Compartmented Mode Workstation (CMW)policy attempted to model the classiﬁcation of information using ﬂoating labels,as in the high water mark policy [2040, 807].
Out of this wave of innovation, the model with the greatest impact on modernsystems is probably the type enforcement (TE) model, due to Earl Boebertand Dick Kain [271], later extended by Lee Badger and others to Domain andType Enforcement (DTE) [153].
 This assigns subjects to domains and objectsto types, with matrices deﬁning permitted domain-domain and domain-typeinteractions.
This is used in SELinux, now a component of Android, whichsimpliﬁes it by putting both subjects and objects in types and having a matrixof allowed type pairs [1187].
 In e↵ect this is a second access-control matrix; inaddition to having a user ID and group ID, each process has a security ID (SID).
The Linux Security Modules framework provides pluggable security where youcan set rules that operate on SIDs.
DTE introduced a language for conﬁguration (DTEL), and implicit typing ofﬁles based on pathname; so all objects in a given subdirectory may be declaredto be in a given domain.
 DTE is more general than BLP, as it starts to deal withintegrity as well as conﬁdentiality concerns.
 One of the early uses was to enforcetrusted pipelines: the idea is to conﬁne a set of processes in a pipeline so thateach can only talk to the previous stage and the next stage.
 This can be usedto assemble guards and ﬁrewalls which cannot be bypassed unless at least twostages are compromised [1430].
 Type-enforcement mechanisms can be awareof code versus data, and privileges can be bound to code; in consequence thetranquility problem can be dealt with at execute time rather than as data areread.
 This can make things much more tractable.
 They are used, for example,in the Sidewinder ﬁrewall.
The downside of the greater ﬂexibility and expressiveness of TE/DTE isthat it is not always straightforward to implement policies like BLP, becauseof state explosion; when writing a security policy you have to consider all thepossible interactions between di↵erent types.
 Other mechanisms may be usedto manage policy complexity, such as running a prototype for a while to observewhat counts as normal behaviour; you can then turn on DTE and block all theinformation ﬂows not seen to date.
 But this doesn’t give much assurance that6Quite a lot else is needed to do this right, such as padding the High tra�c with nulls sothat Low users can’t do tra�c analysis – see [1632] for an early example of such a system.
 Youmay also need to think about Low tra�c over a High network, such as facilities for soldiers tophone home.
Security Engineering307Ross Anderson9.
3.
 MULTILEVEL SECURITY POLICYthe policy you’ve derived is the right one.
In 1992, role-based access control (RBAC) was introduced by David Ferraioloand Richard Kuhn to manage policy complexity.
 It formalises rules that attachprimarily to roles rather than to individual users or machines [678, 679].
 Trans-actions that may be performed by holders of a given role are speciﬁed, thenmechanisms for granting membership of a role (including delegation).
 Roles, orgroups, had for years been the mechanism used in practice in organizations suchas banks to manage access control; the RBAC model started to formalize this.
It can be used to give ﬁner-grained control, for example by granting di↵erentaccess rights to ‘Ross as Professor’, ‘Ross as member of the Admissions Com-mittee’ and ‘Ross reading private email’.
 A variant of it, aspect-based accesscontrol (ABAC), adds context, so you can distinguish ‘Ross at his workstationin the lab’ from ‘Ross on his phone somewhere on Earth’.
Both have beensupported by Windows since Windows 8.
SELinux builds it on top of TE, so that users are mapped to roles at logintime, roles are authorized for domains and domains are given permissions totypes.
 On such a platform, RBAC can usefully deal with integrity issues aswell as conﬁdentiality, by allowing role membership to be revised when certainprograms are invoked.
 Thus, for example, a process calling untrusted softwarethat had been downloaded from the net might lose the role membership requiredto write to sensitive system ﬁles.
 I discuss SELinux in more detail at 9.
5.
2.
9.
3.
5The Biba modelThe incorporation into Windows 7 of a multilevel integrity model revived interestin a security model devised in 1975 by Ken Biba [237], which deals with integrityalone and ignores conﬁdentiality.
 Biba’s observation was that conﬁdentiality andintegrity are in some sense dual concepts – conﬁdentiality is a constraint on whocan read a message, while integrity is a constraint on who can write or alter it.
So you can recycle BLP into an integrity policy by turning it upside down.
As a concrete application, an electronic medical device such as an ECGmay have two separate modes: calibration and use.
 Calibration data must beprotected from corruption, so normal users should be able to read it but notwrite to it; when a normal user resets the device, it will lose its current user state(i.
e.
, any patient data in memory) but the calibration must remain unchanged.
Only an authorised technician should be able to redo the calibration.
To model such a system, we can use a multilevel integrity policy with therules that we can read data at higher levels (i.
e.
, a user process can read thecalibration data) and write to lower levels (i.
e.
, a calibration process can writeto a bu↵er in a user process); but we must never read down or write up, aseither could allow High integrity objects to become contaminated with Low –i.
e.
 potentially unreliable – data.
 The Biba model is often formulated in terms ofthe low water mark principle, which is the dual of the high water mark principlediscussed above: the integrity of an object is the lowest level of all the objectsthat contributed to its creation.
This was the ﬁrst formal model of integrity.
 A surprisingly large numberof real systems work along Biba lines.
For example, the passenger informa-Security Engineering308Ross Anderson9.
4.
 HISTORICAL EXAMPLES OF MLS SYSTEMStion system in a railroad may get information from the signalling system, butshouldn’t be able to a↵ect it; and an electricity utility’s power dispatching sys-tem will be able to see the safety systems’ state but not interfere with them.
The safety-critical systems community talks in terms of safety integrity levels,which relate to the probability that a safety mechanism will fail and to the levelof risk reduction it is designed to give.
Windows, since version 6 (Vista), marks ﬁle objects with an integrity level,which can be Low, Medium, High or System, and implements a default policyof NoWriteUp.
 Critical ﬁles are at System and other objects are at Medium bydefault – except for the browser which is at Low.
 So things downloaded usingIE can read most ﬁles in a Windows system, but cannot write to them.
 Thegoal is to limit the damage that can be done by malware.
As you might expect, Biba has the same fundamental problems as Bell-LaPadula.
 It cannot accommodate real-world operation very well without nu-merous exceptions.
 For example, a real system will usually require trusted sub-jects that can override the security model, but Biba on its own cannot protectand conﬁne them, any more than BLP can.
 For example, a car’s airbag is on aless critical bus than the engine, but when it deploys you assume there’s a riskof a fuel ﬁre and switch the engine o↵.
 There are other real integrity goals thatBiba also cannot express, such as assured pipelines.
 In the case of Windows,Microsoft even dropped the NoReadDown restriction and did not end up usingits integrity model to protect the base system from users, as this would haverequired even more frequent user conﬁrmation.
 In fact, the Type Enforcementmodel was introduced by Boebert and Kain as an alternative to Biba.
 It isunfortunate that Windows didn’t incorporate TE.
9.
4Historical Examples of MLS SystemsThe second edition of this book had a much fuller history of MLS systems; sincethese have largely gone out of fashion, and the MLS research programme hasbeen wound down, I give a shorter version here.
9.
4.
1SCOMPA key product was the secure communications processor (SCOMP), a derivativeof Multics launched in 1983 [710].
 This was a no-expense-spared implementationof what the US Department of Defense believed it wanted for handling messagingat multiple levels of classiﬁcation.
 It had formally veriﬁed hardware and soft-ware, with a minimal kernel to keep things simple.
 Its operating system, STOP,used Multics’ system of rings to maintain up to 32 separate compartments, andto allow appropriate one-way information ﬂows between them.
SCOMP was used in applications such as military mail guards.
 These are ﬁre-walls that allow mail to pass from Low to High but not vice versa [538].
 (In gen-eral, a device which supports one-way ﬂow is known as a data diode.
) SCOMP’ssuccessor, XTS-300, supported C2G, the Command and Control Guard.
 Thiswas used in the time phased force deployment data (TPFDD) system whoseSecurity Engineering309Ross Anderson9.
4.
 HISTORICAL EXAMPLES OF MLS SYSTEMSfunction was to plan US troop movements and associated logistics.
 SCOMP’smost signiﬁcant contribution was to serve as a model for the Orange Book [544]– the US Trusted Computer Systems Evaluation Criteria.
 This was the ﬁrstsystematic set of standards for secure computer systems, being introduced in1985 and ﬁnally retired in December 2000.
 The Orange Book was enormouslyinﬂuential not just in the USA but among allied powers; countries such as theUK, Germany, and Canada based their own national standards on it, until thesenational standards were ﬁnally subsumed into the Common Criteria [1396].
The Orange Book allowed systems to be evaluated at a number of levelswith A1 being the highest, and moving downwards through B3, B2, B1 and C2to C1.
 SCOMP was the ﬁrst system to be rated A1.
 It was also extensivelydocumented in the open literature.
 Being ﬁrst, and being fairly public, it set atarget for the next generation of military systems.
MLS versions of Unix started to appear in the late 1980s, such as AT&T’sSystem V/MLS [47].
 This added security levels and labels, showing that MLSproperties could be introduced to a commercial operating system with minimalchanges to the system kernel.
 By this book’s second edition (2007), Sun’s Solarishad emerged as the platform of choice for high-assurance server systems and formany clients as well.
 Comparted Mode Workstations (CMWs) were an exampleof the latter, allowing data at di↵erent levels to be viewed and modiﬁed at thesame time, so an intelligence analyst could read ‘Top Secret’ data in one windowand write reports at ‘Secret’ in another, without being able to accidentally copyand paste text downwards [932].
 For the engineering, see [635, 636].
9.
4.
2Data diodesIt was soon realised that simple mail guards and crypto boxes were too restric-tive, as more complex networked services were developed besides mail.
 First-generation MLS mechanisms were ine�cient for real-time services.
HIGHPUMPLOWFigure 9.
3: – the NRL pumpThe US Naval Research Laboratory (NRL) therefore developed the Pump –a one-way data transfer device (a data diode) to allow secure one-way informa-tion ﬂow (Figure 9.
3.
 The main problem is that while sending data from Low toHigh is easy, the need for assured transmission reliability means that acknowl-Security Engineering310Ross Anderson9.
4.
 HISTORICAL EXAMPLES OF MLS SYSTEMSedgement messages must be sent back from High to Low.
 The Pump limits thebandwidth of possible backward leakage using a number of mechanisms suchas bu↵ering and random timing of acknowledgements [1012, 1013, 1014].
 Theattraction of this approach is that one can build MLS systems by using datadiodes to connect separate systems at di↵erent security levels.
 As these systemsdon’t process data at more than one level – an architecture called system high– they can be built from cheap commercial-o↵-the-shelf (COTS) components.
You don’t need to worry about applying MLS internally, merely protecting themfrom external attack, whether physical or network-based.
 As the cost of hard-ware has fallen, this has become the preferred option, and the world’s militarybases are now full of KVM switches (which let people switch their keyboard,video display and mouse between Low and High systems) and data diodes (tolink Low and High networks).
 The pump’s story is told in [1015].
An early application was logistics.
 Some signals intelligence equipment is‘Top Secret’, while things like jet fuel and bootlaces are not; but even suchsimple commodities may become ‘Secret’ when their quantities or movementsmight leak information about tactical intentions.
 The systems needed to manageall this can be hard to build; MLS logistics projects in both the USA and UKhave ended up as expensive disasters.
 In the UK, the Royal Air Force’s LogisticsInformation Technology System (LITS) was a 10 year (1989–99), £500m projectto provide a single stores management system for the RAF’s 80 bases [1386].
It was designed to operate on two levels: ‘Restricted’ for the jet fuel and bootpolish, and ‘Secret’ for special stores such as nuclear bombs.
 It was initiallyimplemented as two separate database systems connected by a pump to enforcethe MLS property.
 The project became a classic tale of escalating costs drivenby creeping changes in requirements.
 One of these changes was the easing ofclassiﬁcation rules with the end of the Cold War.
 As a result, it was found thatalmost all the ‘Secret’ information was now static (e.
g.
, operating manuals forair-drop nuclear bombs that are now kept in strategic stockpiles rather than atairbases).
 To save money, the ‘Secret’ information is now kept on a CD andlocked up in a safe.
Another major application of MLS is in wiretapping.
 The target of inves-tigation should not know they are being wiretapped, so the third party mustbe silent – and when phone companies started implementing wiretaps as silentconference calls, the charge for the conference call had to go to the wiretapper,not to the target.
 The modern requirement is a multilevel one: multiple agen-cies at di↵erent levels may want to monitor a target, and each other, with thepolice tapping a drug dealer, an anti-corruption unit watching the police, andso on.
 Eliminating covert channels is harder than it looks; for a survey fromthe mid-2000s, see [1707]; a pure MLS security policy is insu�cient, as suspectscan try to hack or confuse wiretapping equipment, which therefore needs to re-sist online tampering.
 In one notorious case, a wiretap was discovered on themobile phones of the Greek Prime Minister and his senior colleagues during theAthens olympics; the lawful intercept facility in the mobile phone company’sswitchgear was abused by unauthorised software, and was detected when thebuggers’ modiﬁcations caused some text messages not to be delivered [1550].
The phone company was ﬁned 76 million Euros (almost $100m).
 The clean wayto manage wiretaps nowadays with modern VOIP systems may just be to writeeverything to disk and extract what you need later.
Security Engineering311Ross Anderson9.
5.
 MAC: FROM MLS TO IFC AND INTEGRITYThere are many military embedded systems too.
 In submarines, speed, reac-tor output and RPM are all Top Secret, as a history of these three measurementswould reveal the vessel’s performance – and that’s among the few pieces of in-formation that even the USA and the UK don’t share.
 The engineering is mademore complex by the need for the instruments not to be Top Secret when thevessel is in port, as that would complicate maintenance.
 And as for air combat,some US radars won’t display the velocity of a US aircraft whose performanceis classiﬁed, unless the operator has the appropriate clearance.
 When you readstories about F-16 pilots seeing an insanely fast UFO whose speed on their radardidn’t make any sense, you can put two and two together.
 It will be interestingto see what sort of other side-e↵ects follow when powerful actors try to bakeMAC policies into IoT infrastructure, and what sort of superstitious beliefs theygive rise to.
9.
5MAC: from MLS to IFC and integrityIn the ﬁrst edition of this book, I noted a trend to use mandatory access controlsto prevent tampering and provide real-time performance guarantees [1313, 1018],and ventured that “perhaps the real future of multilevel systems is not in conﬁ-dentiality, but integrity.
” Government agencies had learned that MAC was whatit took to stop malware.
 By the second edition, multilevel integrity had hit themass market in Windows, which essentially uses the Biba model.
9.
5.
1WindowsIn Windows, all processes do, and all securable objects (including directories,ﬁles and registry keys) may, have an integrity-level label.
 File objects are la-belled ’Medium’ by default, while Internet Explorer (and everything downloadedusing it) is labelled ’Low’.
 User action is therefore needed to upgrade down-loaded content before it can modify existing ﬁles.
 It’s also possible to implementa crude BLP policy using Windows, as you can also set ‘NoReadUp’ and ‘NoEx-ecuteUp’ policies.
 These are not installed as default; Microsoft was concernedabout malware installing itself in the system and then hiding.
Keeping thebrowser ‘Low’ makes installation harder, and allowing all processes (even Lowones) to inspect the rest of the system makes hiding harder.
 But this integrity-only approach to MAC does mean that malware running at Low can steal allyour data; so some users might care to set ‘NoReadUp’ for sensitive directories.
This is all discussed by Joanna Rutkowska in [1634]; she also describes someinteresting potential attacks based on virtualization.
9.
5.
2SELinuxThe case of SELinux is somewhat similar to Windows in that the immediategoal of mandatory access control mechanisms was also to limit the e↵ects of acompromise.
 SELinux [1187] was implemented by the NSA, based on the Flasksecurity architecture [1811], which separates the policy from the enforcementmechanism; a security context contains all of the security attributes associatedSecurity Engineering312Ross Anderson9.
6.
 WHAT GOES WRONGwith a subject or object in Flask, where one of those attributes includes theType Enforcement type attribute.
 A security identiﬁer is a handle to a securitycontext, mapped by the security server.
 This is where policy decisions are madeand resides in the kernel for performance [819].
 It has been mainstream sinceLinux 2.
6.
 The server provides a security API to the rest of the kernel, behindwhich the security model is hidden.
 The server internally implements a generalconstraints engine that can express RBAC, TE, and MLS.
 In typical Linuxdistributions from the mid-2000s, it was used to separate various services, soan attacker who takes over your web server does not thereby acquire your DNSserver as well.
 Its adoption by Android has made it part of the world’s mostpopular operating system, as described in chapter 6.
9.
5.
3Embedded systemsThere are many ﬁelded systems that implement some variant of the Biba model.
As well as the medical-device and railroad signalling applications I already men-tioned, there are utilities.
 In an electricity utility, for example, there is typicallya hierarchy of safety systems, which operate completely independently at thehighest safety integrity level; these are visible to, but cannot be inﬂuenced by,operational systems such as power dispatching; retail-level metering systems canbe observed by, but not inﬂuenced by, the billing system.
 Both retail metersand the substation-level meters in the power-dispatching system feed informa-tion into fraud detection, and ﬁnally there are the executive information sys-tems, which can observe everything while having no direct e↵ect on operations.
In cars, most makes have separate CAN buses for the powertrain and for thecabin, as you don’t want a malicious app on your radio to be able to operateyour brakes (though in 2010, security researchers found that the separation wascompletely inadequate [1085]).
It’s also worth bearing in mind that simple integrity controls merely stopmalware taking over the machine – they don’t stop it infecting a Low compart-ment and using that as a springboard from which to spread elsewhere, or toissue instructions to other machines.
To sum up, many of the lessons learned in the early multilevel systems goacross to a number of applications of wider interest.
 So do a number of thefailure modes, which I’ll now discuss.
9.
6What Goes WrongEngineers learn more from the systems that fail than from those that succeed,and here MLS systems have been an e↵ective teacher.
 The billions of dollarsspent on building systems to follow a simple policy with a high level of assurancehave clariﬁed many second-order and third-order consequences of informationﬂow controls.
I’ll start with the more theoretical and work through to thebusiness and engineering end.
Security Engineering313Ross Anderson9.
6.
 WHAT GOES WRONG9.
6.
1ComposabilityConsider a simple device that accepts two ‘High’ inputs H1 and H2; multiplexesthem; encrypts them by xor’ing them with a one-time pad (i.
e.
, a random gen-erator); outputs the other copy of the pad on H3; and outputs the ciphertext,which being encrypted with a cipher system giving perfect secrecy, is consideredto be low (output L), as in Figure 9.
4.
H1-H2-XORRAND-H3XOR•--- LFigure 9.
4 – insecure composition of secure systems with feedbackIn isolation, this device is provably secure.
 However, if feedback is permit-ted, then the output from H3 can be fed back into H2, with the result thatthe high input H1 now appears at the low output L.
 Timing inconsistenciescan also break the composition of two secure systems (noted by Daryl McCul-lough [1260]).
In general, the composition problem – how to compose two or more securecomponents into a secure system – is hard, even at the relatively unclutteredlevel of proving results about ideal components [1430].
 (Simple information ﬂowdoesn’t compose; neither does noninterference or nondeducibility.
) Most of thelow-level problems arise when some sort of feedback is introduced; without it,composition can be achieved under a number of formal models [1277].
 However,in real life, feedback is pervasive, and composition of security properties canbe made even harder by interface issues, feature interactions and so on.
 Forexample, one system might produce data at such a rate as to perform a service-denial attack on another.
 And the composition of secure components is oftenfrustrated by higher-level incompatibilities.
 Components might have been de-signed in accordance with two di↵erent security policies, or designed accordingto inconsistent requirements.
9.
6.
2The cascade problemAn example of the composition problem is given by the cascade problem (Fig-ure 9.
5).
 After the Orange book introduced a series of evaluation levels, thisled to span-limit rules about the number of levels at which a system can op-erate [548].
 For example, a system evaluated to B3 was in general allowed toSecurity Engineering314Ross Anderson9.
6.
 WHAT GOES WRONGprocess information at Unclassiﬁed, Conﬁdential and Secret, or at Conﬁdential,Secret and Top Secret; there was no system permitted to process Unclassiﬁedand Top Secret data simultaneously [548].
Top SecretSecretSecretUnclassifiedFigure 9.
5: – the cascade problemAs the diagram shows, it is straightforward to connect together two B3systems in such a way that this policy is broken.
 The ﬁrst system connectstogether Unclassiﬁed and Secret, and its Secret level communicates with thesecond system – which also processes Top Secret information [923].
 This defeatsthe span limit.
9.
6.
3Covert channelsOne of the reasons why span limits are imposed on multilevel systems emergesfrom a famous – and extensively studied – problem: the covert channel.
 Firstpointed out by Lampson in 1973 [1125], a covert channel is a mechanism thatwas not designed for communication but which can nonetheless be abused toallow information to be communicated down from High to Low.
A typical covert channel arises when a high process can signal to a low oneby a↵ecting some shared resource.
 In a modern multicore CPU, it could increasethe clock frequency of the CPU core it’s using at time ti to signal that the i-thbit in a Top Secret ﬁle was a 1, and let it scale back to signal that the bit was a 0.
This gives a covert channel capacity of several tens of bits per second [35].
 Since2018, CPU designers have been struggling with a series of cover channels thatexploit the CPU microarchitecture; with names like Meltdown, Spectre, andForeshadow, they have provided not just ways for High to signal to Low but forLow to circumvent access control and read memory at High.
 I will discuss thesein detail in the chapter on side channels.
The best that developers have been able to do consistently with conﬁdential-ity protection in regular operating systems is to limit it to 1 bit per second orso.
 (That is a DoD target [545], and techniques for doing a systematic analysismay be found in Kemmerer [1036].
) One bit per second may be tolerable inan environment where we wish to prevent large TS/SCI ﬁles – such as satelliteSecurity Engineering315Ross Anderson9.
6.
 WHAT GOES WRONGphotographs – leaking down from TS/SCI users to ‘Secret’ users.
 However, it’spotentially a lethal threat to high-value cryptographic keys.
 This is one of thereasons for the military and banking doctrine of doing crypto in special purposehardware.
The highest-bandwidth covert channel of which I’m aware occurs in largeearly-warning radar systems, where High – the radar processor – controls hun-dreds of antenna elements that illuminate Low – the target – with high speedpulse trains, which are modulated with pseudorandom noise to make jammingharder.
In this case, the radar code must be trusted as the covert channelbandwidth is many megabits per second.
9.
6.
4The threat from malwareThe defense computer community was shocked when Fred Cohen wrote the ﬁrstthesis on computer viruses, and used a virus to penetrate multilevel secure sys-tems easily in 1983.
 In his ﬁrst experiment, a ﬁle virus that took only eighthours to write managed to penetrate a system previously believed to be multi-level secure [450].
 People had been thinking about malware since the 1960s andhad done various things to mitigate it, but their focus had been on Trojans.
There are many ways in which malicious code can be used to break accesscontrols.
 If the reference monitor (or other TCB components) can be corrupted,then malware can deliver the entire system to the attacker, for example byissuing an unauthorised clearance.
 For this reason, slightly looser rules applyto so-called closed security environments which are deﬁned to be those where‘system applications are adequately protected against the insertion of maliciouslogic’ [548], and this in turn created an incentive for vendors to tamper-proofthe TCB, using techniques such as TPMs.
 But even if the TCB remains intact,malware could still copy itself up from Low to High (which BLP doesn’t prevent)and use a covert channel to signal information down.
9.
6.
5PolyinstantiationAnother problem that exercised the research community is polyinstantiation.
Suppose our High user has created a ﬁle named agents, and our Low user nowtries to do the same.
 If the MLS operating system prohibits him, it will haveleaked information – namely that there is a ﬁle called agents at High.
 But if itlets him, it will now have two ﬁles with the same name.
Often we can solve the problem by a naming convention, such as giving Lowand High users di↵erent directories.
 But the problem remains a hard one fordatabases [1649].
 Suppose that a High user allocates a classiﬁed cargo to a ship.
The system will not divulge this information to a Low user, who might thinkthe ship is empty, and try to allocate it another cargo or even to change itsdestination.
Here the US and UK practices diverge.
 The solution favoured in the USAis that the High user allocates a Low cover story at the same time as the realHigh cargo.
 Thus the underlying data will look something like Figure 9.
6.
Security Engineering316Ross Anderson9.
6.
 WHAT GOES WRONGLevelCargoDestinationSecretMissilesIranRestricted––UnclassiﬁedEngine sparesCyprusFigure 9.
6 – how the USA deals with classiﬁed dataIn the UK, the theory is simpler – the system will automatically reply ‘clas-siﬁed’ to a Low user who tries to see or alter a High record.
 The two availableviews would be as in Figure 9.
7.
LevelCargoDestinationSecretMissilesIranRestrictedClassiﬁedClassiﬁedUnclassiﬁed––Figure 9.
7 – how the UK deals with classiﬁed dataThis makes the system engineering simpler.
 It also prevents the mistakesand covert channels that can still arise with cover stories (e.
g.
, a Low user triesto add a container of ammunition for Cyprus).
 The drawback is that everyonetends to need the highest available clearance in order to get their work done.
(In practice, cover stories still get used in order not to advertise the existenceof a covert mission any more than need be.
)9.
6.
6Practical problems with MLSMultilevel secure systems are surprisingly expensive and di�cult to build anddeploy.
 There are many sources of cost and confusion.
1.
 They are built in small volumes, and often to high standards of physi-cal robustness, using elaborate documentation, testing and other qualitycontrol measures driven by military purchasing bureaucracies.
2.
 MLS systems have idiosyncratic administration tools and procedures.
 Atrained Unix administrator can’t just take on an MLS installation withoutsigniﬁcant further training; so many MLS systems are installed withouttheir features being used.
3.
 Many applications need to be rewritten or at least greatly modiﬁed to rununder MLS operating systems [1629].
4.
 Because processes are automatically upgraded as they see new labels, theﬁles they use have to be too.
 New ﬁles default to the highest label belong-ing to any possible input.
 The result of all this is a chronic tendency forthings to be overclassiﬁed.
 There’s a particular problem when system com-ponents accumulate all the labels they’ve seen, leading to label explosionSecurity Engineering317Ross Anderson9.
6.
 WHAT GOES WRONGwhere they acquire such a collection that no single principal can accessthem any more.
 So they get put in the trusted computing base, whichends up containing a quite uncomfortably large part of the operating sys-tem (plus utilities, plus windowing system software, plus middleware suchas database software).
 This ‘TCB bloat’ constantly pushes up the cost ofevaluation and reduces assurance.
5.
 The classiﬁcation of data can get complex:• in the run-up to a conﬂict, the location of ‘innocuous’ stores suchas food could reveal tactical intentions, and so may be suddenly up-graded;• classiﬁcations are not always monotone.
 Equipment classiﬁed at ‘con-ﬁdential’ may easily contain components classiﬁed ‘secret’, and on theﬂip side it’s hard to grant access at ‘secret’ to secret information ina ‘top secret’ database;• information may need to be downgraded.
An intelligence analystmight need to take a satellite photo classiﬁed at TS/SCI, and pasteit into an assessment for ﬁeld commanders at ‘secret‘.
 In case infor-mation was covertly hidden in the image by a virus, this may involvespecial ﬁlters, lossy compression of images and so on.
 One option isa ‘print-and-fax’ mechanism that turns a document into a bitmap,and logs it for traceability.
• we may need to worry about the volume of information availableto an attacker.
 For example, we might be happy to declassify anysingle satellite photo, but declassifying the whole collection wouldreveal our surveillance capability and the history of our intelligencepriorities.
 (I will look at this aggregation problem in more detail insection 11.
2.
)• Similarly, the output of an unclassiﬁed program acting on unclas-siﬁed data may be classiﬁed, for example if standard data miningtechniques applied to an online forum throw up a list of terror sus-pects.
6.
 Although MLS systems can prevent undesired things (such as informationleakage), they also prevent desired things too (such as building a searchengine to operate across all an agency’s Top Secret compartmented data).
So even in military environments, the beneﬁts can be questionable.
 After9/11, many of the rules were relaxed, and access controls above Top Secretare typically discretionary, to allow information sharing.
 The cost of that,of course, was the Snowden disclosures.
7.
 Finally, obsessive government secrecy is a chronic burden.
 The late Sen-ator Daniel Moynihan wrote a critical study of its real purposes, and itshuge costs in US foreign and military a↵airs [1346].
 For example, Presi-dent Truman was never told of the Venona decrypts because the materialwas considered ‘Army Property’.
 As he put it: “Departments and agen-cies hoard information, and the government becomes a kind of market.
Secrets become organizational assets, never to be shared save in exchangefor another organization’s assets.
”Security Engineering318Ross Anderson9.
6.
 WHAT GOES WRONGMore recent examples of MLS doctrine impairing operational e↵ectivenessinclude the use of unencrypted communications to drones in the Afghanwar (as the armed forces feared that if they got the NSA bureaucracyinvolved, the drones would be unusable), and the use of the notoriouslyinsecure Zoom videoconferencing system for British government cabinetmeetings during the coronavirus crisis (the government’s encrypted video-conferencing terminals are classiﬁed, so ministers aren’t allowed to takethem home).
This brings to mind a quip from an exasperated Britishgeneral: “What’s the di↵erence between Jurassic Park and the Ministry ofDefence? One’s a theme park full of dinosaurs, and the other’s a movie!”There has been no shortage of internal strategic critique.
 A 2004 report byMitre’s JASON programme of the US system of classiﬁcation concluded that itwas no longer ﬁt for purpose [978].
 There are many interesting reasons, includingthe widely di↵erent risk/beneﬁt calculations of the producer and consumer com-munities; classiﬁcation comes to be dominated by distribution channels ratherthan by actual risk.
 The relative ease of attack has led government systems to betoo conservative and risk-averse.
 It noted many perverse outcomes; for example,Predator imagery in Iraq is Unclassiﬁed, and was for some time transmitted inclear, as the Army feared that crypto would involve the NSA bureaucracy inkey management and inhibit warﬁghting.
Mitre proposed instead that ﬂexible compartments be set up for speciﬁcpurposes, particularly when getting perishable information to tactical compart-ments; that intelligent use be made of technologies such as rights managementand virtualisation; and that lifetime trust in cleared individuals be replaced witha system focused on transaction risk.
Anyway, one of the big changes since the second edition of this book is thatthe huge DoD research programme on MLS has disappeared, MLS equipmentis no longer very actively promoted on the government-systems market, andsystems have remained fairly static for a decade.
Most government systemsnow operate system high – that is, entirely at O�cial, or at Secret, or at TopSecret.
 The di�culties discussed in the above section, plus the falling cost ofhardware and the arrival of virtualisation, have undermined the incentive tohave di↵erent levels on the same machine.
 The deployed MLS systems thustend to be ﬁrewalls or mail guards between the di↵erent levels, and are oftenreferred to by a new acronym, MILS (for multiple independent levels of secu-rity).
 The real separation is at the network level, between unclassiﬁed networks,the Secret Internet Protocol Router Network (SIPRNet) which handles secretdata using essentially standard equipment behind crypto, and the Joint World-wide Intelligence Communications System (JWICS) which handles Top Secretmaterial and whose systems are kept in Secure Compartmentalized InformationFacilities (SCIFs) – rooms shielded to prevent electronic eavesdropping, whichI’ll discuss later in the chapter on side channels.
There are occasional horrible workarounds such as ‘browse-down’ systemsthat will let someone at High view a website at Low; they’re allowed to click onbuttons and links to navigate, just not to enter any text.
 Such ugly hacks haveclear potential for abuse; at best they can help keep honest people from carelessmistakes.
Security Engineering319Ross Anderson9.
7.
 SUMMARY9.
7SummaryMandatory access control was initially developed for military applications, whereit is still used in specialized ﬁrewalls (guards and data diodes).
 The main use ofMAC mechanisms nowadays, however, is in platforms such as Android, iOS andWindows, where they protect the operating systems themselves from malware.
MAC mechanisms have been a major subject of computer security research sincethe mid-1970’s, and the lessons learned in trying to use them for military mul-tilevel security underlie many of the schemes used for security evaluation.
 Itis important for the practitioner to understand both their strengths and limi-tations, so that you can draw on the research literature when it’s appropriate,and avoid being dragged into overdesign when it’s not.
There are many problems which we need to be a ‘fox’ rather than a ‘hedge-hog’ to solve.
 By trying to cast all security problems as hedgehog problems,MLS often leads to inappropriate security goals, policies and mechanisms.
Research ProblemsA standing challenge, sketched out by Earl Boebert in 2001 after the NSAlaunched SELinux, is to adapt mandatory access control mechanisms to safety-critical systems (see the quote at the head of this chapter, and [270]).
 As a toolfor building high-assurance, special-purpose devices where the consequences oferrors and failures can be limited, mechanisms such as type enforcement androle-based access control should be useful outside the world of security.
 Willwe see them widely used in the Internet of Things? We’ve mentioned Biba-type mechanisms in applications such as cars and electricity distribution; willthe MAC mechanisms in products such as SELinux, Windows and Androidenable designers to lock down information ﬂows and reduce the likelihood ofunanticipated interactions?The NSA continues to fund research on MLS, now under the label of IFC,albeit at a lower level than in the past.
 Doing it properly in a modern smart-phone is hard; for an example of such work, see the Weir system by AdwaitNadkarni and colleagues [1372].
 In addition to the greater intrinsic complexityof modern operating systems, phones have a plethora of side-channels and theirapps are often useful only in communication with cloud services, where the realheavy lifting has to be done.
 The commercial o↵ering for separate ‘low’ and‘high’ phones consists of products such as Samsung’s Knox.
A separate set of research issues surround actual military opsec, where realityfalls far short of policy.
 All armed forces involved in recent conﬂicts, includingUS and UK forces in Iraq and Afghanistan, have had security issues around theirpersonal mobile phones, with insurgents in some cases tracing their families backhome and harassing them with threats.
 The Royal Navy tried to ban phones in2009, but too many sailors left.
 Tracking ships via Instagram is easy; a warshipconsists of a few hundred young men and women, aged 18-24, with nothing muchelse to do but put snaps on social media.
 Discipline tends to focus on immediateoperational threats, such as when a sailor is seen snapchatting on mine disposal:there the issue is the risk of using a radio near a mine! Di↵erent navies haveSecurity Engineering320Ross Anderson9.
7.
 SUMMARYtried di↵erent things: the Norwegians have their own special network for sailorsand the USA is trying phones with MLS features.
 But NATO exercises haveshown that for one navy to hack another’s navigation is shockingly easy.
 Andeven the Israelis have had issues with their soldiers using mobiles on the WestBank and the Golan Heights.
Further ReadingThe unclassiﬁed manuals for the UK government’s system of information clas-siﬁcation, and the physical, logical and other protection mechanisms requiredat the di↵erent levels, have been available publicly since 2013, with the latestdocuments (at the time of writing) having been released in November 2018 onthe Government Security web page [802].
 The report on the Walker spy ring is adetailed account of a spectacular failure, and brings home the sheer complexityof running a system in which maybe three million people have a clearance atany one time, with a million applications being processed each year [876].
 Andthe classic on the abuse of the classiﬁcation process to cover up waste, fraudand mismanagement in the public sector is by Chapman [407].
On the technical side, textbooks such as Dieter Gollmann’s Computer Se-curity [779] give an introduction to MLS systems, while many of the publishedpapers on actual MLS systems can be found in the proceedings of two confer-ences: academics’ conference is the IEEE Symposium on Security & Privacy(known in the trade as ‘Oakland’ as that’s where it used to be held), while theNSA supplier community’s unclassiﬁed bash is the Computer Security Appli-cations Conference (ACSAC) whose proceedings are (like Oakland’s) publishedby the IEEE.
 Fred Cohen’s experiments on breaking MLS systems using virusesare described in his book, ‘A Short Course on Computer Viruses’ [450].
 Manyof the classic early papers in the ﬁeld can be found at the NIST archive [1395];NIST ran a conference series on multilevel security up till 1999.
 Finally, a his-tory of the Orange Book was written by Steve Lipner [1171]; this also tells thestory of the USAF’s early involvement and what was learned from systems likeWWMCCS.
Security Engineering321Ross Anderson