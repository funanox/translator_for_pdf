Chapter 6Access ControlMicrosoft could have incorporated e↵ective security measures asstandard, but good sense prevailed.
 Security systems have a nastyhabit of backﬁring and there is no doubt they would causeenormous problems.
– RICK MAYBURYOptimisation consists of taking something that worksand replacing it with something that almost worksbut is cheaper.
– ROGER NEEDHAM6.
1IntroductionI ﬁrst learned to program on an IBM mainframe whose input was punched cardsand whose output was a printer.
 You queued up with a deck of cards, ran thejob, and went away with printout.
 All security was physical.
 Then along camemachines that would run more than one program at once, and the protectionproblem of preventing one program from interfering with another.
 You don’twant a virus to steal the passwords from your browser, or patch a bankingapplication so as to steal your money.
 And many reliability problems stem fromapplications misunderstanding each other, or ﬁghting with each other.
 But it’stricky to separate applications when the customer wants them to share data.
It would make phishing much harder if your email client and browser ran onseparate machines, so you were unable to just click on URLs in emails, but thatwould make life too hard.
From the 1970s, access control became the centre of gravity of computersecurity.
 It’s where security engineering meets computer science.
 Its function isto control which principals (persons, processes, machines, .
 .
 .
) have access towhich resources in the system – which ﬁles they can read, which programs theycan execute, how they share data with other principals, and so on.
 It’s becomehorrendously complex.
 If you start out by leaﬁng through the 7000-plus pagesof Arm’s architecture reference manual or the equally complex arrangements for2056.
1.
 INTRODUCTIONWindows at the O/S level, your ﬁrst reaction might be ‘I wish I’d studied musicinstead!’ In this chapter I try to help you make sense of it all.
Access control works at a number of di↵erent levels, including at least:1.
 Access controls at the application level may express a very rich, domain-speciﬁc security policy.
 The call centre sta↵ in a bank are typically notallowed to see your account details until you have answered a couple ofsecurity questions; this not only stops outsiders impersonating you, butalso stops the bank sta↵ looking up the accounts of celebrities, or theirneighbours.
 Some transactions might also require approval from a super-visor.
 And that’s nothing compared with the complexity of the accesscontrols on a modern social networking site, which will have a thicket ofrules about who can see, copy, and search what data from whom, andprivacy options that users can set to modify these rules.
2.
 The applications may be written on top of middleware, such as a webbrowser, a bank’s bookkeeping system or a social network’s database man-agement system.
 These enforce a number of protection properties.
 Forexample, bookkeeping systems ensure that a transaction that debits oneaccount must credit another, with the debits and credits balancing so thatmoney cannot be created or destroyed; they must also allow the system’sstate to be reconstructed later.
3.
 As the operating system constructs resources such as ﬁles and commu-nications ports from lower level components, it has to provide ways tocontrol access to them.
 Your Android phone treats apps written by di↵er-ent companies as di↵erent users and protects their data from each other.
The same happens when a shared server separates the VMs, containers orother resources belonging to di↵erent users.
4.
 Finally, the operating system relies on hardware protection provided bythe processor and its associated memory-management hardware, whichcontrol which memory addresses a given process or thread can access.
As we work up from the hardware through the operating system and middle-ware to the application layer, the controls become progressively more complexand less reliable.
 And we ﬁnd the same access-control functions being imple-mented at multiple layers.
 For example, the separation between di↵erent phoneapps that is provided by Android is mirrored in your browser which separatesweb page material according to the domain name it came from (though thisseparation is often less thorough).
 And the access controls built at the appli-cation layer or the middleware layer may largely duplicate access controls inthe underlying operating system or hardware.
 It can get very messy, and tomake sense of it we need to understand the underlying principles, the commonarchitectures, and how they have evolved.
I will start o↵ by discussing operating-system protection mechanisms thatsupport the isolation of multiple processes.
 These came ﬁrst historically – beinginvented along with the ﬁrst time-sharing systems in the 1960s – and they re-main the foundation on which many higher-layer mechanisms are built, as wellas inspiring similar mechanisms at higher layers.
 They are often described asSecurity Engineering206Ross Anderson6.
2.
 OPERATING SYSTEM ACCESS CONTROLSdiscretionary access control (DAC) mechanisms, which leave protection to themachine operator, or mandatory access control (MAC) mechanisms which aretypically under the control of the vendor and protect the operating system itselffrom being modiﬁed by malware.
 I’ll give an introduction to software attacksand techniques for defending against them – MAC, ASLR, sandboxing, virtuali-sation and what can be done with hardware.
 Modern hardware not only providesCPU support for virtualisation and capabilities, but also hardware support suchas TPM chips for trusted boot to stop malware being persistent.
 These help ustackle the toxic legacy of the old single-user PC operating systems such as DOSand Win95/98 which let any process modify any data, and constrain the manyapplications that won’t run unless you trick them into thinking that they arerunning with administrator privileges.
6.
2Operating system access controlsThe access controls provided with an operating system typically authenticateprincipals using a mechanism such as passwords or ﬁngerprints in the case ofphones, or passwords or security protocols in the case of servers, then authoriseaccess to ﬁles, communications ports and other system resources.
Access controls can often be modeled as a matrix of access permissions, withcolumns for ﬁles and rows for users.
 We’ll write r for permission to read, w forpermission to write, x for permission to execute a program, and - for no accessat all, as shown in Figure 6.
1.
OperatingAccountsAccountingAuditSystemProgramDataTrailSamrwxrwxrwrAlicexxrw–BobrxrrrFig 6.
1 – naive access control matrixIn this simpliﬁed example, Sam is the system administrator and has universalaccess (except to the audit trail, which even he should only be able to read).
Alice, the manager, needs to execute the operating system and application, butonly through the approved interfaces – she mustn’t have the ability to tamperwith them.
 She also needs to read and write the data.
 Bob, the auditor, canread everything.
This is often enough, but in the speciﬁc case of a bookkeeping system it’snot quite what we need.
 We want to ensure that transactions are well-formed– that each debit is balanced by credits somewhere else – so we don’t wantAlice to have uninhibited write access to the account ﬁle.
 We would also ratherthat Sam didn’t have this access.
 So we would prefer that write access to theaccounting data ﬁle be possible only via the accounting program.
 The accesspermissions might now look like in Figure 6.
2:Security Engineering207Ross Anderson6.
2.
 OPERATING SYSTEM ACCESS CONTROLSUserOperatingAccountsAccountingAuditSystemProgramDataTrailSamrwxrwxrrAlicerxx––Accounts programrxrxrwwBobrxrrrFig 6.
2 – access control matrix for bookkeepingAnother way of expressing a policy of this type would be with access triplesof (user, program, ﬁle).
 In the general case, our concern isn’t with a programso much as a protection domain which is a set of processes or threads that shareaccess to the same resources.
Access control matrices (whether in two or three dimensions) can be used toimplement protection mechanisms as well as just model them.
 But they don’tscale well: a bank with 50,000 sta↵ and 300 applications would have a matrix of15,000,000 entries, which might not only impose a performance overhead but alsobe vulnerable to administrators’ mistakes.
 We will need a better way of storingand managing this information, and the two main options are to compress theusers and to compress the rights.
 With the ﬁrst, we can use groups or rolesto manage large sets of users simultaneously, while with the second we maystore the access control matrix either by columns (access control lists) or rows(capabilities, also known as ‘tickets’ to protocol engineers and ‘permissions’ onmobile phones) [1639, 2020].
6.
2.
1Groups and RolesWhen we look at large organisations, we usually ﬁnd that most sta↵ ﬁt into oneof a small number of categories.
 A bank might have 40 or 50: teller, call centreoperator, loan o�cer and so on.
 Only a few dozen people (security manager,chief foreign exchange dealer, .
.
.
) will need personally customised access rights.
So we need to design a set of groups, or functional roles, to which sta↵ canbe assigned.
 Some vendors (such as Microsoft) use the words group and rolealmost interchangeably, but a more careful deﬁnition is that a group is a listof principals, while a role is a ﬁxed set of access permissions that one or moreprincipals may assume for a period of time.
 The classic example of a role isthe o�cer of the watch on a ship.
 There is exactly one watchkeeper at any onetime, and there is a formal procedure whereby one o�cer relieves another whenthe watch changes.
 In most government and business applications, it’s the rolethat matters rather than the individual.
Groups and roles can be combined.
 The o�cers of the watch of all shipscurrently at sea is a group of roles.
 In banking, the manager of the Cambridgebranch might have their privileges expressed by membership of the group man-ager and assumption of the role acting manager of Cambridge branch.
Thegroup manager might express a rank in the organisation (and perhaps even asalary band) while the role acting manager might include an assistant accoun-tant standing in while the manager, deputy manager, and branch accountantare all o↵ sick.
Security Engineering208Ross Anderson6.
2.
 OPERATING SYSTEM ACCESS CONTROLSWhether we need to be careful about this distinction is a matter for theapplication.
 In a warship, even an ordinary seaman may stand watch if everyonemore senior has been killed.
 In a bank, we might have a policy that “transfersover $10m must be approved by two sta↵, one with rank at least manager andone with rank at least assistant accountant”.
 If the branch manager is sick, thenthe assistant accountant acting as manager might have to get the regional heado�ce to provide the second signature on a large transfer.
6.
2.
2Access control listsThe traditional way to simplify the management of access rights is to store theaccess control matrix a column at a time, along with the resource to which thecolumn refers.
 This is called an access control list or ACL (pronounced ‘ackle’).
In the ﬁrst of our above examples, the ACL for ﬁle 3 (the account ﬁle) mightlook as shown here in Figure 6.
3.
UserAccountingDataSamrwAlicerwBobrFig 6.
3 – access control list (ACL)ACLs have a number of advantages and disadvantages as a means of man-aging security state.
 They are a natural choice in environments where usersmanage their own ﬁle security, and became widespread in Unix systems fromthe 1970s.
 They are the basic access control mechanism in Unix-based systemssuch as Linux and Apple’s macOS, as well as in derivatives such as Android andiOS.
 The access controls in Windows were also based on ACLs, but have becomemore complex over time.
 Where access control policy is set centrally, ACLs aresuited to environments where protection is data-oriented; they are less suitedwhere the user population is large and constantly changing, or where users wantto be able to delegate their authority to run a particular program to anotheruser for some set period of time.
 ACLs are simple to implement, but are note�cient for security checking at runtime, as the typical operating system knowswhich user is running a particular program, rather than what ﬁles it has beenauthorized to access since it was invoked.
 The operating system must eithercheck the ACL at each ﬁle access, or keep track of the active access rights insome other way.
Finally, distributing the access rules into ACLs makes it tedious to ﬁnd allthe ﬁles to which a user has access.
 Verifying that no ﬁles have been left world-readable or even world-writable could involve checking ACLs on millions of userﬁles; this is a real issue for large complex ﬁrms.
 Although you can write a scriptto check whether any ﬁle on a server has ACLs that breach a security policy,you can be tripped up by technology changes; the move to containers has led tomany corporate data exposures as admins forgot to check the containers’ ACLstoo.
 (The containers themselves are often dreadful as it’s a new technology beingsold by dozens of clueless startups.
) And revoking the access of an employeeSecurity Engineering209Ross Anderson6.
2.
 OPERATING SYSTEM ACCESS CONTROLSwho has just been ﬁred will usually have to be done by cancelling their passwordor authentication token.
Let’s look at an important example of ACLs – their implementation in Unix(plus its derivatives Android, MacOS and iOS).
6.
2.
3Unix operating system securityIn traditional Unix systems, ﬁles are not allowed to have arbitrary access controllists, but simply rwx attributes that allow the ﬁle to be read, written and exe-cuted.
 The access control list as normally displayed has a ﬂag to show whetherthe ﬁle is a directory, then ﬂags r, w and x for owner, group and world respec-tively; it then has the owner’s name and the group name.
 A directory with allﬂags set would have the ACL:drwxrwxrwx Alice AccountsIn our ﬁrst example in Figure 6.
1, the ACL of ﬁle 3 would be:-rw-r----- Alice AccountsThis records that the ﬁle is simply a ﬁle rather than a directory; that theﬁle owner can read and write it; that group members (including Bob) can readit but not write it; that non-group members have no access at all; that the ﬁleowner is Alice; and that the group is Accounts.
The program that gets control when the machine is booted (the operatingsystem kernel) runs as the supervisor, and has unrestricted access to the wholemachine.
 All other programs run as users and have their access mediated by thesupervisor.
 Access decisions are made on the basis of the userid associated withthe program.
 However if this is zero (root), then the access control decision is‘yes’.
 So root can do what it likes – access any ﬁle, become any user, or whatever.
What’s more, there are certain things that only root can do, such as startingcertain communication processes.
 The root userid is typically made available tothe system administrator in systems with discretionary access control.
This means that the system administrator can do anything, so we have dif-ﬁculty implementing an audit trail as a ﬁle that they cannot modify.
 In ourexample, Sam could tinker with the accounts, and have di�culty defendinghimself if he were falsely accused of tinkering; what’s more, a hacker who man-aged to become the administrator could remove all evidence of his intrusion.
The traditional, and still the most common, way to protect logs against rootcompromise is to keep them separate.
 In the old days that meant sending thesystem log to a printer in a locked room; nowadays, it means sending it to an-other machine, or even to a third-party service.
 Increasingly, it may also involvemandatory access control, as we discuss later.
Second, ACLs only contain the names of users, not of programs; so thereis no straightforward way to implement access triples of (user, program, ﬁle).
Instead, Unix provides an indirect method: the set-user-id (suid) ﬁle attribute.
The owner of a program can mark the ﬁle representing that program as suid,which enables it to run with the privilege of its owner rather than the privilegeof the user who has invoked it.
 So in order to achieve the functionality neededby our second example above, we could create a user ‘account-package’ to ownSecurity Engineering210Ross Anderson6.
2.
 OPERATING SYSTEM ACCESS CONTROLSﬁle 2 (the accounts package), make the ﬁle suid and place it in a directory towhich Alice has access.
 This special user can then be given the access that theaccounts program needs.
But when you take an access control problem that has three dimensions –(user, program, data) – and implement it using two-dimensional mechanisms,the outcome is much less intuitive than triples and people are liable to makemistakes.
 Programmers are often lazy or facing tight deadlines; so they justmake the application suid root, so it can do anything.
 This practice leadsto some shocking security holes.
 The responsibility for making access controldecisions is moved from the operating system environment to the applicationprogram, and most programmers are insu�ciently experienced to check every-thing they should.
 (It’s hard to know what to check, as the person invokinga suid root program controls its environment and could manipulate this inunexpected ways.
)Third, ACLs are not very good at expressing mutable state.
 Suppose wewant a transaction to be authorised by a manager and an accountant beforeit’s acted on; we can either do this at the application level (say, by havingqueues of transactions awaiting a second signature) or by doing something fancywith suid.
 Managing stateful access rules is di�cult; they can complicate therevocation of users who have just been ﬁred, as it can be hard to track downthe ﬁles they’ve opened, and stu↵ can get stuck.
Fourth, the Unix ACL only names one user.
 If a resource will be used bymore than one of them, and you want to do access control at the OS level, youhave a couple of options.
 With older systems you had to use groups; newersystems implement the Posix system of extended ACLs, which may containany number of named user and named group entities.
 In theory, the ACL andsuid mechanisms can often be used to achieve the desired e↵ect.
 In practice,programmers are often in too much of a hurry to ﬁgure out how to do this, andsecurity interfaces are usually way too ﬁddly to use.
 So people design their codeto require much more privilege than it strictly ought to have, as that seems tobe the only way to get the job done.
6.
2.
4CapabilitiesThe next way to manage the access control matrix is to store it by rows.
 Theseare called capabilities, and in our example in Figure 6.
1 above, Bob’s capabilitieswould be as in Figure 6.
4 here:UserOperatingAccountsAccountingAuditSystemProgramDataTrailBobrxrrrFig 6.
4 – a capabilityThe strengths and weaknesses of capabilities are roughly the opposite ofACLs.
 Runtime security checking is more e�cient, and we can delegate a rightwithout much di�culty: Bob could create a certiﬁcate saying ‘Here is my capa-bility and I hereby delegate to David the right to read ﬁle 4 from 9am to 1pm,Security Engineering211Ross Anderson6.
2.
 OPERATING SYSTEM ACCESS CONTROLSsigned Bob’.
 On the other hand, changing a ﬁle’s status becomes more trickyas it can be hard to ﬁnd out which users have access.
 This can be tiresomewhen we have to investigate an incident or prepare evidence.
 In fact, scalablesystems end up using de-facto capabilities internally, as instant system-wide re-vocation is just too expensive; in Unix, ﬁle descriptors are really capabilities,and continue to grant access for some time even after ACL permissions or evenﬁle owners change.
 In a distributed Unix, access may persist for the lifetime ofKerberos tickets.
Could we do away with ACLs entirely then? People built experimental ma-chines in the 1970s that used capabilities throughout [2020]; the ﬁrst commercialproduct was the Plessey System 250, a telephone-switch controller [1575].
 TheIBM AS/400 series systems brought capability-based protection to the main-stream computing market in 1988, and enjoyed some commercial success.
 Thepublic key certiﬁcates used in cryptography are in e↵ect capabilities, and becamemainstream from the mid-1990s.
 Capabilities have started to supplement ACLsin operating systems, including more recent versions of Windows, FreeBSD andiOS, as I will describe later.
In some applications, they can be the natural way to express security policy.
For example, a hospital may have access rules like ‘a nurse shall have accessto all the patients who are on his or her ward, or who have been there in thelast 90 days’.
 In early systems based on traditional ACLs, each access controldecision required a reference to administrative systems to ﬁnd out which nursesand which patients were on which ward, when – but this made both the HRsystem and the patient administration system safety-critical, which hammeredreliability.
 Matters were ﬁxed by giving nurses ID cards with certiﬁcates thatentitle them to access the ﬁles associated with a number of wards or hospitaldepartments [535, 536].
 If you can make the trust relationships in systems mirrorthe trust relationships in that part of the world you’re trying to automate, youshould.
 Working with the grain can bring advantages at all levels in the stack,making things more usable, supporting safer defaults, cutting errors, reducingengineering e↵ort and saving money too.
6.
2.
5DAC and MACIn the old days, anyone with physical access to a computer controlled all of it:you could load whatever software you liked, inspect everything in memory or ondisk and change anything you wanted to.
 This is the model behind discretionaryaccess control (DAC): you start your computer in supervisor mode and then,as the administrator, you can make less-privileged accounts available for less-trusted tasks – such as running apps written by companies you don’t entirelytrust, or giving remote logon access to others.
 But this can make things hardto manage at scale, and in the 1970s the US military started a huge computer-security research program whose goal was to protect classiﬁed information: toensure that a ﬁle marked ‘Top Secret’ would never be made available to a userwith only a ‘Secret’ clearance, regardless of the actions of any ordinary user oreven of the supervisor.
 In such a multilevel secure (MLS) system, the sysadminis no longer the boss: ultimate control rests with a remote government authoritythat sets security policy.
 The mechanisms started to be described as mandatorySecurity Engineering212Ross Anderson6.
2.
 OPERATING SYSTEM ACCESS CONTROLSaccess control (MAC).
 The supervisor, or root access if you will, is under remotecontrol.
 This drove development of technology for mandatory access control –a fascinating story, which I tell in Part 2 of the book.
From the 1980s, safety engineers also worked on the idea of safety integritylevels; roughly, that a more dependable system must not rely on a less depend-able one.
 They started to realise they needed something similar to multilevelsecurity, but for safety.
 Military system people also came to realise that thetamper-resistance of the protection mechanisms themselves was of central im-portance.
In the 1990s, as computers and networks became fast enough tohandle audio and video, the creative industries lobbied for digital rights man-agement (DRM) in the hope of preventing people undermining their businessmodels by sharing music and video.
 This is also a form of mandatory accesscontrol – stopping a subscriber sharing a song with a non-subscriber is in manyways like stopping a Top Secret user sharing an intelligence report with a Secretuser.
In the early 2000s, these ideas came together as a number of operating-systemvendors started to incorporate ideas and mechanisms from the MAC researchprogramme into their products.
The catalyst was an initiative by Microsoftand Intel to introduce cryptography into the PC platform to support DRM.
Intel believed the business market for PCs was saturated, so growth wouldcome from home sales where, they believed, DRM would be a requirement.
Microsoft started with DRM and then realised that o↵ering rights managementfor documents too might be a way of locking customers tightly into Windowsand O�ce.
 They set up an industry alliance, now called the Trusted ComputingGroup, to introduce cryptography and MAC mechanisms into the PC platform.
To do this, the operating system had to be made tamper-resistant, and thisis achieved by means of a separate processor, the Trusted Platform Module(TPM), basically a smartcard chip mounted on the PC motherboard to supporttrusted boot and hard disk encryption.
 The TPM monitors the boot process,and at each stage a hash of everything loaded so far is needed to retrieve thekey needed to decrypt the next stage.
 The real supervisor on the system is nowno longer you, the machine owner – it’s the operating-system vendor.
MAC, based on TPMs and trusted boot, was used in Windows 6 (Vista)from 2006 as a defence against persistent malware1.
 The TPM standards andarchitecture were adapted by other operating-system vendors and device OEMs,and there is now even a project for an open-source TPM chip, OpenTitan, basedon Google’s product.
 However the main purpose of such a design, whether itsown design is open or closed, is to lock a hardware device to using speciﬁcsoftware.
1Microsoft had had more ambitious plans; its project Palladium would have provided anew, more trusted world for rights-management apps, alongside the normal one for legacysoftware.
 They launched Information Rights Management – DRM for documents – in 2003but corporates didn’t buy it, seeing it as a lock-in play.
 A two-world implementation turned outto be too complex for Vista and after two separate development e↵orts it was was abandoned;but the vision persisted from 2004 in Arm’s TrustZone, which I discuss below.
Security Engineering213Ross Anderson6.
2.
 OPERATING SYSTEM ACCESS CONTROLS6.
2.
6Apple’s macOSApple’s macOS operating system (formerly called OS/X or Mac OS X) is basedon the FreeBSD version of Unix running on top of the Mach kernel.
 The BSDlayer provides memory protection; applications cannot access system memory(or each others’) unless running with advanced permissions.
 This means, forexample, that you can kill a wedged application using the ‘Force Quit’ commandwithout having to reboot the system.
 On top of this Unix core are a number ofgraphics components, including OpenGL, Quartz, Quicktime and Carbon, whileat the surface the Aqua user interface provides an elegant and coherent view tothe user.
At the ﬁle system level, macOS is almost a standard Unix.
The defaultinstallation has the root account disabled, but users who may administer thesystem are in a group ‘wheel’ that allows them to su to root.
 If you are such auser, you can install programs (you are asked for the root password when you doso).
 Since version 10.
5 (Leopard), it has been based on TrustedBSD, a variantof BSD that incorporates mandatory access control mechanisms, which are usedto protect core system components against tampering by malware.
6.
2.
7iOSSince 2008, Apple has led the smartphone revolution with the iPhone, which(along with other devices like the iPad) uses the iOS operating system – whichis now (in 2020) the second-most popular.
 iOS is based on Unix; Apple tookthe Mach kernel from CMU and fused it with the FreeBSD version of Unix,making a number of changes for performance and robustness.
For example,in vanilla Unix a ﬁlename can have multiple pathnames that lead to an inoderepresenting a ﬁle object, which is what the operating system sees; in iOS, thishas been simpliﬁed so that ﬁles have unique pathnames, which in turn are thesubject of the ﬁle-level access controls.
 Again, there is a MAC component, wheremechanisms from Domain and Type Enforcement (DTE) are used to tamper-proof core system components (we’ll discuss DTE in more detail in chapter 9).
Apple introduced this because they were worried that apps would brick theiPhone, leading to warranty claims.
Apps also have permissions, which are capabilities; they request a capabilityto access device services such as the mobile network, the phone, SMSes, thecamera, and the ﬁrst time the app attempts to use such a service.
This isgranted if the user consents2.
 The many device services open up possible side-channel attacks; for example, an app that’s denied access to the keyboard coulddeduce keypresses using the accelerometer and gyro.
 We’ll discuss side channelsin Part 2, in the chapter on that subject.
The Apple ecosystem is closed in the sense that an iPhone will only run apps2The trust-on-ﬁrst-use model goes back to the 1990s with the Java standard J2ME, popu-larised by Symbian, and the Resurrecting Duckling model from about the same time.
 J2MEalso supported trust-on-install and more besides.
 When Apple and Android came along, theyinitially made di↵erent choices.
 In each case, having an app store was a key innovation; Nokiafailed to realise that this was important to get a two-sided market going.
 The app store doessome of the access control by deciding what apps can run.
 This is hard power in Apple’s case,and soft power in Android’s; we’ll discuss this in the chapter on phones.
Security Engineering214Ross Anderson6.
2.
 OPERATING SYSTEM ACCESS CONTROLSthat Apple has signed3.
 This enables the company to extract a share of apprevenue, and also to screen apps for malware or other undesirable behaviour,such as the exploitation of side channels to defeat access controls.
The iPhone 5S introduced a ﬁngerprint biometric and payments, addinga secure enclave (SE) to the A7 processor to give them separate protection.
Apple decided to trust neither iOS nor TrustZone with such sensitive data,since vulnerabilities give transient access until they’re patched.
 Its engineersalso worried that an unpatchable exploit might be found in the ROM (thiseventually happened, with Checkm8).
While iOS has access to the systempartition, the user’s personal data are encrypted, with the keys managed bythe SE.
 Key management is bootstrapped by a unique 256-bit AES key burnedinto fusible links on the system-on-chip.
 when the device is powered up, theuser has ten tries to enter a passcode; only then are ﬁle keys derived from themaster key and made available4.
 When the device is locked, some keys are stillusable so that iOS can work out who sent an incoming message and notify you;the price of this convenience is that forensic equipment can get some access touser data.
 The SE also manages upgrades and prevents rollbacks.
 Such publicinformation as there is can be found in the iOS Security white paper [128].
The security of mobile devices is a rather complex issue, involving not justaccess controls and tamper resistance, but the whole ecosystem – from theprovision of SIM cards through the operation of app stores to the culture of howpeople use devices, how businesses try to manipulate them and how governmentagencies spy on them.
 I will discuss this in detail in the chapter on phones inPart 2.
6.
2.
8AndroidAndroid is the world’s most widely used operating system, with 2.
5 billion activeAndroid devices in May 2019, according to Google’s ﬁgures.
 Android is basedon Linux; apps from di↵erent vendors run under di↵erent userids.
 The Linuxmechanisms control access at the ﬁle level, preventing one app from readinganother’s data and exhausting shared resources such as memory and CPU.
 Asin iOS, apps have permissions, which are in e↵ect capabilities: they grant accessto device services such as SMSes, the camera and the address book.
Apps come in signed packages, as .
apk ﬁles, and while iOS apps are signedby Apple, the veriﬁcation keys for Android come in self-signed certiﬁcates andfunction as the developer’s name.
This supports integrity of updates whilemaintaining an open ecosystem.
 Each package contains a manifest that demandsa set of permissions, and users have to approve the ‘dangerous’ ones – roughly,those that can spend money or compromise personal data.
 In early versions ofAndroid, the user would have to approve the lot on installation or not run theapp.
 But experience showed that most users would just click on anything to getthrough the installation process, and you found even ﬂashlight apps demandingaccess to your address book, as they could sell it for money.
 So Android 6 moved3There are a few exceptions: corporates can get signing keys for internal apps, but thesecan be blacklisted if abused.
4I’ll discuss fusible links in the chapter on tamper resistance, and iPhone PIN retry defeatsin the chapter on surveillance and privacy.
Security Engineering215Ross Anderson6.
2.
 OPERATING SYSTEM ACCESS CONTROLSto the Apple model of trust on ﬁrst use; apps compiled for earlier versions stilldemand capabilities on installation.
Since Android 5, SELinux has been used to harden the operating systemwith mandatory access controls, so as not only to protect core system functionsfrom attack but also to separate processes strongly and log violations.
 SELinuxwas developed by the NSA to support MAC in government systems; we’ll discussit further in chapter 9.
 The philosophy is actions require the consent of threeparties: the user, the developer and the platform.
As with iOS (and indeed Windows), the security of Android is a matter ofthe whole ecosystem, not just of the access control mechanisms.
 The new phoneecosystem is su�ciently di↵erent from the old PC ecosystem, but inherits enoughof the characteristics of the old wireline phone system, that it merits a separatediscussion in the chapter on Phones in Part II.
 We’ll consider other aspects inthe chapters on Side Channels and Surveillance.
6.
2.
9WindowsThe current version of Windows (Windows 10) appears to be the third-mostpopular operating system, having achieved a billion monthly active devices inMarch 2020 (until 2016, Windows was the leader).
 Windows has a scarily com-plex access control system, and a quick canter through its evolution may makeit easier to understand what’s going on.
Early versions of Windows had no access control.
 A break came with Win-dows 4 (NT), which was very much like Unix, and was inspired by it, but withsome extensions.
 First, rather than just read, write and execute there were sep-arate attributes for take ownership, change permissions and delete, to supportmore ﬂexible delegation.
 These attributes apply to groups as well as users, andgroup permissions allow you to achieve much the same e↵ect as suid programsin Unix.
 Attributes are not simply on or o↵, as in Unix, but have multiplevalues: you can set AccessDenied, AccessAllowed or SystemAudit.
 These areparsed in that order: if an AccessDenied is encountered in an ACL for therelevant user or group, then no access is permitted regardless of any conﬂictingAccessAllowed ﬂags.
 The richer syntax lets you arrange matters so that ev-eryday conﬁguration tasks, such as installing printers, don’t have to require fulladministrator privileges.
Second, users and resources can be partitioned into domains with distinctadministrators, and trust can be inherited between domains in one direction orboth.
 In a typical large company, you might put all the users into a personneldomain administered by HR, while assets such as servers and printers may be inresource domains under departmental control; individual workstations may evenbe administered by their users.
 Things can be arranged so that the departmentalresource domains trust the user domain, but not vice versa – so a hacked orcareless departmental administrator can’t do too much external damage.
 Theindividual workstations would in turn trust the department (but not vice versa)so that users can perform tasks that require local privilege (such as installingsoftware packages).
 Limiting the damage a hacked administrator can do stillneeds careful organisation.
 The data structure used to manage all this, and hideSecurity Engineering216Ross Anderson6.
2.
 OPERATING SYSTEM ACCESS CONTROLSthe ACL details from the user interface, is called the Registry.
 Its core used tobe the Active Directory which managed remote authentication – using eithera Kerberos variant or TLS, encapsulated behind the Security Support ProviderInterface (SSPI) which enables administrators to plug in other authenticationservices.
 Active Directory is essentially a database that organises users, groups,machines, and organisational units within a domain in a hierarchical namespace.
It lurked behind Exchange, but is now being phased out as Microsoft becomesa cloud-based company and moves its users to O�ce365.
Windows has added capabilities in two ways which can override or comple-ment ACLs.
 First, users or groups can be either allowed or denied access bymeans of proﬁles.
 Security policy is set by groups rather than for the systemas a whole; group policy overrides individual proﬁles, and can be associatedwith sites, domains or organisational units, so it can start to tackle complexproblems.
 Policies can be created using standard tools or custom coded.
The second way in which capabilities insinuate their way into Windows isthat in many applications, people use TLS for authentication, and TLS certiﬁ-cates provide another, capability-oriented, layer of access control outside thepurview of the Active Directory.
I already mentioned that Windows Vista introduced trusted boot to makethe operating system itself tamper-resistant, in the sense that it always bootsinto a known state, limiting the persistence of malware.
 It added three furtherprotection mechanisms to get away from the previous default of all softwarerunning as root.
First, the kernel was closed o↵ to developers; second, thegraphics subsystem and most drivers were removed from the kernel; and third,User Account Control (UAC) replaced the default administrator privilege withuser defaults instead.
 Previously, so many routine tasks needed administrativeprivilege that many enterprises made all their users administrators, which madeit di�cult to contain malware; and many developers wrote their software on theassumption that it would have access to everything (for a hall of shame, see [?]).
According to Microsoft engineers, this was a major reason for Windows’ lack ofrobustness: applications monkey with system resources in incompatible ways.
So they added an Application Information Service that launches applicationswhich require elevated privilege and uses virtualisation to contain them: if theymodify the registry, for example, they don’t modify the ‘real’ registry but simplythe version of it that they can see.
Since Vista, the desktop acts as the parent process for later user processes,so even administrators browse the web as normal users, and malware they down-load can’t overwrite system ﬁles unless given later authorisation.
 When a taskrequires admin privilege, the user gets an elevation prompt asking them for anadmin password.
 (Apple’s macOS is similar although the details under the hooddi↵er somewhat.
) As admin users are often tricked into installing malicious soft-ware, Vista added mandatory access controls in the form of ﬁle integrity levels.
The basic idea is that low-integrity processes (such as code you download fromthe Internet) should not be able to modify high-integrity data (such as systemﬁles) in the absence of some trusted process (such as veriﬁcation of a signatureby Microsoft on the code in question).
In 2012, Windows 8 added dynamic access control which lets you controlSecurity Engineering217Ross Anderson6.
2.
 OPERATING SYSTEM ACCESS CONTROLSuser access by context, such as their work PC versus their home PC and theirphone; this is done via account attributes in Active Directory, which appear asclaims about a user, or in Kerberos tickets as claims about a domain.
 In 2016,Windows 8.
1 added a cleaner abstraction with principals, which can be a user,computer, process or thread running in a security context or a group to whichsuch a principal belongs, and security identiﬁers (SIDs) which represent suchprincipals.
 When a user signs in, they get tickets with the SIDs to which theybelong.
 Windows 8.
1 also prepared for the move to cloud computing by addingMicrosoft accounts (formerly LiveID), whereby a user signs in to a Microsoftcloud service rather than to a local server.
 Where credentials are stored locally,it protects them using virtualisation.
Finally, Windows 10 added a numberof features to support the move to cloud computing with a diversity of clientdevices, ranging from certiﬁcate pinning (which we’ll discuss in the chapter onNetwork Security) to the abolition of the old secure attention sequence ctrl-alt-del (which is hard to do on touch-screen devices and which users didn’tunderstand anyway).
To sum up, Windows evolved to provide a richer and more ﬂexible set ofaccess control tools than any system previously sold in mass markets.
 It wasdriven by corporate customers who need to manage tens of thousands of sta↵performing hundreds of di↵erent job roles across hundreds of di↵erent sites, pro-viding internal controls to limit the damage that can be done by small numbersof dishonest sta↵ or infected machines.
 (How such controls are actually designedwill be our topic in the chapter on Banking and Bookkeeping.
) The driver forthis development was the fact that Microsoft made over half of its revenue fromﬁrms that licensed more than 25,000 seats; but the cost of the ﬂexibility thatcorporate customers demanded is complexity.
 Setting up access control for abig Windows shop is a highly skilled job.
6.
2.
10MiddlewareDoing access control at the level of ﬁles and programs was ﬁne in the early daysof computing, when these were the resources that mattered.
 Since the 1980s,growing scale and complexity has led to access control being done at otherlevels instead of (or as well as) at the operating system level.
 For example,bookkeeping systems often run on top of a database product such as Oracle,which looks to the operating system as one large ﬁle.
 So most of the accesscontrol has to be done in the database; all the operating system supplies maybe an authenticated ID for each user who logs on.
 And since the 1990s, a lot ofthe work at the client end has been done by the web browser.
6.
2.
10.
1Database access controlsBefore people started using websites for shopping, database security was largelya back-room concern.
 But enterprises now have critical databases to handleinventory, dispatch and e-commerce, fronted by web servers that pass transac-tions to the databases directly.
 These databases now contain much of the datathat matter to our lives – bank accounts, vehicle registrations and employmentrecords – and failures sometimes expose them to random online users.
Security Engineering218Ross Anderson6.
2.
 OPERATING SYSTEM ACCESS CONTROLSDatabase products, such as Oracle, DB2 and MySQL, have their own accesscontrol mechanisms, which are modelled on operating-system mechanisms, withprivileges typically available for both users and objects (so the mechanisms area mixture of access control lists and capabilities).
 However, the typical databaseaccess control architecture is comparable in complexity with Windows; moderndatabases are intrinsically complex, as are the things they support – typicallybusiness processes involving higher levels of abstraction than ﬁles or domains.
There may be access controls aimed at preventing any user learning too muchabout too many customers; these tend to be stateful, and may deal with possiblestatistical inference rather than simple yes-no access rules.
 I devote a wholechapter in Part 2 to exploring the topic of Inference Control.
Ease of administration is often a bottleneck.
 In companies I’ve advised, theoperating-system and database access controls have been managed by di↵erentdepartments, which don’t talk to each other; and often IT departments have toput in crude hacks to make the various access control systems seem to work asone, but which open up serious holes.
Some products let developers bypass operating-system controls.
 For exam-ple, Oracle has both operating system accounts (whose users must be authen-ticated externally by the platform) and database accounts (whose users areauthenticated directly by the Oracle software).
 It is often convenient to usethe latter, to save the e↵ort of synchronising with what other departments aredoing.
 In many installations, the database is accessible directly from the out-side; and even where it’s shielded by a web service front-end, this often containsloopholes that let SQL code be inserted into the database.
Database security failures can thus cause problems directly.
 The Slammerworm in 2003 propagated itself using a stack-overﬂow exploit against MicrosoftSQL Server 2000 and created large amounts of tra�c as compromised machinessent ﬂoods of attack packets to random IP addresses.
Just as Windows is tricky to conﬁgure securely, because it’s so complicated,the same goes for the typical database system.
 If you ever have to lock one down– or even just understand what’s going on – you had better read a specialisttextbook, such as [1174], or get in an expert.
6.
2.
10.
2BrowsersThe web browser is another middleware platform on which we rely for accesscontrol and whose complexity often lets us down.
 The main access control ruleis the same-origin policy whereby JavaScript or other active content on a webpage is only allowed to communicate with the IP address that it originally camefrom; such code is run in a sandbox to prevent it altering the host system, as I’lldescribe in the next section.
 But many things can go wrong.
In previous editions of this book, we considered web security to be a matterof how the servers were conﬁgured, and whether this led to cross-site vulnerabil-ities.
 For example a malicious website can include links or form buttons aimedat creating a particular side-e↵ect:https://mybank.
com/transfer.
cgi?amount=10000USD&recipient=thiefSecurity Engineering219Ross Anderson6.
2.
 OPERATING SYSTEM ACCESS CONTROLSThe idea is that if a user clicks on this who is logged into mybank.
com, theremay be a risk that the transaction will be executed, as there’s a valid sessioncookie.
 So payment websites deploy countermeasures such as using short-livedsessions and an anti-CSRF token (an invisible MAC of the session cookie), andchecking the Referer: header.
 There are also issues around web authenticationmechanisms; I described OAuth brieﬂy in section 4.
7.
4.
 If you design web pagesfor a living you had better understand the mechanics of all this in rather moredetail (see for example [119]); but many developers don’t take enough care.
For example, as I write in 2020, Amazon Alexa has just turned out to have amisconﬁgured policy on cross-origin resource sharing, which meant that anyonewho compromised another Amazon subdomain could replace the skills on atarget Alexa with malicious ones [1481].
By now there’s a realisation that we should probably have treated browsersas access control devices all along.
 After all, the browser is the place on yourlaptop were you run code written by people you don’t want to trust and whowill occasionally be malicious; as we discussed earlier, mobile-phone operatingsystems run di↵erent apps as di↵erent users to give even more robust protection.
Even in the absence of malice, you don’t want to have to reboot your browserif it hangs because of a script in one of the tabs.
 (Chrome tries to ensure thisby running each tab in a separate operating-system process.
)Bugs in browsers are exploited in drive-by download attacks, where visitingan attack web page can infect your machine, and even without this the modernweb environment is extremely di�cult to control.
Many web pages are fullof trackers and other bad things, supplied by multiple ad networks and databrokers, which make a mockery of the intent behind the same-origin policy.
Malicious actors can even use web services to launder origin: for example, theattacker makes a mash-up of the target site plus some evil scripts of his own, andthen gets the victim to view it through a proxy such as Google Translate [1854].
A prudent person will go to their bank website by typing in the URL directly,or using a bookmark; unfortunately, the marketing industry trains everyone toclick on links in emails.
6.
2.
11SandboxingThe late 1990s saw the emergence of yet another type of access control: thesoftware sandbox, introduced by Sun with its Java programming language.
 Themodel is that a user wants to run some code that she has downloaded as anapplet, but is concerned that the applet might do something nasty, such asstealing her address book and mailing it o↵ to a marketing company, or justhogging the CPU and running down the battery.
The designers of Java tackled this problem by providing a ‘sandbox’ – arestricted environment in which the code has no access to the local hard disk(or at most only temporary access to a restricted directory), and is only allowedto communicate with the host it came from (the same-origin policy).
Thisis enforced by having the code executed by an interpreter – the Java VirtualMachine (JVM) – with only limited access rights [783].
 This idea was adapted toJavaScript, the main scripting language used in web pages, though it’s actuallya di↵erent language; and other active content too.
 A version of Java is also usedSecurity Engineering220Ross Anderson6.
2.
 OPERATING SYSTEM ACCESS CONTROLSon smartcards so they can support applets written by di↵erent ﬁrms.
6.
2.
12VirtualisationVirtualisation is what powers cloud computing; it enables a single machine toemulate a number of machines independently, so that you can rent a virtual ma-chine (VM) in a data centre for a few tens of dollars a month rather than havingto pay maybe a hundred for a whole server.
 Virtualisation was invented in the1960s by IBM [496]; a single machine could be partitioned using VM/370 intomultiple virtual machines.
 Initially this was about enabling a new mainframe torun legacy apps from several old machine architectures; it soon became normalfor a company that bought two computers to use one for its production environ-ment and the other as a series of logically separate machines for development,testing, and minor applications.
 It’s not enough to run a virtual machine mon-itor (VMM) on top of a host operating system, and then run other operatingsystems on top; you have to deal with sensitive instructions that reveal proces-sor state such as absolute addresses and the processor clock.
 Working VMMsappeared for Intel platforms with VMware ESX Server in 2003 and (especially)Xen in 2003, which accounted for resource usage well enough to enable AWSand the cloud computing revolution.
 Things can be done more cleanly withprocessor support, which Intel has provided since 2006 with VT-x, and whosedetails I’ll discuss below.
 VM security claims rest to some extent on the argu-ment that a VMM hypervisor’s code can be much smaller than an operatingsystem and thus easier to code-review and secure; whether there are actuallyfewer vulnerabilities is of course an empirical question [1575].
At the client end, virtualisation allows people to run a guest operating systemon top of a host (for example, Windows on top of macOS), which o↵ers not justﬂexibility but the prospect of better containment.
 For example, an employeemight have two copies of Windows running on their laptop – a locked-downversion with the o�ce environment, and another for use at home.
 Samsungo↵ers Knox, which creates a virtual machine on a mobile phone that an employercan lock down and manage remotely, while the user enjoys a normal Android aswell on the same device.
But using virtualisation to separate security domains on clients is harderthan it looks.
 People need to share data between multiple VMs and if they usead-hoc mechanisms, such as USB sticks and webmail accounts, this underminesthe separation.
 Safe data sharing is far from trivial.
 For example, Bromium5o↵ers VMs tailored to speciﬁc apps on corporate PCs, so you have one VM forO�ce, one for Acrobat reader, one for your browser and so on.
 This enablesﬁrms to work reasonably securely with old, unsupported software.
 So how do youdownload an O�ce document? Well, the browser exports the ﬁle from its VMto the host hard disc, marking it ‘untrusted’, so when the user tries to open itthey’re given a new VM which holds that document plus O�ce and nothing else.
When they then email this untrusted document, there’s an Outlook plugin thatstops it being rendered in the ‘sent mail’ pane.
 Things get even more messy withnetwork services integrated into apps; the rules on what sites can access whichcookies are complicated, and it’s hard to deal with single signon and workﬂows5Now owned by HPSecurity Engineering221Ross Anderson6.
3.
 HARDWARE PROTECTIONthat cross multiple domains.
 The clipboard also needs a lot more rules to controlit.
 Many of the rules change from time to time, and are heuristics rather thanhard, veriﬁable access logic.
 In short, using VMs for separation at the clientrequires deep integration with the OS and apps if it’s to appear transparent tothe user, and there are plenty of tradeo↵s made between security and usability.
In e↵ect, you’re retroﬁtting virtualisation on to an existing OS and apps thatwere not built for it.
Containers have been the hot new topic in the late 2010s.
 They evolvedas a lightweight alternative to virtualisation in cloud computing and are oftenconfused with it, especially by the marketing people.
 My deﬁnition is that whilea VM has a complete operating system, insulated from the hardware by a hy-pervisor, a container is an isolated guest process that shares a kernel with othercontainers.
 Container implementations separate groups of processes by virtu-alising a subset of operating-system mechanisms, including process identiﬁers,interprocess communication, and namespaces; they also use techniques such assandboxing and system call ﬁltering.
 The business incentive is to minimise theguests’ size, their interaction complexity and the costs of managing them, sothey are deployed along with orchestration tools.
 Like any other new technol-ogy, there are many startups with more enthusiasm than experience.
 A 2019survey by Jerry Gamblin disclosed that of the top 1000 containers available todevelopers on Docker Hub, 194 were setting up blank root passwords [743].
 Ifyou’re going to use cloud systems, you need to pay serious attention to yourchoice of tools, and also learn yet another set of access control mechanisms –those o↵ered by the service provider, such as the Amazon AWS Identity andAccess Management (IAM).
 This adds another layer of complexity, which peoplecan get wrong.
 For example, in 2019 a security ﬁrm providing biometric iden-tiﬁcation services to banks and the police left its entire database unprotected;two researchers found it using Elasticsearch and discovered millions of people’sphotos, ﬁngerprints, passwords and security clearance levels on a database thatthey could not only read but write [1864].
But even if you tie down a cloud system properly, there are hardware limitson what the separation mechanisms can achieve.
 In 2018, two classes of powerfulside-channel attacks were published: Meltdown and Spectre, which I discuss inthe following section and at greater length in the chapter on side channels.
 Thosebanks that use containers to deploy payment processing rely, at least implicitly,on their containers being di�cult to target in a cloud the size of Amazon’s orGoogle’s.
For a comprehensive survey of the evolution of virtualisation andcontainers, see Randal [1575].
6.
3Hardware ProtectionMost access control systems set out not just to control what users can do, butto limit what programs can do as well.
 In many systems, users can either writeprograms, or download and install them, and these programs may be buggy oreven malicious.
Preventing one process from interfering with another is the protection prob-lem.
 The conﬁnement problem is that of preventing programs communicatingSecurity Engineering222Ross Anderson6.
3.
 HARDWARE PROTECTIONoutward other than through authorized channels.
 There are several ﬂavours ofeach.
 The goal may be to prevent active interference, such as memory over-writing, or to stop one process reading another’s memory directly.
 This is whatcommercial operating systems set out to do.
 Military systems may also try toprotect metadata – data about other data, or subjects, or processes – so that,for example, a user can’t ﬁnd out what other users are logged on to the systemor what processes they’re running.
Unless one uses sandboxing techniques (which are too restrictive for generalprogramming environments), solving the protection problem on a single proces-sor means, at the very least, having a mechanism that will stop one programfrom overwriting another’s code or data.
 There may be areas of memory that areshared to allow interprocess communication; but programs must be protectedfrom accidental or deliberate modiﬁcation, and must have access to memorythat is similarly protected.
This usually means that hardware access control must be integrated withthe processor’s memory management functions.
 A classic mechanism is segmentaddressing.
 Memory is addressed by two registers, a segment register that pointsto a segment of memory, and an address register that points to a location withinthat segment.
 The segment registers are controlled by the operating system,often by a component of it called the reference monitor which links the accesscontrol mechanisms with the hardware.
The implementation has become more complex as processors themselveshave.
 Early IBM mainframes had a two-state CPU: the machine was eitherin authorized state or it was not.
 In the latter case, the program was restrictedto a memory segment allocated by the operating system; in the former, it couldwrite to segment registers at will.
 An authorized program was one that wasloaded from an authorized library.
Any desired access control policy can be implemented on top of this, givensuitable authorized libraries, but this is not always e�cient; and system securitydepended on keeping bad code (whether malicious or buggy) out of the autho-rized libraries.
 So later processors o↵ered more complex hardware mechanisms.
Multics, an operating system developed at MIT in the 1960s and which inspiredUnix, introduced rings of protection which express di↵ering levels of privilege:ring 0 programs had complete access to disk, supervisor states ran in ring 2,and user code at various less privileged levels [1684].
 Many of its features havebeen adopted in more recent processors.
There are a number of general problems with interfacing hardware and soft-ware security mechanisms.
 For example, it often happens that a less privilegedprocess such as application code needs to invoke a more privileged process (e.
g.
a device driver).
 The mechanisms for doing this need to be designed with care,or security bugs can be expected.
 Also, performance may depend quite drasti-cally on whether routines at di↵erent privilege levels are called by reference orby value [1684].
Security Engineering223Ross Anderson6.
3.
 HARDWARE PROTECTION6.
3.
1Intel processorsThe Intel 8088/8086 processors used in early PCs had no distinction betweensystem and user mode, and thus any running program controlled the wholemachine6.
 The 80286 added protected segment addressing and rings, so for theﬁrst time a PC could run proper operating systems.
 The 80386 had built-invirtual memory, and large enough memory segments (4 Gb) that they could beignored and the machine treated as a 32-bit ﬂat address machine.
 The 486 andPentium series chips added more performance (caches, out of order executionand additional instructions such as MMX).
The rings of protection are supported by a number of mechanisms.
Thecurrent privilege level can only be changed by a process in ring 0 (the ker-nel).
 Procedures cannot access objects in lower-level rings directly but there aregates that allow execution of code at a di↵erent privilege level and manage thesupporting infrastructure, such as multiple stack segments.
From 2006, Intel added hardware support for x86 virtualisation, known asIntel VT, which helped drive the adoption of cloud computing.
Some pro-cessor architectures such as S/370 and PowerPC are easy to virtualise, andthe theoretical requirements for this had been established in 1974 by GeraldPopek and Robert Goldberg [1532]; they include that all sensitive instructionsthat expose raw processor state be privileged instructions.
The native Intelinstruction set, however, has sensitive user-mode instructions, requiring messyworkarounds such as application code rewriting and patches to hosted operat-ing systems.
 Adding VMM support in hardware means that you can run anoperating system in ring 0 as it was designed; the VMM has its own copy ofthe memory architecture underneath.
 You still have to trap sensitive opcodes,but system calls don’t automatically require VMM intervention, you can rununmodiﬁed operating systems, things go faster and systems are generally morerobust.
 Modern Intel CPUs now have nine rings: ring 0–3 for normal code,under which is a further set of ring 0–3 VMM root mode for the hypervisor, andat the bottom is system management mode (SMM) for the BIOS.
 In practice,the four levels that are used are SMM, ring 0 of VMX root mode, the normalring 0 for the operating system, and ring 3 above that for applications.
In 2015, Intel released Software Guard eXtensions (SGX), which lets trustedcode run in an enclave – an encrypted section of the memory – while the rest ofthe code is executed as usual.
 The company had worked on such architecturesin the early years of the Trusted Computing initiative, but let things slide untilit needed an enclave architecture to compete with TrustZone, which I discussin the next section.
 The encryption is performed by a Memory Encryption En-gine (MEE), while SGX also introduces new instructions and memory-accesschecks to ensure non-enclave processes cannot access enclave memory (not evenroot processes).
 SGX has been promoted for DRM and securing cloud VMs,particularly those containing crypto keys, credentials or sensitive personal in-formation; this is under threat from Spectre and similar attacks, which I discussin detail in the chapter on side channels.
 Since SGX’s security perimeter is theCPU, its software is encrypted in main memory, which imposes real penalties6They had been developed on a crash programme to save market share following the adventof RISC processors and the market failure of the iAPX432.
Security Engineering224Ross Anderson6.
3.
 HARDWARE PROTECTIONin both time and space.
 Another drawback used to be that SGX code had tobe signed by Intel.
 The company has now delegated signing (so bad people canget code signed) and from SGXv2 will open up the root of trust to others.
 Sopeople are experimenting with SGX malware, which can remain undetectable byanti-virus software.
 As SGX apps cannot issue syscalls, it had been hoped thatenclave malware couldn’t do much harm, yet Michael Schwarz, Samuel Weiserand Daniel Gruss have now worked out how to mount stealthy return-orientedprogramming (ROP) attacks from an enclave on a host app; they argue thatthe problem is a lack of clarity about what enclaves are supposed to do, andthat any reasonable threat model must include untrusted enclaves [1688].
 Thissimple point may force a rethink of enclave architectures; Intel says ‘In thefuture, Intel’s control-ﬂow enforcement technology (CET) should help addressthis threat inside SGX’7.
 As for what comes next, AMD released full systemmemory encryption in 2016, and Intel announced a competitor.
 This aimed todeal with cold-boot and DMA attacks, and protect code against an untrustedhypervisor; it might also lift space and performance limits on next-generationenclaves.
However, Jan Werner and colleagues found multiple inference anddata-injection attacks on AMD’s o↵ering when it’s used in a virtual environ-ment.
 [2010].
 There’s clearly some way to go.
As well as the access-control vulnerabilities, there are crypto issues, whichI’ll discuss in the chapter on Advanced Cryptographic Engineering.
6.
3.
2Arm processorsThe Arm is the processor core most commonly used in phones, tablets and IoTdevices; billions have been used in mobile phones alone, with a high-end devicehaving several dozen Arm cores of various sizes in its chipset.
 The original Arm(which stood for Acorn Risc Machine) was the ﬁrst commercial RISC design; itwas released in 1985, just before MIPS.
 In 1991, Arm became a separate ﬁrmwhich, unlike Intel, does not own or operate any fabs: it licenses a range ofprocessor cores, which chip designers include in their products.
 Early cores hada 32-bit datapath and contained ﬁfteen registers, of which seven were shadowedby banked registers for system processes to cut the cost of switching context oninterrupt.
 There are multiple supervisor modes, dealing with fast and normalinterrupts, the system mode entered on reset, and various kinds of exceptionhandling.
 The core initially contained no memory management, so Arm-baseddesigns could have their hardware protection extensively customized; there arenow variants with memory protection units (MPUs), and others with memorymanagement units (MMUs) that handle virtual memory as well.
In 2011, Arm launched version 8, which supports 64-bit processing and en-ables multiple 32-bit operating systems to be virtualised.
 Hypervisor supportadded yet another supervisor mode.
 The cores come in all sizes, from large64-bit superscalar processors with pipelines over a dozen stages deep, to tinyones for cheap embedded devices.
TrustZone is a security extension that supports the ‘two worlds’ model men-7The best defence against ROP attacks in 2019 appears to be Apple’s mechanism, in theiPhone X3 and later, for signing pointers with a key that’s kept in a register; this stops ROPattacks as the attacker can’t guess the signatures.
Security Engineering225Ross Anderson6.
4.
 WHAT GOES WRONGtioned above; it was made available to mobile phone makers in 2004 [44].
 Phoneswere the ‘killer app’ for enclaves as operators wanted to lock subsidised phonesand regulators wanted to make the baseband software that controls the RFfunctions tamper-resistant [1239].
 TrustZone supports an open world for a nor-mal operating system and general-purpose applications, plus a closed enclaveto handle sensitive operations such as cryptography and critical I/O (in a mo-bile phone, this can include the SIM card and the ﬁngerprint reader).
 Whetherthe processor is in a secure or non-secure state is orthogonal to whether it’s inuser mode or a supervisor mode (though it must choose between secure and hy-pervisor mode).
 The closed world hosts a single trusted execution environment(TEE) with separate stacks, a simpliﬁed operating system, and typically runsonly trusted code signed by the OEM – although Samsung’s Knox, which setsout to provide ‘home’ and ‘work’ environments on your mobile phone, allowsregular rich apps to execute in the secure environment.
Although TrustZone was released in 2004, it was kept closed until 2015;OEMs used it to protect their own interests and didn’t open it up to app devel-opers, except occasionally under NDA.
 As with Intel SGX, there appears to beno way yet to deal with malicious enclave apps, which might come bundled asDRM with gaming apps or be mandated by authoritarian states; and, as withIntel SGX, enclave apps created with TrustZone can raise issues of transparencyand control, which can spill over into auditability, privacy and much else.
 Again,company insiders mutter ‘wait and see’; no doubt we shall.
Arm’s latest o↵ering is CHERI8 which adds ﬁne-grained capability supportto Arm CPUs.
 At present, browsers such as Chrome put tabs in di↵erent pro-cesses, so that one webpage can’t slow down the other tabs if its scripts runslowly.
 It would be great if each object in each web page could be sandboxedseparately, but this isn’t possible because of the large cost, in terms of CPUcycles, of each inter-process context switch.
 CHERI enables a process spawninga subthread to allocate it read and write accesses to speciﬁc ranges of memory,so that multiple sandboxes can run in the same process.
 This was announcedas a product in 2018 and we expect to see ﬁrst silicon in 2021.
 The long-termpromise of this technology is that, if it were used thoroughly in operating sys-tems such as Windows, Android and iOS, it would have prevented most of thezero-day exploits of recent years.
 Incorporating a new protection technology atscale costs real money, just like the switch from 32-bit to 64-bit CPUs, but itcould save the cost of lots of patches.
6.
4What Goes WrongPopular operating systems such as Android, Linux and Windows are very largeand complex, with their features tested daily by billions of users under verydiverse circumstances.
 Many bugs are found, some of which give rise to vulner-abilities, which have a typical lifecycle.
 After discovery, a bug is reported to aCERT or to the vendor; a patch is shipped; the patch is reverse-engineered, andan exploit may be produced; and people who did not apply the patch in time8Full disclosure: this was developed by a team of my colleagues at Cambridge and else-where, led by Robert Watson.
Security Engineering226Ross Anderson6.
4.
 WHAT GOES WRONGmay ﬁnd that their machines have been compromised.
 In a minority of cases,the vulnerability is exploited at once rather than reported – called a zero-dayexploit as attacks happen from day zero of the vulnerability’s known existence.
The economics, and the ecology, of the vulnerability lifecycle are the subject ofintensive study by security economists; I’ll discuss this in Part III.
The traditional goal of an attacker was to get a normal account on the systemand then become the system administrator, so they could take over the systemcompletely.
 The ﬁrst step might have involved guessing, or social-engineering,a password, and then using an operating-system bug to escalate from user toroot [1129].
The user/root distinction became less important in the twenty-ﬁrst centuryfor two reasons.
 First, Windows PCs were the most common online devices(until 2017 when Android overtook them) so they were the most common attacktargets; and as they ran many applications as administrator, any applicationthat could be compromised gave administrator access.
 Second, attackers comein two basic types: targeted attackers, who want to spy on a speciﬁc individualand whose goal is typically to acquire access to that person’s accounts; andscale attackers, whose goal is typically to compromise large numbers of PCs,which they can organise into a botnet in order to make money.
This, too,doesn’t require administrator access.
 Even if your mail client does not run asadministrator, it can still be useful to a spammer who takes control.
However, botnet herders do prefer to install rootkits which, as their namesuggests, run as root; they are also known as remote access trojans or RATs.
The user/root distinction does still matter in business environments, where youdo not want such a kit installed as an advanced persistent threat by a hostileintelligence agency, or corporate espionage ﬁrm, or by a crime gang doing re-connaissance to set you up for a large fraud.
A separate distinction is whether an exploit is wormable – whether it canbe used to spread malware quickly online from one machine to another withouthuman intervention.
 The Morris worm was the ﬁrst large-scale case of this, andthere have been many since.
 I mentioned Wannacry and NotPetya in chapter 2;these used a vulnerability developed by the NSA and then leaked to other stateactors.
 Operating system vendors react quickly to wormable exploits, typicallyreleasing out-of-sequence patches, because of the scale of the damage they cando.
 The most troublesome wormable exploits at the time of writing are variantsof Mirai, a worm used to take over IoT devices that use known root passwords.
This appeared in October 2016 to exploit CCTV cameras, and hundreds ofversions have been produced since, adapted to take over di↵erent vulnerabledevices and recruit them into botnets.
 Wormable exploits often use root accessbut don’t have to; it is su�cient that the exploit be capable of automatic onwardtransmission9.
In any case, the basic types of technical attack have not changed hugely ina generation and I’ll now consider them brieﬂy.
9In rare cases even human transmission can make malware spread quickly: an examplewas the ILoveYou worm which spread itself in 2000 via an email with that subject line, whichcaused enough people to open it, running a script that caused it to be sent to everyone in thenew victim’s address book.
Security Engineering227Ross Anderson6.
4.
 WHAT GOES WRONG6.
4.
1Smashing the stackThe classic software exploit is the memory overwriting attack, colloquially knownas ‘smashing the stack’, as used by the Morris worm in 1988; this infected somany Unix machines that it disrupted the Internet and brought malware force-fully to the attention of the mass media [1806].
 Attacks involving violationsof memory safety accounted for well over half the exploits against operatingsystems in the late 1990s and early 2000s [487] but the proportion has beendropping slowly since then.
Programmers are often careless about checking the size of arguments, so anattacker who passes a long argument to a program may ﬁnd that some of it getstreated as code rather than data.
 The classic example, used in the Morris worm,was a vulnerability in the Unix finger command.
 A common implementationof this would accept an argument of any length, although only 256 bytes hadbeen allocated for this argument by the program.
 When an attacker used thecommand with a longer argument, the trailing bytes of the argument ended upoverwriting the stack and being executed by the system.
The usual exploit technique was to arrange for the trailing bytes of theargument to have a landing pad – a long space of no-operation (NOP) commands,or other register commands that didn’t change the control ﬂow, and whose taskwas to catch the processor if it executed any of them.
 The landing pad deliveredthe processor to the attack code which will do something like creating a shellwith administrative privilege directly (see Figure 6.
5).
Figure 6.
5: – stack smashing attackStack-overwriting attacks were around long before 1988.
 Most of the early1960s time-sharing systems su↵ered from this vulnerability, and ﬁxed it [804].
Penetration testing in the early ’70s showed that one of the most frequently-used attack strategies was still “unexpected parameters” [1165].
 Intel’s 80286processor introduced explicit parameter checking instructions – verify read, ver-ify write, and verify length – in 1982, but they were avoided by most softwaredesigners to prevent architecture dependencies.
 Stack overwriting attacks havebeen found against all sorts of programmable devices – even against things likesmartcards and hardware security modules, whose designers really should haveknown better.
Security Engineering228Ross Anderson6.
4.
 WHAT GOES WRONG6.
4.
2Other technical attacksMany vulnerabilities are variations on the same general theme, in that theyoccur when data in grammar A is interpreted as being code in grammar B.
 Astack overﬂow is when data are accepted as input (e.
g.
 a URL) and end upbeing executed as machine code.
 These are failures of type safety.
 In fact, astack overﬂow can be seen either as a memory safety failure or as a failure tosanitise user input, but there are purer examples of each type.
The use after free type of safety failure is now the most common cause ofremote execution vulnerabilities and has provided a lot of attacks on browsersin recent years.
 It can happen when a chunk of memory is freed and then stillused, perhaps because of confusion over which part of a program is responsiblefor freeing it.
 If a malicious chunk is now allocated, it may end up taking itsplace on the heap, and when an old innocuous function is called a new, maliciousfunction may be invoked instead.
 There are many other variants on the memorysafety theme; bu↵er overﬂows can be induced by improper string termination,passing an inadequately sized bu↵er to a path manipulation function, and manyother subtle errors.
 See Gary McGraw’s book ‘Software Security [1266] for ataxonomy.
SQL injection attacks are the most common attack based on failure to sani-tise input, and arise when a careless web developer passes user input to a back-end database without checking to see whether it contains SQL code.
 The gameis often given away by error messages, from which a capable and motivated usermay infer enough to mount an attack.
 There are similar command-injectionproblems a✏icting other languages used by web developers, such as PHP.
 Theusual remedy is to treat all user input as suspicious and validate it.
 But thiscan be harder than it looks, as it’s di�cult to anticipate all possible attacks andthe ﬁlters written for one shell may fail to be aware of extensions present inanother.
 Where possible, one should only act on user input in a safe context,by designing such attacks out; where it’s necessary to blacklist speciﬁc exploits,the mechanism needs to be competently maintained.
Once such type-safety and input-sanitisation attacks are dealt with, raceconditions are probably next.
 These occur when a transaction is carried outin two or more stages, where access rights are veriﬁed at the ﬁrst stage andsomething sensitive is done at the second.
 If someone can alter the state inbetween the two stages, this can lead to an attack.
 A classic example arosein early versions of Unix, where the command to create a directory, ‘mkdir’,used to work in two steps: the storage was allocated, and then ownership wastransferred to the user.
 Since these steps were separate, a user could initiatea ‘mkdir’ in background, and if this completed only the ﬁrst step before beingsuspended, a second process could be used to replace the newly created directorywith a link to the password ﬁle.
 Then the original process would resume, andchange ownership of the password ﬁle to the user.
A more modern example arises with the wrappers used in containers tointercept system calls made by applications to the operating system, parse them,and modify them if need be.
 These wrappers execute in the kernel’s addressspace, inspect the enter and exit state on all system calls, and encapsulate onlysecurity logic.
 They generally assume that system calls are atomic, but modernSecurity Engineering229Ross Anderson6.
4.
 WHAT GOES WRONGoperating system kernels are highly concurrent.
 System calls are not atomicwith respect to each other; there are many possibilities for two system calls torace each other for access to shared memory, which gives rise to time-of-check-to-time-of-use (TOCTTOU) attacks.
 An early (2007) example calls a path whosename spills over a page boundary by one byte, causing the kernel to sleep whilethe page is fetched; it then replaces the path in memory [1992].
 There havebeen others since, and as more processors ship in each CPU chip as time passes,and containers become an ever more common way of deploying applications,this sort of attack may become more and more of a problem.
 Some operatingsystems have features speciﬁcally to deal with concurrency attacks, but this ﬁeldis still in ﬂux.
A di↵erent type of timing attack can come from backup and recovery sys-tems.
 It’s convenient if you can let users recover their own ﬁles, rather thanhaving to call a sysadmin – but how do you protect information assets from atime traveller? People can reacquire access rights that were revoked, and playeven more subtle tricks.
One attack that has attracted a lot of research e↵ort recently is return-oriented programming (ROP) [1708].
 Many modern systems try to prevent typesafety attacks by data execution prevention – marking memory as either codeor data, a measure that goes back to the Burroughs 5000; and if all the code issigned, surely you’d think that unauthorised code cannot be executed? Wrong!An attacker can look for gadgets – sequences of instructions with some use-ful e↵ect, ending in a return.
By collecting enough gadgets, it’s possible toassemble a machine that’s Turing powerful, and implement our attack codeas a chain of ROP gadgets.
 Then all one has to do is seize control of the callstack.
 This evolved from the return-to-libc attack which uses the common sharedlibrary libc to provide well-understood gadgets; many variants have been de-veloped since, including an attack that enables malware in an SGX enclave tomount stealthy attacks on host apps [1688].
 The latest attack variant, block-oriented programming (BOP), can often generate attacks automatically fromcrashes discovered by program fuzzing, defeating current control-ﬂow integritycontrols [964].
 This coevolution of attack and defence will no doubt continue.
Finally there are side channels.
 The most recent major innovation in attacktechnology targets CPU pipeline behaviour.
 In early 2018, two game-changingattacks pioneered the genre: Meltdown, which exploits side-channels created byout-of-order execution on Intel processors [1172], and Spectre, which exploitsspeculative execution on Intel, AMD and Arm processors [1068].
 The basic ideais that large modern CPUs’ pipelines are so long and complex that they lookahead and anticipate the next dozen instructions, even if these are instructionsthat the current process wouldn’t be allowed to execute (imagine the accesscheck is two instructions in the future and the read operation it will forbid istwo instructions after that).
 The path not taken can still load information into acache and thus leak information in the form of delays.
 With some cunning, oneprocess can arrange things to read the memory of another.
 I will discuss Spectreand Meltdown in more detail in the chapter on side channels in the second partof this book.
 Although mitigations have been published, further attacks of thesame general kind keep on being discovered, and it may take several years anda new generation of processors before they are brought entirely under control.
Security Engineering230Ross Anderson6.
4.
 WHAT GOES WRONGIt all reminds me of the saying by Roger Needham at the head of this chapter.
Optimisation consists of replacing something that works with something thatalmost works, but is cheaper; and modern CPUs are so heavily optimised thatwe’re bound to see more variants on the Spectre theme.
Such attacks limitthe protection that can be o↵ered not just by containers and VMs, but alsoby enclave mechanisms such as TrustZone and SGX.
 In particular, they maystop careful ﬁrms from entrusting high-value cryptographic keys to enclavesand prolong the service life of old-fashioned hardware cryptography.
6.
4.
3User interface failuresA common way to attack a fortress is to trick the guards into helping you,and operating systems are no exception.
 One of the earliest attacks was theTrojan Horse, a program the administrator is invited to run but which containsa nasty surprise.
 People would write games that checked whether the player wasthe system administrator, and if so would create another administrator accountwith a known password.
 A variant was to write a program with the same nameas a common system utility, such as the ls command which lists all the ﬁlesin a Unix directory, and design it to abuse the administrator privilege (if any)before invoking the genuine utility.
 You then complain to the administrator thatsomething’s wrong with the directory.
 When they enter the directory and typels to see what’s there, the damage is done.
 This is an example of the confuseddeputy problem: if A does some task on behalf of B, and its authority comesfrom both A and B, and A’s authority exceeds B, things can go wrong.
 The ﬁxin this particular case was simple: an administrator’s ‘PATH’ variable (the listof directories to be searched for a suitably-named program when a command isinvoked) should not contain ‘.
’ (the symbol for the current directory).
 ModernUnix versions ship with this as a default.
 But it’s still an example of how youhave to get lots of little details right for access control to be robust, and thesedetails aren’t always obvious in advance.
Perhaps the most serious example of user interface failure, in terms of thenumber of systems historically attacked, consists of two facts: ﬁrst, Windows isforever popping up conﬁrmation dialogues, which trained people to click boxesaway to get their work done; and second, that until 2006 a user needed to bethe administrator to install anything.
 The idea was that restricting software in-stallation to admins enabled Microsoft’s big corporate customers, such as banksand government departments, to lock down their systems so that sta↵ couldn’trun games or other unauthorised software.
 But in most environments, ordinarypeople need to install software to get their work done.
 So hundreds of millionsof people had administrator privileges who shouldn’t have needed them, andinstalled malicious code when a website simply popped up a box telling them todo something.
 This was compounded by the many application developers whoinsisted that their code run as root, either out of laziness or because they wantedto collect data that they really shouldn’t have had.
 Windows Vista started tomove away from this, but a malware ecosystem is now well established in the PCworld, and one is starting to take root in the Android ecosystem as businessespressure people to install apps rather than using websites, and the apps demandaccess to all sorts of data and services that they really shouldn’t have.
 We’llSecurity Engineering231Ross Anderson6.
4.
 WHAT GOES WRONGdiscuss this later in the chapter on phones.
6.
4.
4RemediesSoftware security is not all doom and gloom; things got substantially betterduring the 2000s.
 At the turn of the century, 90% of vulnerabilties were bu↵eroverﬂows; by the time the second edition of this book came out in 2008, it wasjust under half, and now it’s even less.
 Several things made a di↵erence.
1.
 The ﬁrst consists of speciﬁc defences.
 Stack canaries are a random num-ber inserted by the compiler next to the return address on the stack.
If the stack is overwritten, then with high probability the canary willchange [487].
 Data execution prevention (DEP) marks all memory as ei-ther data or code, and prevents the former being executed; it appearedin 2003 with Windows XP.
 Address space layout randomisation (ASLR)arrived at the same time; by making the memory layout di↵erent in eachinstance of a system, it makes it harder for an attacker to predict targetaddresses.
 This is particularly important now that there are toolkits todo ROP attacks, which bypass DEP.
 Control ﬂow integrity mechanismsinvolve analysing the possible control-ﬂow graph at compile time and en-forcing this at runtime by validating indirect control-ﬂow transfers; thisappeared in 2005 and was incorporated in various products over the follow-ing decade [348].
 However the analysis is not precise, and block-orientedprogramming attacks are among the tricks that have evolved to exploitthe gaps [964].
2.
 The second consists of better general-purpose tools.
 Static-analysis pro-grams such as Coverity can ﬁnd large numbers of potential software bugsand highlight ways in which code deviates from best practice; if used fromthe start of a project, they can make a big di↵erence.
 (If added later, theycan throw up thousands of alerts that are a pain to deal with.
) The rad-ical solution is to use a better language; my colleagues increasingly writesystems code in Rust rather than in C or C++10.
3.
 The third is better training.
 In 2002, Microsoft announced a security ini-tiative that involved every programmer being trained in how to write se-cure code.
 (The book they produced for this, ‘Writing Secure Code’ [927],is still worth a read.
) Other companies followed suit.
4.
 The latest approach is DevSecOps, which I discuss in Part 3.
 Agile de-velopment methodology is extended to allow very rapid deployment ofpatches and response to incidents; it may enable the e↵ort put into de-sign, coding and testing to be aimed at the most urgent problems.
Architecture matters; having clean interfaces that evolve in a controlled way,under the eagle eye of someone experienced who has a long-term stake in thesecurity of the product, can make a huge di↵erence.
 Programs should only have10Rust emerged from Mozilla research in 2010 and has been used to redevelop Firefox; it’sbeen voted the favourite language in the Stack Overﬂow annual survey from 2016–2019.
Security Engineering232Ross Anderson6.
4.
 WHAT GOES WRONGas much privilege as they need: the principle of least privilege [1639].
 Softwareshould also be designed so that the default conﬁguration, and in general, theeasiest way of doing something, should be safe.
 Sound architecture is criticalin achieving safe defaults and using least privilege.
 However, many systems areshipped with dangerous defaults and messy code, exposing all sorts of interfacesto attacks like SQL injection that just shouldn’t happen.
 These involve failuresof incentives, personal and corporate, as well as inadequate education and thepoor usability of security tools.
6.
4.
5Environmental creepMany security failures result when environmental change undermines a securitymodel.
 Mechanisms that worked adequately in an initial environment often failin a wider one.
Access control mechanisms are no exception.
 Unix, for example, was origi-nally designed as a ‘single user Multics’ (hence the name).
 It then became anoperating system to be used by a number of skilled and trustworthy people in alaboratory who were sharing a single machine.
 In this environment the functionof the security mechanisms is mostly to contain mistakes; to prevent one user’styping errors or program crashes from deleting or overwriting another user’sﬁles.
 The original security mechanisms were quite adequate for this purpose.
But Unix security became a classic ‘success disaster’.
Over the 50 yearssince Ken Thomson started work on it at Bell Labs in 1969, Unix was repeat-edly extended without proper consideration being given to how the protectionmechanisms also needed to be extended.
The Berkeley versions assumed anextension from a single machine to a network of machines that were all onone LAN and all under one management.
 The Internet mechanisms (telnet,ftp, DNS, SMTP) were originally written for mainframes on a secure network.
Mainframes were autonomous, the network was outside the security protocols,and there was no transfer of authorisation.
 So remote authentication, which theBerkeley model really needed, was simply not supported.
 The Sun extensionssuch as NFS added to the party, assuming a single ﬁrm with multiple trustedLANs.
 We’ve had to retroﬁt protocols like Kerberos, TLS and SSH as duct tapeto hold the world together.
 The arrival of billions of phones, which communicatesometimes by wiﬁ and sometimes by a mobile network, and which run apps frommillions of authors (most of them selﬁsh, some of them actively malicious), hasleft security engineers running ever faster to catch up.
Mixing many di↵erent models of computation together has been a factor inthe present chaos.
 Some of their initial assumptions still apply partially, butnone of them apply globally any more.
 The Internet now has billions of phones,billions of IoT devices, maybe a billion PCs, and millions of organisations whosemanagers not only fail to cooperate but may be in conﬂict.
 There are companiesthat compete; political groups that despise each other, and nation states thatare at war with each other.
 Users, instead of being trustworthy but occasionallyincompetent, are now largely unskilled – but some are both capable and hostile.
Code used to be simply buggy – but now there is a lot of malicious code outthere.
 Attacks on communications used to be the purview of intelligence agencies– now they can be done by youngsters who’ve downloaded attack tools from theSecurity Engineering233Ross Anderson6.
5.
 SUMMARYnet and launched them without any real idea of how they work.
6.
5SummaryAccess control mechanisms operate at a number of levels in a system, from thehardware up through the operating system and middleware like browsers to theapplications.
 Higher-level mechanisms can be more expressive, but also tendto be more vulnerable to attack for a variety of reasons ranging from intrinsiccomplexity to implementer skill.
The main function of access control is to limit the damage that can be doneby particular groups, users, and programs whether through error or malice.
 Themost widely ﬁelded examples are Android and Windows at the client end andLinux at the server end; they have a common lineage and many architecturalsimilarities.
 The basic mechanisms (and their problems) are pervasive.
 Most at-tacks involve the opportunistic exploitation of bugs; products that are complex,widely used, or both are particularly likely to have vulnerabilities found andturned into exploits.
 Many techniques have been developed to push back on thenumber of implementation errors, to make it less likely that the resulting bugsgive rise to vulnerabilties, and harder to turn the vulnerabilities into exploits;but the overall dependability of large software systems improves only slowly.
Research ProblemsMost of the issues in access control were identiﬁed by the 1960s or early 1970sand were worked out on experimental systems such as Multics [1684] and theCAP [2020].
Much of the research in access control systems since then hasinvolved reworking the basic themes in new contexts, such as mobile phones.
Recent threads of research include enclaves, and the CHERI mechanisms foradding ﬁner-grained access control.
 Another question is: how will developersuse such tools e↵ectively?In the second edition I predicted that ‘a useful research topic for the nextfew years will be how to engineer access control mechanisms that are not justrobust but also usable – by both programmers and end users.
’ Recent workby Yasemin Acar and others has picked that up and developed it into one ofthe most rapidly-growing ﬁelds of security research [11].
Many if not mosttechnical security failures are due at least in part to the poor usability of theprotection mechanisms that developers are expected to use.
 I already mentionin the chapter on cryptography how crypto APIs often induce people to usereally unsafe defaults, such as encrypting long messages with ECB mode; accesscontrol is just as bad, as anyone coming cold to the access control mechanismsin a Windows system or either an Intel or Arm CPU will ﬁnd.
As a teaser, here’s a new problem.
 Can we extend what we know aboutaccess control at the technical level – whether hardware, OS or app – to theorganisational level? In the 20th century, there were a number of security poli-cies proposed, from Bell-LaPadula to Clark-Wilson, which we discuss at greaterSecurity Engineering234Ross Anderson6.
5.
 SUMMARYlength in Part 2.
 Is it time to revisit this for a world of deep outsourcing andvirtual organisations, now that we have interesting technical analogues?Further ReadingThere’s a history of virtualisation and containers by Allison Randal at [1575]; adiscussion of how mandatory access controls were adapted to operating systemssuch as OS X and iOS by Robert Watson in [1993]; and a reference book forJava security written by its architect Li Gong [783].
 The Cloud Native Secu-rity Foundation is trying to move people towards better open-source practicesaround containers and other technologies for deploying and managing cloud-native software.
 Going back a bit, the classic descriptions of Unix security areby Fred Grampp and Robert Morris in 1984 [805] and by Simson Garﬁnkel andEugene Spa↵ord in 1996 [753], while the classic on Internet security by BillCheswick and Steve Bellovin [221] gives many examples of network attacks onUnix systems.
Carl Landwehr gives a useful reference to many of the ﬂaws found in oper-ating systems in the 1960s through the 1980s [1129].
 One of the earliest reportson the subject (and indeed on computer security in general) is by Willis Warein 1970 [1986]; Butler Lampson’s seminal paper on the conﬁnement problemappeared in 1970s [1125] and three years later, another inﬂuential early paperwas written by Jerry Saltzer and Mike Schroeder [1639].
 The textbook we getour students to read on access control issues is Dieter Gollmann’s ‘ComputerSecurity’ [779].
The standard reference on Intel’s SGX and indeed its CPUsecurity architecture is by Victor Costan and Srini Devadas [479].
The ﬁeld of software security is fast-moving; the attacks change signiﬁcantly(at least in their details) from one year to the next.
 The classic starting point isGary McGraw’s 2006 book [1266].
 Since then we’ve had ROP attacks, Spectreand much else; a short but useful update is Matthias Payer’s Software Secu-rity [1504].
 But to really keep up, it’s not enough to just read textbooks; youneed to follow security conferences such as Usenix and CCS as well as the se-curity blogs such as Bruce Schneier, Brian Krebs and – dare I say it – our ownlightbluetouchpaper.
org.
 The most detail on the current attacks is proba-bly in Google’s Project Zero blog; see for example their analysis of attacks oniPhones found in the wild for an insight into what’s involved in hacking modernoperating systems with mandatory access control components [204].
Security Engineering235Ross Anderson