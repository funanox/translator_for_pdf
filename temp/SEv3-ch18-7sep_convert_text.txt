Chapter 18Tamper ResistanceIt is relatively easy to build an encryption system that is secure if itis working as intended and is used correctly but it is still very hardto build a system that does not compromise its security in situationsin which it is either misused or one or more of its sub-componentsfails (or is ’encouraged’ to misbehave) .
.
.
 this is now the only areawhere the closed world is still a long way ahead of the open worldand the many failures we see in commercial cryptographic systemsprovide some evidence for this.
– BRIAN GLADMANThe amount of careful, critical security thinking that has gone into agiven security device, system or program is inversely proportional tothe amount of high-technology it uses.
– ROGER JOHNSTON18.
1IntroductionTamper-resistant devices are everywhere now.
 Examples we’ve discussed so farinclude:• the EMV chips used in bank cards and the SIMs used in mobile phonesfor authentication;• the contactless cards used as transport tickets and the smartcards used inpay-TV decoders for service control;• chips used for accessory control in printer toner cartridges and game-console accessories;• the TPM chips in phones, laptops and servers to provide a root of trust tosupport secure boot and hard-disk encryption;• hardware security modules used to encrypt bank PINs, not just in bankserver farms but in ATMs and some point-of-sale terminals;54718.
1.
 INTRODUCTION• the NFC chips used in Android phones to store contactless payment cre-dentials, and the enclave chips in iPhones that store your ﬁngerprint andcrypto keys;• cryptographic modules buried in vending machines that sell everythingfrom railway tickets through postage stamps to the magic numbers thatactivate your electricity meter;• various chips used for manufacturing control by ﬁrms who want to havetheir products made by low-cost overseas manufacturers but don’t wantto see extra products made without their consent on a ‘third shift’ andsold on the grey market.
Many of the devices on the market are insecure.
 In section 4.
3.
1 I describedhow reverse engineering remote key entry devices for cars led to class breaks thatnotably increased car theft; in section 13.
2.
5 I described how reverse engineer-ing the Mifare card compromised many building locks and transport ticketingsystems; and in section 12.
6.
1.
1, I described card payment terminals that couldbe compromised trivially, leading to card counterfeiting and transaction manip-ulation attacks.
Yet some are pretty good.
 The best cryptographic modules used in bankingand government withstand all known types of physical attack, and can only bedefeated when people either run insecure software on them or rely on insecuredevices to interface with users.
Smartcard tamper resistance has evolved ina long war between pay-TV pirates cloning subscriber cards and the pay-TVindustry trying to stop them, and was honed in an arms race between ﬁrms thatwanted to lock down their products, and others who wanted to unlock them.
 Thetussles over printer cartridges were important here, as both the printer makerswho were trying to control aftermarkets, and the independent cartridge makerswho were trying to break into these markets, are acting lawfully.
 Other hackerswork for lawyers, reverse engineering products to prove patent infringements.
There are academics who hack systems for glory, and to push forward the stateof the art.
 And ﬁnally there are lots of grey areas.
 If you ﬁnd a way to unlocka mobile phone, so that it can be used on any network, is that a crime? Itdepends on how you do it, and on what country you’re in.
Given the wide range of products and the huge variation in quality, thesecurity engineer needs to understand what tamper resistance is, and what itcan and can’t do.
 In this chapter I’m going to take you through the past thirtyyears of evolution of attack and defence.
If a computer cannot resist physical tampering, an attacker can simplychange the software.
 Computers in data centres are protected by physical barri-ers, sensors and alarms.
 And an ATM is basically a PC in a safe with banknotedispensers and alarm sensors, often bolted to a wall or a plinth.
Where tamper resistance is needed purely for integrity and availability, itcan sometimes be implemented using replication on di↵erent servers that per-form transactions simultaneously and vote on the result; this is being rein-vented nowadays with blockchains and other consensus protocols.
 The thresh-old schemes discussed in section 15.
4 can also provide conﬁdentiality for keymaterial.
 But tamper-resistant devices can provide conﬁdentiality for the dataSecurity Engineering548Ross Anderson18.
2.
 HISTORYtoo, and the arrival of CPUs that support enclaves such as SGX and TrustZonehold out the prospect of computing with encrypted data in cloud services.
18.
2HistoryThe use of tamper resistance in cryptography goes back centuries [1001].
 Navalcodebooks were weighted so they could be thrown overboard if capture wasimminent; the dispatch boxes used by British government ministers’ aides tocarry state papers were lead-lined to make sure they’d sink.
 Codes have beenprinted in water-soluble ink; Russian one-time pads were printed on cellulosenitrate, so they’d burn furiously if lit; and one US wartime cipher machinecame with self-destruct thermite charges.
 But key material was often capturedin surprise attacks, so attempts were made to automate the tamper responseprocess.
 Some mechanical cipher machines were built so that opening the caseerased the key settings, and early electronic devices followed suit.
After the notorious Walker family sold US Navy key material to the Russiansfor over 20 years [876], engineers paid more attention to the question of how toprotect keys in transit too.
 The goal was ‘to reduce the street value of keymaterial to zero’, and this can be achieved either by tamper resistant devicesfrom which the key cannot be readily extracted, or tamper evident ones fromwhich key extraction would be obvious.
Paper keys were once carried in ‘tattle-tale containers’, designed to showevidence of tampering.
 When electronic key distribution came along, a typicalsolution was the ‘ﬁll gun’: a portable device that dispenses crypto keys in acontrolled way.
 Nowadays the physical transport of crypto key material usuallyinvolves a smartcard, or a similar chip packaged as a key.
 Your SIM card andbank card are just the most visible examples.
 The control of key material alsoacquired broader purposes, with both the US and the UK governments using itto restrict their networks to approved devices.
 Live key material would only besupplied once the system had been properly accredited.
Once initial keys have been loaded, further keys may be distributed usingauthentication protocols.
 Our subject here is the physical defenses against tam-pering.
18.
3Hardware Security ModulesThe IBM 4758 (Figures 18.
1 and 18.
2) was the leading commercial cryptographicprocessor in the early 2000s, and is important for four reasons.
 First, it wasthe ﬁrst commercial product to be evaluated to the highest level of tamperresistance (FIPS 140-1 level 4) [1399] then set by the US government.
 Second,there is an extensive literature about it, including its history, hardware andsoftware [1795, 1998, 2001].
 Third, it was therefore a high proﬁle target, andfrom 2000–2005 my students and I put a lot of e↵ort into attacking it andunderstanding the residual vulnerabilities.
Fourth, the current IBM ﬂagshipproduct, the 4765, isn’t hugely changed except for ﬁxing some of the bugs wefound.
Security Engineering549Ross Anderson18.
3.
 HARDWARE SECURITY MODULESFigure 18.
1: – the IBM 4758 cryptoprocessor (courtesy of Steve Weingart)The back story starts in the 1970s, when Mikhail Atalla had the idea ofa black-box cryptographic module to manage bank PINs.
As early crypto-graphic schemes for ATMs were rather weak, IBM developed a better blockcipher which became the Data Encryption Standard, as described in Chapter 5.
There followed a period of intense research about precisely how block cipherscould be used to manage PINs in a single bank, and then in a network of manybanks [1301].
 The banking community realised that commercial operating sys-tems were likely to remain insu�cient to protect PINs, particularly from bankinsiders, and decided to use separate hardware to manage them.
This led to the development of standalone cryptographic modules or hard-ware security modules (HSMs), as ﬁntech people call them.
 These are micro-computers encased in robust metal enclosures, with encryption hardware andspecial key memory, static RAM that is zeroized when the enclosure is opened.
Initially, this just involved wiring the power supply to the key memory througha number of lid switches.
 So whenever the maintenance crew came to replacebatteries, they’d open the lid and destroy the keys.
 Once they’d ﬁnished, theHSM custodians would reload the key material.
 In this way, the HSM’s ownercould hope that its keys were under the unique control of its own trustworthysta↵.
How to hack a cryptoprocessor (1)The obvious attack is just to steal the keys.
 In early HSMs, the master keyswere kept in PROMs that were loaded into a special socket in the device tobe read during initialization, or as strings of numbers that were typed in at aconsole.
 The PROMs could be pocketed, taken home and read out.
 CleartextSecurity Engineering550Ross Anderson18.
3.
 HARDWARE SECURITY MODULESFigure 18.
2: – the 4758 partially opened showing (from top left downward)the circuitry, aluminium electromagnetic shielding, tamper-sensing mesh andpotting material (courtesy of Frank Stajano)paper keys were even easier: just scribble down a copy.
The ﬁx was shared control – to have two or three master key components,and make the actual master key by combining them.
 The PROMs (or paperkeys) would be kept in di↵erent safes under the control of di↵erent departments.
This taught us that shared control is a serious security usability hazard.
 Themanual may tell the custodians to erase the live keys, let the engineer ﬁx thedevice, and then re-load the keys afterwards.
 But many senior men used tothink that touching keyboards was women’s work, and even today they thinkthat technical work is beneath them.
 And who reads the manual anyway? Somanagers often give both keys to the engineer to save the bother.
 In one case,a dishonest engineer got them to enter the keys using a laptop that acted as aterminal but had logging switched on [54].
 I’ve even come across cases of papermaster keys for an automatic teller machine being kept in the correspondenceﬁle in a bank branch, where any of the sta↵ could look them up.
How to hack a cryptoprocessor (2)Early devices were vulnerable to attackers cutting through the casing.
 Second-generation devices made physical attacks harder by adding photocells and tiltswitches.
 But the di�cult opponent is the maintenance engineer – who coulddisable the sensors on one visit and extract the keys on the next.
By about 2000, the better products separated all the components that canbe serviced (such as batteries) from the core of the device (such as the tampersensors, cryptoprocessor, key memory and alarm circuits).
 The core was thenpotted into a solid block of a hard, opaque substance such as epoxy.
 The ideaSecurity Engineering551Ross Anderson18.
3.
 HARDWARE SECURITY MODULESwas that any physical attack would involve cutting or drilling, which could bedetected by the guard who accompanies the engineer into the bank computerroom1.
At least it should leave evidence of tampering after the fact.
Thisis the level of protection needed for medium-level evaluations under the FIPSstandard.
How to hack a cryptoprocessor (3)However, if a competent attacker can get unsupervised access to the devicefor even a short period of time – and, to be realistic, that’s what the maintenanceengineer probably has, as the guard doesn’t understand what’s going on – thenpotting the device core is inadequate.
For example, you might scrape awaythe potting with a knife and drop the probe from a logic analyzer on to one ofthe chips.
 In theory, scraping the sticky epoxy should damage the componentsinside; in practice, it’s just a matter of patience.
 Cryptographic algorithms suchas RSA, DES and AES have the property that an attacker who can monitor anybitplane during the computation can recover the key [860].
So the high-end products acquired a tamper-sensing barrier.
 An early exam-ple appeared in IBM’s µABYSS system in the mid-1980s, which used loops of40-gauge nichrome wire wound loosely around the device as it was embedded inepoxy, and then connected to a sensing circuit [1998].
 The theory was that tech-niques such as milling, etching and laser ablation would break the wire, erasingthe keys.
 But the wire-in-epoxy technique can be vulnerable to slow erosionusing sand blasting; when the sensing wires become visible at the surface ofthe potting, shunts can be connected round them.
 In 2018 Sergei Skoroboga-tov managed to use a combination of acid etching and masking to expose abattery-powered chip, on the Vasco Digipass 270, showing that given decent labtechnique you can indeed attack live circuits protected by wires in epoxy [1781].
The next major product from IBM, the 4753, used a metal shield combinedwith a membrane printed with a pattern of conductive ink and surrounded by amore durable material of similar chemistry.
 The idea was that any attack wouldbreak the membrane with high probability.
 The 4758 had an improved tamper-sensing membrane in which four overlapping zig-zag conducting patterns weredoped into a urethane sheet, which was potted in a chemically similar substanceso that an attacker cutting into the device had di�culty even detecting theconductive path, let alone connecting to it.
 This potting surrounds the metalshielding which in turn contains the cryptographic core.
 The design is describedin more detail in [1795].
How to hack a cryptoprocessor (4)The next class of attack exploited memory remanence, the fact that manykinds of computer memory retain some trace of data that have been storedthere.
 Once a certain security module had run for some years using the samemaster keys, their values burned in to the device’s static RAM.
 On power-up,about 90% of the relevant memory bits would assume the values of the previ-ously stored secret keybits, which was quite enough to recover the keys [107].
1That at least was the theory; experience suggests it’s a bit much to ask a minimum-wageguard to ensure that a specialist in some exotic piece of equipment repairs it using some toolsbut not others.
Security Engineering552Ross Anderson18.
3.
 HARDWARE SECURITY MODULESMemory remanence a↵ects not just static and dynamic RAM, but other storagemedia too.
 The relevant engineering and physics issues are discussed in [837]and[840], and in 2005 Sergei Skorobogatov discovered how to extract datafrom Flash memory in microcontrollers, even after it had been ‘erased’ severaltimes [1770]; like it or not, the wear-levelling processors in Flash chips becomepart of your trusted computing base.
 RAM contents can also be burned in byionising radiation, so radiation sensing or hardening might make sense too.
How to hack a cryptoprocessor (5)Computer memory can also be frozen by low temperatures.
 By the 1980s itwas realized that below about -20o C, static RAM contents can persist for severalseconds after power is removed.
 This extends to minutes at the temperaturesof liquid nitrogen.
 So an attacker might freeze a device, remove the power, cutthrough the tamper sensing barrier, extract the RAM chips containing the keys,and power them up again in a test rig.
In 2008, Alex Halderman and colleagues developed this into the cold bootattack on encryption keys in PCs and phones [854].
 Modern DRAM retainsmemory contents for several seconds after power is removed, and even longer atlow temperatures; by chilling memory with a freezing spray, then rebooting thedevice with a lightweight operating system, keys can often be read out.
 Softwareencryption of disk contents can be defeated unless there are mechanisms tozeroise the keys on power-down.
 Even keeping keys in special hardware suchas a TPM isn’t enough if all it’s doing is limiting the number of times you canguess the hard disk encryption password, but then copying the master key tomain memory once you get the password right so that the CPU can do the restof the work.
 You need to really understand what guarantees the crypto chip isgiving you – a matter we’ll discuss at greater length in the chapter on AdvancedCryptographic Engineering.
Anyway, the better cryptographic devices have temperature and radiationalarms.
 But modern RAM chips exhibit a wide variety of memory remanencebehaviors; remanence seems to have got longer as feature sizes have shrunk,and in unpredictable ways even within standard product lines.
So althoughyour product might pass a remanence test using a given make of SRAM chip, itmight fail the same test with the same make of chip purchased a year later [1768].
This shows the dangers of relying on a property of some component to whosemanufacturer this property is unimportant.
The main constraints on the HSM alarms are similar to those we encoun-tered with more general alarms.
 There’s a trade-o↵ between the false alarmrate and the missed alarm rate, and thus between security and robustness.
 Vi-bration, power transients and electromagnetic interference can be a problem,but temperature is the worst.
 A device that self-destructs if frozen can’t be sentreliably through normal distribution channels, as aircraft holds can get as low as-40oC.
 (We’ve bought crypto modules on eBay and found them dead on arrival.
)Military equipment makers have the converse problem: their kit must be ratedfrom -55o to +155o C.
 Some military devices use protective detonation; mem-ory chips are potted in steel cans with a thermite charge precisely calculated todestroy the chip without causing gas release from the can.
 Meeting simultane-ous targets for tamper resistance, temperature tolerance, radiation hardening,Security Engineering553Ross Anderson18.
3.
 HARDWARE SECURITY MODULESshipping safety, weight and cost can be nontrivial.
How to hack a cryptoprocessor (6)The next set of attacks on cryptographic hardware involve monitoring theRF and other electromagnetic signals emitted by the device, or even injectingsignals into it and measuring their externally visible e↵ects.
 This technique,which is variously known as ‘Tempest’, ‘power analysis,’ ‘side-channel attacks’or ‘emission security’, is such a large subject that I devote the next chapter toit.
As far as the 4758 was concerned, the strategy was to have solid aluminiumshielding and to low-pass ﬁlter the power supply to block the egress of any signalsat the frequencies used internally for computation.
 This shielding is inside thetamper-sensing membrane, to prevent an opponent cutting a slot that couldfunction as an antenna.
How to hack a cryptoprocessor (7)We never ﬁgured out how to attack the hardware of the 4758.
 The attacks wehave seen on high-end systems have involved the exploitation of logical ratherthan physical ﬂaws.
 One hardware security module, the Chrysalis-ITS LunaCA3, had its key token’s software reverse engineered by Mike Bond, DanielCvrˇcek and Steven Murdoch who found code that enabled an unauthenticated“Customer Veriﬁcation Key” to be introduced and used to certify the exportof live keys [283].
 Most recently, in 2019, Gabriel Campana and Jean-BaptisteB´edrune found a bu↵er overﬂow attack on the Gemalto Safenet Protect ServerPSI-E2/PSE2 by fuzzing the HSM emulator that came with its software devel-opment kit, then checked this on a real HSM, and wrote code to upload arbitraryﬁrmware, which is persistent and can download all the secrets [203].
This did not happen to IBM’s 4758, which had a formally veriﬁed operatingsystem.
 But most of its users ran a banking crypto application called CCAthat is described in [915].
 Mike Bond and I discovered that the application pro-gramming interface (API) that CCA exposed to the host contained a numberof exploitable ﬂaws.
 The e↵ect was that a programmer with access to the hostcould send the security module a series of commands that would cause it to leakPINs or keys.
 These vulnerabilities were largely the legacy of previous encryp-tion devices with which 4758 users needed to be backward compatible, and infact most other security modules were worse.
 Such attacks were hard to stop, asfrom time to time Visa would mandate new cryptographic operations to supportnew payment network features and these would introduce new systemic vulner-abilities across the whole ﬂeet of security modules [22].
 Some HSMs now havetwo APIs: an internal one which the vendor tries to keep clean (but which needsto have the ability to import and export keys) and an external one which im-plements the standards of whatever industry the HSM is being used to support.
The software between the two APIs may be trusted, but can be hard to maketrustworthy if the external API is insecure.
 In e↵ect, it has to anticipate andblock API attacks.
 So many banks pay top dollar for secure HSMs which theyuse for formal compliance while actually relying on other access control mecha-nisms to shield the devices from attack.
 There are even specialist ﬁrms sellingﬁrewalls to shield HSMs from software-based harm.
 I’ll discuss API attacks inSecurity Engineering554Ross Anderson18.
4.
 EVALUATIONdetail in the chapter on Advanced Cryptographic Engineering.
18.
4EvaluationA few comments about the evaluation of HSMs are in order before we go onto discuss cheaper devices.
 When IBM launched the 4753 they proposed thefollowing classiﬁcation of attackers in the associated white paper [9]:1.
 Class 1 attackers – ‘clever outsiders’ – are often very intelligent but mayhave insu�cient knowledge of the system.
 They may have access to onlymoderately sophisticated equipment.
 They often try to take advantage ofan existing weakness in the system, rather than try to create one.
2.
 Class 2 attackers – ‘knowledgeable insiders’ – have substantial specializedtechnical education and experience.
 They have varying degrees of under-standing of parts of the system but potential access to most of it.
 Theyoften have highly sophisticated tools and instruments for analysis.
3.
 Class 3 attackers – ‘funded organizations’ – are able to assemble teams ofspecialists with related and complementary skills backed by great fundingresources.
 They are capable of in-depth analysis of the system, designingsophisticated attacks, and using the most advanced analysis tools.
 Theymay use Class 2 adversaries as part of the attack team.
Within this scheme, the typical microcontroller is aimed at blocking cleveroutsiders; the early 4753 aimed at stopping knowledgeable insiders, and the 4758was aimed at (and certiﬁed for) blocking funded organizations.
 By the way, thisclassiﬁcation is becoming a bit dated; we see class 1 attackers renting access toclass 3 equipment.
 And class 3 attackers nowadays are not just national labs,but your commercial competitors and even university security teams.
 In ourcase, we have people with backgrounds in maths, physics, software and banking,and we’ve had friendly manufacturers giving us samples of their competitors’products for us to break.
The FIPS certiﬁcation scheme is operated by laboratories licensed by theUS government.
 The original 1994 standard, FIPS 140-1, set out four levels ofprotection, with level 4 being the highest, and this remained in the next version,FIPS 140-2, which was introduced in 2001.
 There was a huge gap between level4 and level 3; devices at that level were often easy for experts to attack.
 In fact,the original paper on evaluation by IBM engineers proposed six levels [2001];the FIPS standard adopted the ﬁrst three of these as its levels 1–3, and theproposed level 6 as its level 4 (the 4758 designer Steve Weingart tells the storyin [2000]).
 The gap, commonly referred to as level 3.
5 or 3+, is where many ofthe better commercial systems were aimed from the 1990s through 2019.
 Suchequipment attempts to keep out the class 1 attack community, while making lifehard for class 2 and expensive for class 3.
There was about a decade of consultation about whether to abandon FIPS140 in favour of ISO 19790 – a move supported by vendors, particularly thoseSecurity Engineering555Ross Anderson18.
5.
 SMARTCARDS AND OTHER SECURITY CHIPSoutside the USA.
 Critics of the FIPS approach noted that it didn’t cover non-invasive security such as bu↵er overﬂows and API attacks; that its concept ofroles was tied to human actors in companies, rather than other system com-ponents; that it failed to cover some methods of side-channel analysis; that itwas generally aimed at outdated technology; that the FIPS standard includesthe dual elliptic curve deterministic random bit generator, known to containan NSA backdoor; and that it was changed too often by NIST issuing im-plementation guidelines, rather than by updating the standard regularly [1410].
Eventually, the US Department of Commerce gave up and approved an updatedversion, FIPS 140-3, which simply refers to the ISO standards 19790:2012 and24759:2017, and speciﬁes some reﬁnements.
 This came into force in September2019 and in 2021 testing under FIPS 140-2 will cease.
18.
5Smartcards and other security chipsWhile there are tens of thousands of HSMs in use, there are billions of self-contained one-chip crypto modules containing nonvolatile memory, I/O, usuallya CPU, often some specialised logic, and mechanisms to protect memory frombeing read out.
 Most are packaged as cards, while some look like physical keys.
They range from transport tickets at the low end, through smartcards and theTPMs that now ship with most computers and phones, up to pay-TV cardsand accessory control chips designed to withstand attack by capable motivatedopponents for as long as possible.
Many attacks have been developed; we already discussed the consequencesof the breaks of the Mifare cards and car keys.
 Pay-TV subscriber cards inparticular have been subjected to intensive attacks as they often have a universalshared secret key, so a compromise enables an attacker to make lots of counterfeitcards, while a break of a bank smartcard only lets the attacker loot that speciﬁcbank account.
 The accessory control chips in printer cartridges also protect alot of ‘value’, and have driven real innovation in both attack and defence.
 Inthis section, I’ll tell the story of how chip-level security evolved.
18.
5.
1HistorySmartcards were developed in France from the mid-70s to mid-80s; for the earlyhistory, see [832].
 From the late 1980s, they started to be used at scale, initiallyas the subscriber identity modules (SIMs) in GSM mobile phones and as sub-scriber cards for satellite-TV stations.
 They started being used as bank cardsin France and South Africa in 1994, followed by trials in the UK and Norway;this led to the EMV standard I mentioned in the chapter on banking and book-keeping, with deployment in the rest of Europe from 2003 and the USA fromabout 2015.
A smartcard is a self-contained microcontroller, with a microprocessor, mem-ory and a serial interface integrated in a single chip and packaged in a plasticcard.
 Smartcards used in banking use a standard-size bank card, while in mod-ern mobile phones a much smaller size is used.
 Smartcard chips are also packagedin other ways.
 In the STU-III secure telephones used in the US government fromSecurity Engineering556Ross Anderson18.
5.
 SMARTCARDS AND OTHER SECURITY CHIPS1987–2009, each user had a ‘crypto ignition key’, packaged to look and feel likea physical key; some prepayment electricity meters and pay-TV set-top boxesused the same approach.
 The TPM chips built into computer motherboardsto support trusted boot are basically smartcard chips with an added parallelport, so the TPM can verify that the right software is being used to start upthe computer.
 Contactless smartcards contain a smartcard chip plus a wire-loop antenna; most car keys are a slightly more complex version of the sameidea, with an added battery to give greater range.
 In what follows I’ll mostlydisregard the packaging form factor and just refer to single-chip cryptographicmodules as ‘smartcards’ or ‘chipcards’.
Apart from bank cards, the single most widespread application is the mobilephone SIM.
 The handsets are personalized for each user by the SIM, whichcontains the key with which you authenticate yourself to the network.
Thestrategy of using a cheap card to personalise a more expensive electronic deviceis found in other applications from pay-TV set-top boxes to smart meters.
 Thedevice can be manufactured in bulk for global markets, while each subscribergets a card to pay for service.
 The cards can also be replaced relatively quicklyand cheaply in the event of a successful attack.
18.
5.
2ArchitectureThe typical smartcard consists of a single die of up to 25 square millimeters ofsilicon containing a microprocessor (larger dies are more likely to break as thecard is ﬂexed).
 Cheap products have an 8-bit processor such as an 8051 or 6805,and the more expensive products have either a modular multiplication circuitto do public-key cryptography, or a 32-bit processor such as an Arm, or indeedboth (hardware crypto is easier to protect against side-channel attacks).
 Thehigh-end ones also tend to have a hardware random number generator.
 There’salso serial I/O and a hierarchy of memory – ROM or Flash to hold the programand immutable data, Flash or EEPROM to hold customer data such as theuser’s account number, crypto keys, PIN retry counters and value counters; andRAM to hold transient data during computation.
The memory is limited by the standards of normal computers; outside thedevice, the only connections are for power, reset, a clock and a serial port.
 Thephysical, electrical and low-level logical connections, together with a ﬁle-system-like access protocol, are speciﬁed in ISO 7816.
 There are several main softwarearchitectures on o↵er, including at the bottom end the Application ProgrammingData Units (APDUs) deﬁned by ISO 7816 which allow a reader to invoke speciﬁcapplications directly, through the Multos operating system to JavaCard, wherethe card can run apps written in a subset of the Java language, and which you(and your opponents in the underground) can use to code up custom apps2.
You can even buy overlay SIMs – smartcards 160 microns thick, with contactstop and bottom, which you can program in JavaCard to carry out middlepersonattacks on other smartcards (you stick the overlay on top of the target device).
2JavaCard has quietly become one of the most widely deployed operating systems in theworld with over 6 billion cards sold [1250].
Security Engineering557Ross Anderson18.
5.
 SMARTCARDS AND OTHER SECURITY CHIPS18.
5.
3Security evolutionWhen I ﬁrst heard a sales pitch from a smartcard vendor – in 1986 when I wasworking as a banker – I asked how come the device was secure.
 I was assuredthat since the machinery needed to make the card cost $20m, just as for makingbanknotes, the system must be secure.
 I didn’t believe this but didn’t havethe time or the tools to prove the claim wrong.
 I later learned from industryexecutives that none of their customers were prepared to pay for serious securityuntil about 1995, and so until then they relied on the small size of the devices,the obscurity of their design, and the inaccessibility of chip testing tools to makeattacks more di�cult.
 In any case, so long as they were only used for SIM cards,there were no capable motivated opponents.
 All I can achieve by hacking mySIM card is the ability to charge calls to my own account.
The application that changed this was satellite TV.
 TV operators broadcasttheir signals over a large footprint – such as all of Europe – and give eachsubscriber a card to compute the keys needed to decipher the channels they’vepaid for.
 Since the operators had usually only bought the rights to the moviesfor one or two countries, they couldn’t sell subscriber cards elsewhere.
 Thiscreated a black market, into which forged cards could be sold.
 A critical factorwas that ‘Star Trek’, which people in Europe had picked up from UK satellitebroadcasts for years, was suddenly encrypted in 1993.
 In some countries, suchas Germany, it wasn’t available legally at any price.
 This motivated a lot of keenyoung computer science and engineering students to look for vulnerabilities.
 Afurther factor was that some countries, notably Ireland and Canada, didn’t havelaws yet against selling forged pay-TV cards; Canada didn’t do this until 2002.
So hackers could sell their wares openly.
This rapidly had knock-on e↵ects.
 The ﬁrst large ﬁnancial fraud reportedto involve a cloned smartcard was about a year later, in February/March 1995.
The perpetrator targeted a card used to give Portuguese farmers rebates onfuel, conspiring with petrol stations who registered other fuel sales to the boguscards in return for a share of the proceeds.
 The proceeds were reported to havebeen about $30m [1330].
How to hack a smartcard (1)The earliest hacks targeted the protocols rather than the cards themselves.
For example, some early pay-TV systems gave each customer a card with accessto all channels, and then sent messages over the air to cancel those channelsto which the customer hadn’t subscribed after an introductory period.
 Thisopened an attack in which a device was inserted between the smartcard andthe decoder to intercept and discard any messages addressed to the card.
 Soyou could cancel your subscription without the vendor being able to cancel yourservice.
The same kind of attack was launched on the German phone cardsystem, with handmade chip cards sold in brothels and in hostels for asylumseekers [1813, 184].
How to hack a smartcard (2)As smartcards use an external power supply, and store security state such ascrypto keys and value counters in EEPROM, an attacker could freeze the EEP-Security Engineering558Ross Anderson18.
5.
 SMARTCARDS AND OTHER SECURITY CHIPSROM contents by removing the programming voltage, VP P .
 Early smartcardsreceived VP P from the card reader on a dedicated contact.
 So by covering thiscontact with sticky tape, cardholders could prevent a value counter from beingdecremented.
 With some payphone chipcards, this gave inﬁnite units.
The ﬁx was to generate VP P internally from the supply voltage VCC using avoltage multiplier.
 However, this isn’t foolproof as the circuit can be destroyedby an attacker, for example with a laser shot.
 As well as bypassing value controls,they can also bypass a PIN retry counter and try every possible PIN, one afteranother.
So a prudent programmer won’t just ask for a customer PIN anddecrement the counter if it fails.
 You decrement the counter, check it, get thePIN, verify it, and if it’s correct then increment the counter again3.
How to hack a smartcard (3)Another early attack was to read the voltages on the chip surface using ascanning electron microscope (SEM).
 The low-cost SEMs found in universitiesback then couldn’t do voltage contrast microscopy at more than a few tens ofkilohertz, so attackers would slow down the clock.
 In one card, attackers foundthey read out RAM contents with a suitable transaction after reset, as workingmemory wasn’t zeroized.
Modern smartcard processors have a watchdog timer or other circuit to de-tect low clock frequency and reset the card, or else use dynamic logic.
 And theattacker could sometimes single-step the program by repeatedly resetting thecard and clocking it n times, then n+1 times, and so on.
 But as with burglaralarms, there’s a trade-o↵ between false alarms and missed alarms.
 Cheap cardreaders can have wild ﬂuctuations in clock frequency when a card is poweredup, causing many false alarms.
 Eventually, cards acquired an internal clock.
How to hack a smartcard (4)Once pay-TV operators had blocked the easy attacks, pirates turned to phys-ical probing.
 Early smartcards had no protection against physical tamperingexcept the microscopic scale of the circuit, a thin glass passivation layer on thesurface of the chip, and potting which is typically some kind of epoxy.
 Tech-niques for depackaging chips are well known, and discussed in detail in standardworks on semiconductor testing, such as [197].
 In most cases, a milliliter offuming nitric acid is more than enough to dissolve the epoxy.
Probing stations consist of microscopes with micromanipulators attached forlanding ﬁne probes on the surface of the chip.
 They are used in the semiconduc-tor industry for testing production-line samples, and can be bought second-hand(see Figure 18.
4).
 They may have specialized accessories, such as a laser to shootholes in the chip’s passivation layer.
The usual target of a probing attack is the processor’s bus.
 If the bus tra�ccan be recorded, this gives a trace of the program’s operation.
 (It was once arecommended industry practice for the card to compute a checksum on memoryimmediately after reset – giving a complete listing of all code and data.
) So3Such defensive programming was common in the early days of computing, when computersused valves rather than transistors and used to break down every few hours.
 Back then, ifyou masked o↵ three bits, you’d check the result was no more than seven, just to make sure.
Security Engineering559Ross Anderson18.
5.
 SMARTCARDS AND OTHER SECURITY CHIPSFigure 18.
4: – our probing stationthe attacker will ﬁnd the bus and expose it for probing (see Figure 18.
5).
 If thechip is using algorithms like AES and RSA, then unless there’s some defensemechanism that masks the computation, a trace from even a single bus line willbe enough to reconstruct the key [860].
The ﬁrst defense used by the pay-TV card industry was to endow each cardwith multiple keys or algorithms, and arrange things so that only those in currentuse would appear on the processor bus.
 Whenever pirate cards appeared on themarket, a command would be issued over the air to cause legitimate cards toactivate new keys or algorithms from previously unused memory.
 In this way,the pirates’ customers would su↵er a loss of service until the attack could berepeated and new pirate cards or updates could be distributed [2064].
How to hack a smartcard (5)This strategy was defeated by Oliver K¨ommerling’s memory linearizationattack in which the analyst damages the chip’s instruction decoder in such away that instructions such as jumps and calls – which change the programaddress other than by incrementing it – are broken [1078].
 One way to do thisis to drop a grounded microprobe needle on the control line to the instructionlatch, so that whatever instruction happens to be there on power-up is executedrepeatedly.
 The memory contents can now be read o↵ the bus.
 In fact, onceSecurity Engineering560Ross Anderson18.
5.
 SMARTCARDS AND OTHER SECURITY CHIPSFigure 18.
5: – the data bus of an ST16 smartcard prepared for probing byexcavating eight trenches through the passivation layer with laser shots (Photocourtesy Oliver K¨ommerling)some of the device’s ROM and EEPROM is understood, the attacker can skipover unwanted instructions and cause the device to execute only instructions oftheir choice.
 So with a single probing needle, they can get the card to executearbitrary code, and in theory could get it to output its secret key material onthe serial port.
 This can be thought of as an early version of the return-orientedprogramming attack.
 But probing the memory contents o↵ the bus is usuallymore convenient.
There are often several places in the instruction decoder where a groundedneedle will prevent programmed changes in the control ﬂow.
 So even if it isn’tfully understood, memory linearization can often be achieved by trial and error.
One particularly vulnerable smartcard family was the Hitachi H8/300 architec-ture, which had a 16-bit bus with the property that if the most signiﬁcant bitequals 1 then the CPU will always execute single-cycle instructions without anybranches.
 So by shooting the MSB bus line with a laser, the memory could beeasily read out [1781].
 Other CPUs based on RISC cores also tend to su↵er fromthis.
 Some of the more modern processors have traps which prevent memorylinearization, such as watchdog timers that reset the card unless they themselvesare reset every few thousand instructions.
Memory linearization is an example of a fault induction attack.
 There aremany other examples.
 Faults can be injected into processors in many ways,from hardware probing through power transients and laser illumination.
 Onecommon target is the test circuitry.
 A typical chip has a self-test routine inROM that is executed in the factory and allows all the memory contents toSecurity Engineering561Ross Anderson18.
5.
 SMARTCARDS AND OTHER SECURITY CHIPSbe read and veriﬁed.
In some cases, a fuse is blown in the chip to stop anattacker using the facility.
 But the attacker can cause a fault in this mechanism– whether by ﬂipping a bit in Flash memory [1776], or just ﬁnding the fuse andbridging it with two probing needles [302].
 In other cases, the test routine isprotected with a password, which can be found [1775].
We noted in section 5.
7.
1 that the RSA algorithm is fragile in the presenceof failure; one laser shot is all it takes to cause a signature to be right modulop and wrong modulo q, enabling the attacker to factor the key pq.
 Adi Shamirpointed out that if a CPU has an error in its multiply unit – even just a singlecomputation ab = c whose result is returned consistently wrong in a single bit –then you can design an RSA ciphertext for decryption (or an RSA plaintext forsignature) so that the computation will be done correctly mod p but incorrectlymod q, again enabling you to factor the key [1705].
 So a careful programmer willalways check the results of critical computations, and think hard about whaterror messages might disclose.
How to hack a smartcard (6)The next thing the pay-TV card industry tried was to incorporate hardwarecryptographic processors, in order to force attackers to reconstruct hardwarecircuits rather than simply clone software, and to force them to use more ex-pensive processors in their pirate cards.
 In the ﬁrst such implementation, thecrypto processor was a separate chip packaged into the card, and it had an inter-esting protocol failure: it would always work out the key needed to decrypt thecurrent video stream, and then pass it to the CPU which would decide whetheror not to pass it on to the outside world.
 Hackers just tapped the wire betweenthe two chips.
The next version had the crypto hardware built into the CPU itself.
 Wherethis consists of just a few thousand gates, an attacker can trace the circuitmanually from micrographs.
 But with larger gate counts and deep submicronprocesses, a successful attack needs serious tools: you need to etch or grind awaythe layers of the chip, take electron micrographs, and use image processingsoftware to reconstruct the circuit [269].
 Equipment can now be rented andcircuit-reconstruction software can be bought; the short resource now is skilledreverse engineers.
By the late 1990s, some pirates had started to get commercial reverse-engineering labs to reconstruct chips for them.
Such labs get much of theirbusiness from analyzing integrated circuits on behalf of chip makers’ competi-tors, looking for patent infringements.
 They also reverse chips used for accessorycontrol, as doing this for compatibility rather than piracy is lawful.
 Many labswere located in Canada, where copying pay-TV cards wasn’t a crime until 2002(though there were at least two cases where these labs were sued by pay-TV op-erators).
 Some labs are now in China, whose legal system is harder for outsidersto navigate.
How to hack a smartcard (7)In 1995 STM pioneered a new defence, a protective shield on the chip surface.
This was a serpentine sensor line, zig-zagging round ground lines in a top metalSecurity Engineering562Ross Anderson18.
5.
 SMARTCARDS AND OTHER SECURITY CHIPSlayer.
 Any break or short would be sensed as soon as the chip was powered up,whereupon the chip would overwrite the keys.
Sensor mesh shields can really push up the cost of an attack.
 One bypassis to hold the sensor line to VDD with a needle, but this can be fragile; andother vendors have multiple sensor lines with real signals on them.
 So if youcut them, you have to repair them, and the tool for the job is the FocusedIon Beam Workstation (FIB).
 This is a device similar to a scanning electronmicroscope but which uses a beam of ions instead of electrons.
 By varying thebeam current, it can be used either as a microscope or as a milling machine, witha useful resolution under 10 nanometers.
 By introducing a gas that’s brokendown by the ion beam, you can lay down either conductors or insulators witha precision of a few tens of nanometers.
 For a detailed description of FIBs andother semiconductor test equipment that can be used in reverse engineering,see [1233].
FIBs are so useful in all sorts of applications, from semiconductor testingthrough metallurgy and forensics to nanotechnology, that they are widely avail-able in physics and material-science labs, and can be rented for about a hundreddollars an hour.
Given such a tool, it is straightforward to attack a shield that is not poweredup.
 The direct approach is to drill a hole through the mesh to the metal line thatcarries the desired signal, ﬁll it up with insulator, drill another hole through thecenter of the insulator, ﬁll it with metal, and plate a contact on top – typicallya platinum ‘X’ a few microns wide, which you then contact with a needle fromyour probing station (see Figure 18.
6).
 There are many more tricks, such asusing the voltage contrast and backscatter modes of your electron microscopeto work out exactly where to cut, so you can disable a whole section of the mesh.
John Walker has a video tutorial on how to use these tricks to defeat a shieldat [1975]Many other defenses can force the attacker to do more work.
 Some chipshave protective coatings of silicon carbide or boron nitride, which can force theFIB operator to go slowly rather than damage the chip through a build-up ofelectrical charge.
Chips with protective coatings are on display at the NSAMuseum at Fort Meade, Maryland.
How to hack a smartcard (8)In 1998, the smartcard industry was shaken when Paul Kocher announced anew attack known as di↵erential power analysis (DPA).
 This relies on the factthat di↵erent instructions consume di↵erent amounts of power, so by measuringthe current drawn by a chip it was possible to extract the key.
 Smartcard makershad known since the 1980s that this was theoretically possible, and had evenpatented some crude countermeasures.
 But Paul came up with e�cient signalprocessing techniques that made it easy, and which I’ll describe in the followingchapter.
 He came up with even simpler attacks based on timing; if cryptographicoperations don’t take the same number of clock cycles, this can leak key materialtoo4.
 Power and timing attacks are examples of side-channel attacks, where the4On larger processors, it can be even worse; a number of researchers developed attacks oncrypto algorithms such as AES based on cache misses during the 2000s, and in 2018 we hadthe Spectre and Meltdown attacks that exploit transient execution.
 See the chapter on sideSecurity Engineering563Ross Anderson18.
5.
 SMARTCARDS AND OTHER SECURITY CHIPSFigure 18.
6: – the protective mesh of an ST16 smartcard with a FIB cross forprobing the bus line visible underneath (Photo courtesy Oliver K¨ommerling)opponent can observe some extra information about the processor’s state duringa cryptographic computation.
 All the smartcards on the market in 1998 turnedout to be highly vulnerable to DPA, and this held up the industry’s developmentfor a couple of years while countermeasures were developed.
Attacks were traditionally classed as either invasive attacks such as mechan-ical probing, which involves penetrating the passivation layer, and noninvasiveattacks such as power analysis, which leaves the card untouched.
 Noninvasiveattacks can be further classiﬁed into local attacks where the opponent needs ac-cess to the device, as with power analysis; and remote attacks where she couldbe anywhere, such as timing attacks.
 But that was not the whole story.
How to hack a smartcard (9)Mechanical probing techniques have been getting steadily harder becauseof shrinking feature sizes.
 The next attack technology to develop was opticalprobing.
 The ﬁrst report was from Sandia National Laboratories who in 1995described a way to read out a voltage directly using a laser [32].
 Since 2001optical probing has been developed into an e↵ective and low-cost technology,largely by my Cambridge colleague Sergei Skorobogatov.
 In 2002 Sergei and Ireported using a photographic ﬂashgun, mounted on the microscope of a probingstation, to induce transient faults in selected transistors of an IC [1782].
 Thelight ionises the silicon, causing transistors to conduct.
 Once you understandphotoconductivity and learn to focus the light on single transistors, by upgrad-ing from a ﬂashgun to a laser, this enables many direct attacks.
 For example,channels.
Security Engineering564Ross Anderson18.
5.
 SMARTCARDS AND OTHER SECURITY CHIPSmicrocontrollers can be opened by toggling the ﬂip-ﬂop that latches their pro-tection state.
 This gave a new way of causing not just transient fault attacks,as on fragile cryptosystems such as RSA, but faults that are precisely directedand controlled in both space and time.
Later in 2002, Sergei reported using a laser mounted on the same cheapmicroscope to read out a microcontroller’s memory directly.
 The basic idea issimple: if you shine a laser on a transistor, that will induce a photocurrent andincrease the device’s power consumption – unless it was conducting already.
 Soby scanning the laser across the device, you map which transistors are o↵ andwhich are on.
 We developed this into a reasonably dependable way of readingout ﬂip-ﬂops and RAM memory [1648].
 We named our attack semi-invasiveanalysis as it lies between the existing categories of invasive and non-invasive.
It’s not invasive, as we don’t break the passivation; but we do remove the epoxy,so it doesn’t count as non-invasive either.
Optical probing from the front side of the chip remained the state of the artfor about ﬁve years.
 By the time of this book’s second edition (2007), smartcardvendors were using 0.
18 and 0.
13 micron processes, typically with seven metallayers.
 Direct optical probe attacks from the chip surface had become di�cult,not so much because of the feature size but because the metal layers get inthe way.
 In addition, the sheer size and complexity of the chips was makingit di�cult to know where to aim.
 The di�culty was increased by glue logic –essentially randomised place-and-route.
Older chips have clearly distinguishable blocks, and quite a lot can be learnedabout their structure and organisation just by looking.
 Bus lines could be pickedout and targeted for attack.
 However, the SX28 in Figure 18.
7 just looks like arandom sea of gates.
 The only easily distinguishable features are the EEPROM(at top left) and the RAM (at top right).
 It takes some work to ﬁnd the CPU,the instruction decoder and the bus.
I wrote in the second edition, “The two current windows of vulnerability arethe memory and the rear side.
” These have provided our Tamper Lab’s mainresearch targets during the decade since.
How to hack a smartcard (10)Rear-side attacks are the practical semi-invasive option once you get below0.
35µ.
 You go through the back of the chip using an infrared laser at a wave-length around 1.
1µ where silicon is transparent.
 For feature sizes below 65nm,you need to thin down the chip to 2–5µ using some combination of mechanicalpolishing and chemical etching; and there are now special methods to improvethe resolution, such as silicon immersion lenses.
 One physical limit is you can’tget a bandwidth of much over a few MHz because of the time taken for thecharge carriers to recombine.
Rear-side attacks can sometimes be used to extract ROM contents by directobservation, but the main technique is optical fault induction (OFI), which hasnow become a standard security test procedure.
 Silicon immersion lenses haveenabled OFI attacks to continue to create single-event upsets down to 28nmsilicon, even though the laser spot size is about a micron [593].
 Most smartcardscurrent in 2019 tend to use about 90nm with the smallest about 65nm [1862].
The three big vendors have all announced 40nm products.
 So OFI will continueSecurity Engineering565Ross Anderson18.
5.
 SMARTCARDS AND OTHER SECURITY CHIPSFigure 18.
7: – SX28 microcontroller with ‘glue logic’ (courtesy of Sergei Sko-robogatov)to be practical for some time.
With the smaller feature sizes, you have to accept that your aim in bothspace and time will often be fuzzy, and you may use the laser in combinationwith another more precise technique.
 One starting point here was optically-enhanced position-locked power analysis.
By illuminating the n channels ofa memory cell, the signal observed from a state change by power analysis isincreased; with higher light levels, even read accesses can be detected.
 Thisenables much more selective analysis [1771].
How to hack a smartcard (11)By 2010, the logic in most security chips was glue logic with few discerniblefeatures, but since Flash memory needs high voltages and large charge pumps,Flash arrays are large and easily identiﬁable.
Chipmakers worried that theattacks that targeted chips with a separate VP P programming voltage might bereinvented by using a laser to interfere with the charge pumps.
 So they tried tostop both memory corruption and the exploitation of memory readback accessby making secure Flash with a per-block verify-only operation when memory iswritten.
 Sergei’s bumping attack was inspired by the bumping attacks on locksdescribed in chapter 13.
 Just as lock bumping forces cylinders into a desiredstate, so Flash bumping forces bus lines into a desired state as they report theresults of memory veriﬁcation [1774].
Security Engineering566Ross Anderson18.
5.
 SMARTCARDS AND OTHER SECURITY CHIPSBut perhaps the most signiﬁcant recent breakthrough was in 2016, whenFranck Courbon, Sergei Skorobogatov and Chris Woods discovered how to usethe latest generation of scanning electron microscopes to automate the directread-out of Flash and EEPROM.
 As the memory cells store a bit by the presenceor absence of a few hundred electrons in a ﬂoating gate, it’s tricky to read themout without using the circuits designed for the purpose – especially when usinga beam consisting of billions of electrons, aimed through the rear side of thechip.
 (We used to compare this with reading a palimpsest with a blowlamp.
)Making it work requires very careful sample preparation, a SEM that supportspassive voltage contrast (PVC), ﬁne-tuned scan acquisition and e�cient imageprocessing [480].
 Using such tools and techniques, it’s now possible to read outthe 256K of Flash or EEPROM from a typical smartcard or other security chipwith perhaps half a dozen single-bit errors.
 This had been predicted as longago as 2000 by Steve Weingart, the 4758 designer [1999]; PVC made it a reality.
The e↵ect on the smartcard industry is that the entire memory of the chip cannow be read out.
 Reverse engineering is a matter of ﬁguring out the CPU’sinstruction set, how the memory is encrypted, and so on.
How to hack a smartcard (12)Reverse engineering services in China now charge 30c per gate, so the brute-force approach is to just reverse the whole chip and drop it in a simulator withouttrying to understand it in detail.
 Given that a typical smartcard has 100,000gates, this means you can get a simulator for $30,000.
 Then you have all sortsof options.
 Once you have su�ciently understood one card of a particular type,the per-card cloning cost is now the cost of memory extraction.
 You can alsouse the simulation to look for side-channel attacks, to plan FIB edits, or to fuzzthe device and look for other vulnerabitities.
As smartcards are computers, they can sometimes fall to the usual computerattacks, such as stack overwriting by sending too long a string of parameters.
 Asearly as 1996, the Mondex card, used in a payment trial by the UK’s NatWestBank, boasted a formally veriﬁed operating system.
 Yet as late as 2019, softwareattacks worked against at least one SIM card.
 Malicious SMSes were used bynation-state attackers to download malware into the SIMs of target users sothat their location could be tracked [575].
18.
5.
4Random number generators and PUFsMany crypto chips are o↵ered with a random number generator, a physicalunclonable function, or both.
Hardware random number generators (RNGs) are used to produce proto-col nonces and session keys.
 Weak generators have led to many catastrophicsecurity failures, of which a number pop up in this book.
 Poor nonces leadto replay attacks, while weak session keys can compromise long-term signingkeys in cryptographic algorithms such as ECDSA.
 During the 1990s, the fash-ion was for algorithmic random number generation; this is properly known asa pseudorandom number generator (PRNG).
 A crypto chip might have had aspecial key-generation key that was used in counter encryption mode; operatingsystems often had something similar.
 However, if the counter is reset, then theSecurity Engineering567Ross Anderson18.
5.
 SMARTCARDS AND OTHER SECURITY CHIPSoutput is repeated; there have been several variants on this theme.
 I also men-tioned the NIST Dual-EC-DRBG, which was built into Windows and seemed tohave contained an NSA trapdoor [1734]; Ed Snowden later conﬁrmed that theNSA paid RSA $10m to use this standard in tools that many tech companieslicensed [1290].
Hardware random number generators typically quantise jitter or use somesource of metastability such as a cross-coupled inverter pair.
 Such generatorsare notoriously di�cult to test; faults can be induced by external noise suchas temperature, supply voltage and radiation.
 Standards such as NIST SP800-A/B/C call for RNG output to be run through test circuits.
 Crypto productsoften mix together the randomness from a number of sources both environmentaland internal [838], and this is a requirement for the highest levels of certiﬁcation.
The way these sources are combined is often the critical thing and one shouldbeware of designs that try to be too clever [1033].
 One must also beware thathardware RNGs are usually proprietary, obscure designs, sometimes speciﬁc toa single fab, so it’s hard to check that the design is sound, let alone that itdoesn’t contain a subtle backdoor.
 An example of conservative design may bethat used in Intel chips since 2012, which combines both a hardware RNG anda software PRNG that follows it [856].
The manufacture of crypto chips typically involves a personalisation stagewhere serial numbers and crypto keys are loaded into Flash or EEPROM.
 Thisis another attack point: Ed Snowden reported that GCHQ had hacked themechanisms used by Gemalto to personalise cards, and got copies of the keys inmillions of SIMs.
 So one might ask whether chips could be manufactured withan intrinsic key that would never leave the device.
 Each chip would create aprivate key and export the public key, which the vendor would certify duringpersonalisation.
 But this takes time, and also seems to need an RNG on thechip.
 Is there another way?A physical unclonable function (PUF) is a means of identifying a devicefrom variations that occur naturally during manufacture.
 In the 1980s, SandiaNational Laboratories were asked by the US Federal Reserve whether it waspossible to make unforgeable banknote paper, and they came up with the ideaof chopping up optical ﬁbre into the mash, so you could recognise each note bya unique speckle pattern [1746].
 Such a mechanism should be unclonable, andits behaviour should change detectably if it’s tampered with.
 Could somethingsimilar be devised for integrated circuits? In 2000, Oliver and Fritz K¨ommerlingproposed loading chip packaging with metal ﬁbres and measuring its propertiesto generate a key with which the chip contents would be encrypted, so thatdrilling through the packaging would destroy the key [1079].
In 2002 BlaiseGassend, Dwaine Clarke, Marten Van Dijk and Srini Devadas proposed usingprocess variability in the silicon itself, suggesting that a collection of ring os-cillators might be chaotic enough to be unique [754].
 There followed the usualcoevolution of attack and defence as people proposed designs and others brokethem.
Through the 2010s we’ve started to see PUFs appearing in signiﬁcant num-bers of low-cost chips as well as in higher-value products such as FPGAs.
 Thetypical ‘weak PUF’ generates a consistent random number on power-up fromprocess variability; an SRAM PUF reads the initial state of some SRAM cellsSecurity Engineering568Ross Anderson18.
5.
 SMARTCARDS AND OTHER SECURITY CHIPSand is used, with error correction, as a stable random ID or as an AES key toencrypt memory or to drive a PRNG.
 If your opponent is capable of reversingyour circuit and scanning your Flash memory, a PUF may at least force themto go to the trouble of probing the key o↵ the bus, or inducing faults one busline at a time to read it out using di↵erential fault analysis.
PUF marketing often claims much more, and one claim (as well as a researchgoal) is a ‘strong PUF’ which would act as a hardware challenge-response mech-anism.
 Given an input, it would return an output that would be su�cientlydi↵erent for each chip (and each input) to be usable as a cryptographic primi-tive in itself.
 For example, one might send a thousand challenges to the chip atpersonalisation and store the responses for later key updating.
 Note that thiswould not of itself have stopped the NSA attack on Gemalto, as they hacked thepersonalisation ﬁles and if PUFs had been used they’d have got the challenge-response pair ﬁles too.
The state of the art in 2020 appears to be XOR arbiter PUFs, which consistof a chain of multiplexers followed by an arbiter.
 The challenge to the PUF isinput to the address lines of the multiplexers which select a route for signalsto race through them to get to the arbiter.
 To make it harder for an attackerto work out the relative delay on each circuit path, the outputs of a number ofarbiters are XORed together.
 However Fatemeh Ganji, Shahin Tajik and Jean-Pierre Seifert have shown that suitable machine-learning techniques can be usedto model the underlying circuits [745].
 The same authors worked with HeikoLohrke and Christian Boit to develop laser fault induction attacks, guided by thechip’s optical emissions, that disable some arbiters so that others can be learnedmore quickly, and thus signiﬁcantly reduce the PUFs’ entropy [1859].
 There arealways probing attacks, as some routine on the chip has to be able to readthe PUF for it to do any work, and this means the bootloader or the monitor.
As these are often left open to parts of the supply chain for personalisation,warranty and upgrade purposes, it’s hard to see what extra protection suchdevices would give, even if we could invent one that works properly.
 Also, usingsuch devices at scale would tend to make personalisation slower and protocolsmore complex.
 Finally, the strength of a PUF depends on variation that thefab tries its best to eliminate, so a change in silicon process can suddenly makea PUF design insecure.
18.
5.
5Larger chipsThere’s a growing number of larger chips with embedded security functions, typ-ically aimed at manufacturing control or accessory control.
 The granddaddy ofthese products may be the Clipper chip, which the Clinton administration pro-posed in 1993 as a replacement for DES.
 Also known as the Escrowed EncryptionStandard (EES), this was a tamper-resistant chip containing the Skipjack blockcipher and a protocol designed to allow the FBI to decrypt any tra�c encryptedusing it.
 When a user supplied Clipper with some plaintext and a key to encryptit, the chip returned not just the ciphertext but also a Law Enforcement AccessField (LEAF) which contained the user-supplied key encrypted under an FBIkey embedded in the device.
 To prevent people cheating by sending the wrongLEAF with a message, the LEAF had a MAC computed with a ‘family key’Security Engineering569Ross Anderson18.
5.
 SMARTCARDS AND OTHER SECURITY CHIPSshared by all Clipper chips – which had to be tamper-resistant to keep both theSkipjack block cipher and the LEAF family key secret.
As often happens, it wasn’t the tamper-resistance that failed, but the proto-col.
 Almost as soon as Clipper hit the market, Matt Blaze found a vulnerability:as the MAC used to bind the LEAF to the message was only 16 bits long, itwas possible to feed message keys into the device until you got one with a givenLEAF, so a message could be sent with a LEAF that would reveal nothing tothe government [258].
 Clipper was replaced with the Capstone chip, the cryptowars continued by other means, and the Skipjack block cipher was placed in thepublic domain [1400].
Of interest in this chapter are the tamper protection mechanisms used, whichwere claimed at the time to be su�cient to withstand a ‘very sophisticated, wellfunded adversary’ [1398].
 Although it was claimed that the Clipper chip wouldbe unclassiﬁed and exportable, I was never able to get hold of a sample despiterepeated attempts.
 It used Vialink read only memory (VROM) in which bitsare set by blowing antifuses between the metal 1 and metal 2 layers on thechip.
A high-voltage programming pulse is used to melt a conducting paththrough the polysilicon between two metal layers.
This technology was alsoused in the QuickLogic FPGA, which was advertised as a way for ﬁrms toconceal proprietary algorithms, and claimed to be ‘virtually impossible to reverseengineer’; further details and micrographs appeared in its data book [801].
 Arecent variant is the spot breakdown PUF where a high enough voltage is appliedto a bank of transistors for just long enough that about half of them su↵erbreakdown of the gate oxide, creating random failures that can be read as onesand zeros [422].
Fusible links are used on other devices too; recent iPhones, for example,have an AES key burned into the system-on-chip.
 There are basically threeapproaches to reverse engineering an antifuse device.
• The ﬁrst thing to look at is the programming circuitry.
 All such chipshave a test circuit used to read back and verify the bitstream duringprogramming, and many disabled this by melting a single fuse afterwards.
If you can get sample devices and a programmer, you can maybe ﬁnd thisfuse using di↵erential optical probing [1772].
 You then use a FIB to repairit, or bridge it with two probe needles, and read out the bitstream.
 Thisattack technique works not just for antifuse FPGAs but also for the Flashand EEPROM varieties.
• Where you need to read out many fuses, as where they’re used to storean AES key, the brute-force approach is to strip the chip down one layerat a time and read the fuses directly; they turn out to be visible undera suitable chemical stain.
 As this attack is destructive it is typically oflimited interest against keys that are di↵erent in each device (as in theiPhone, or a spot breakdown PUF).
• Where the device implements a cryptographic algorithm, a side-channelattack may be the fastest way in.
Most devices manufactured beforeabout 2000 are rather vulnerable to power analysis, and while smartcardchipmakers have incorporated defences, the makers of larger chips maySecurity Engineering570Ross Anderson18.
5.
 SMARTCARDS AND OTHER SECURITY CHIPShave preferred to avoid paying royalties to Cryptography Research, whichpatented many of the best ones.
 You can always try optical fault inductionto read the key one bit at a time, and since the late 2000s we also knowhow to work with optical emissions, which I’ll discuss later.
Secure FPGAs became big business in the 21st century as ﬁrms outsourcethe manufacture of electronic goods to the Far East but want to control at leastone critical component to prevent overbuild and counterfeiting.
 Most FPGAssold now have conventional memory rather than antifuse, so they can be madereprogrammable.
 If you use a volatile FPGA that stores the bitstream in SRAM,you will want one or more embedded keys kept in nonvolatile memory, so thebitstream is uploaded and then decrypted on power-up.
 For faster power-upyou might choose a non-volatile device that stores the whole bitstream in Flash.
In both cases, there may be fuses to protect the key material and the securitystate [583].
 But do watch out for service denial attacks via the upgrade mecha-nism.
 For example, a Flash FPGA may only have enough memory for one copyof the bitstream, not two; so the na¨ıve approach is to read in the bitstream onceto decrypt it and verify the MAC, and then a second time to reprogram thepart.
 But if the bitstream supplied the second time is corrupt, will you have adead product? And if you allow rollback, your customers can perhaps escapeupgrades by replaying old bitstreams.
 And if an attacker gets your productsto load a random encrypted bitstream, this could cause short circuits and brickthe part.
 So stop and think whether anyone might try to destroy your productbase via a corrupt upgrade; if so, you might consider a secure bitstream loader.
You might also consider a more expensive FPGA with enough on-chip memoryto support old and new bitstreams at the same time.
The second type of large-chip security product is the system-on-chip (SoC)with inbuilt authentication logic.
 The pioneer may have been Sony’s Playstation2 in 2000, which ﬁelded MagicGate, a cryptographic challenge-response protocolrun between the device’s graphics chip and small authentication chips embeddedin legitimate accessories.
 The business model of games console manufacturersincluded charging premium prices for software and additional memory cards,whose sellers had to use copy-control technology and pay the console vendora royalty; this was used to subsidise the initial cost of the console.
 Of course,aftermarket operators would then hack their copy-control mechanisms, so Sonyset out to dominate its aftermarket with a better copy-control technology.
 Thisused some interesting protection tricks; the MagicGate protocol was both simple(so protocol attacks couldn’t be found) and randomised (so attackers couldn’tlearn anything from repeating transactions).
 It took several years and millionsof dollars for the aftermarket ﬁrms to catch up.
 While the authentication logicin a small chip may need a top metal shield, copy traps and layout obfuscationto hide it, the same logic in a large chip can hide among the billions of othertransistors.
By the mid-to-late 2000s, similar logic was appearing in system-on-chip prod-ucts in other industries – sometimes for accessory control, and sometimes to en-able one product be sold with several di↵erent levels of performance as a meansof price discrimination.
 This practice has led to some interesting edge cases.
For example, in 2017 Tesla temporarily ‘upgraded’ the batteries of its model Sand X cars so that owners could get out of the path of Hurricane Irma [1930].
Security Engineering571Ross Anderson18.
5.
 SMARTCARDS AND OTHER SECURITY CHIPSSo how can you hack the magic devices that we ﬁnd everywhere nowadays?Memory readout can be the most dependable attack path.
As an example,Sergei Skorobogatov used the new PVC Flash / EEPROM readout technique toreverse the OmniPod insulin pump.
 Diabetics who know how to program preferto control their own insulin pumps but vendors try to stop them, for bothmarket control and liability reasons.
 The OmniPod’s system-on-chip thereforeruns an authentication protocol with the device’s authorised controller, and theNightscout Foundation, an NGO that supports diabetics, wanted to extract thekeys so patients could optimise the control for their own health needs ratherthan following the treatment protocols devised by Omnipod.
 The analysis isdescribed in [1778].
A second attack path is to look to see whether the device computes withencrypted data, and if so look for a protocol failure or side-channel that givesa way in.
 An early example was the cipher instruction search attack inventedby Markus Kuhn on the DS5002 processor [1102].
 This device pioneered busencryption with hardware that encrypts memory addresses and contents on theﬂy as data are loaded and stored, so it was not limited to the small amount ofRAM that could be ﬁtted into a low-cost tamper-sensing package at the time(1995).
 Markus noticed that some of the processor’s instructions have a visibleexternal e↵ect; one instruction in particular caused the next byte in memoryto be output to the device’s parallel port.
 So if you intercept the bus betweenthe processor and memory using a test clip, you can feed in all possible 8-bitinstruction bytes at some point in the instruction stream until you see a one-byte output.
After using this technique to tabulate the encryption functionfor a few bytes, you can encipher and execute a short program to dump theentire memory contents.
 Similar tricks are still used today, and variants on theattack still work.
 In 2017 Sergei Skorobogatov demonstrated an active attackon a system-on-chip used in the car industry, which used memory encryption tomake bus probing harder.
 By selectively injecting wrong opcodes into the bus,he was able to reverse the encryption function [1779].
A tougher problem was presented by the iPhone.
 In March 2016 FBI directorJames Comey demanded that Apple produce a law-enforcement ‘upgrade’ to itsiOS operating system to enable access to locked iPhones, claiming that the FBIwould otherwise be unable to unlock the phone of the San Bernardino shooter.
Sergei set out to prove him wrong and by August had a working attack.
 Thephone in question, the Apple 5c, has an SoC with an embedded AES key, setup by burning fusible links; as these can be seen under an electron microscope,read-out may be possible but would destroy the SoC.
 AES isn’t vulnerable tocryptanalysis, and the encryption appears to work one cache line at a time,so cipher instruction search won’t work.
 But no matter, as there’s a NANDmirroring attack.
 The phone’s non-volatile memory is a NAND Flash chip whosethe contents are encrypted, one cache line, by the embedded device key, so thatthe chip from one phone can’t be read in another.
 The attack is to desolder thememory chip, mount it in a socket, and copy its contents.
 You then make halfa dozen PIN guesses, and the phone starts to slow down (it locks after ten).
Next, you remove the memory chip and restore its original contents.
 You cannow make half a dozen more attempts.
 With a bit more work, you can clone thechip or build a circuit board to emulate it, so you can guess faster.
 The detailscan be found in [1777].
 In the end, the FBI used a service from Cellebrite, aSecurity Engineering572Ross Anderson18.
5.
 SMARTCARDS AND OTHER SECURITY CHIPSforensics company, which later turned out to be exploiting the Checkm8 bug inthe iPhone ROM [793].
The third type of attack I’ll mention is optical emission analysis, which isstrictly speaking a side channel but which I’ll introduce here as it’s becomingone of the main ways of attacking high-grade crypto chips.
 Photons are emittedwhen semiconductor junctions switch, and photon emission microscopy is anestablished failure analysis technique, with silicon emitting mostly in the nearinfrared near the drain area of n-MOS transistors.
 This was ﬁrst used to attacka crypto implementation in 2008 by Julie Ferrigno and Martin Hlavac, who usedan expensive single-photon counting photomultiplier to read out AES keys froman outdated 0.
8µ microcontroller, but worried that their technique would notwork for technologies smaller than 0.
12µ [681].
 By the following year, SergeiSkorobogatov found that a photomultiplier sold to hobby astronomers was nearideal and discovered a voltage boost trick: increasing the chip supply voltagefrom 1.
5V to 2V increases the photon output sixfold.
 He found he was almostable to read out the AES keys from the internal crypto engine of a modern chip,the Actel ProASIC3 FGPA.
 Then, once the AES algorithm timing had beenestablished, and he knew each round key took 1.
6µs, he further increased thevoltage to 2.
5V for the 0.
2µs of an individual bus write, giving a further fourfoldincrease in the photon output plus temporal resolution, which enabled him toread each word of round key clearly o↵ the bus.
 This was all rather embarrass-ing as I’d consulted on the design to Actel back in 2001.
 The ProASIC3 wasfabricated in a 0.
13µ technology with 7 metal layers and ﬂash memory, and wehad built in all sorts of countermeasures to block the attacks we knew about atthe time; reading it out invasively would have been tedious.
 That was a sharpreminder that it’s hard to block the attacks that haven’t been invented yet, andthat attacks can improve very quickly once experts start to hone them.
 Opticalemission analysis is now used in combination attacks: if you want to attack achip that’s too big to reverse engineer, you observe the emissions as it does thecryptography and this tells you where to aim your laser as you try a fault attackor optically-enhanced power analysis.
 It can also suggest where you might laydown a few probe points with your FIB.
18.
5.
6The state of the artHow well can you protect a single-chip product against a capable motivatedopponent? In the late 1990s, everything got broken, and in the 2001 edition ofthis book, I wrote, “there isn’t any technology, or combination of technologies,known to me which can make a smartcard resistant to penetration by a skilledand determined attacker.
” During the 2000s, the defence improved because ofthe e↵orts of the pay-TV ﬁrms and the banking industry, so in the second editionI wrote “This is still almost true, but .
.
.
 you can be looking at a year’s delay, abudget of over a million dollars, and no certainty of success.
”Now, in 2019, Moore’s law has run out of steam; crypto chips are mostlystuck at about 100nm, while the semiconductor test equipment industry is aim-ing to support 9nm processing and still turning out innovations such as passivevoltage contrast microscopy; and researchers are ﬁnding innovative ways to usetheir products.
 So the attackers are starting to catch up.
 The scope of theSecurity Engineering573Ross Anderson18.
5.
 SMARTCARDS AND OTHER SECURITY CHIPSindustry is also increasing.
In 2007, we had a handful of smartcard OEMs,a handful of reversing labs and a handful of interested academics; now manychipmakers are being asked by their customers for some tamper-resistance, asproducts from routers to the Raspberry Pi acquire some kind of secure bootcapability to defeat persistent malware.
 So there are ever more medium-gradeproducts that are suitable for grad students to learn the art and craft of hard-ware reverse engineering5.
 And the growing demand, particularly in China, toreverse devices for compatibility drives the growth of commercial reversing labs.
The market’s now big enough for people to make a living selling specialist toolssuch as layout-reconstruction software and optical fault induction workstations.
As a result, attackers are getting more numerous and more e�cient.
 I suspectthat the cost of cloning a smartcard will steadily come down through the tensof thousands and perhaps into the single thousands.
Security economics remains a big soft spot, with security chips being in manyways a market for lemons.
 A banker buying HSMs probably won’t be awareof the huge gap between FIPS level 3 and level 4, and understand that level3 can sometimes be defeated with a Swiss army knife.
 The buying incentivethere is compliance, and where real security clashes with operations it’s notsurprising to see weaker standards designed to make compliance easier.
 APIsecurity is too hard, and the di↵erence between HSMs’ internal and externalAPIs makes it too confusing.
 The near-abdication of FIPS in favour of ISO19790 and various protection proﬁles touted under the Common Criteria willconfuse things further, as will the UK’s move away from the Criteria.
 Confusionmarketing and liability games appear set to continue.
 But does this matter?First, most of the HSM business is moving to the cloud, with Azure andAWS each having of the order of 2,000 HSMs, and Google playing catchup.
Instead of having a few thousand banks each running a few, or a few dozen,HSMs we’ll have three companies running a few thousand.
 As the prices aredriven down, the HSM vendor engineers’ expertise will be lost; and as the cloudservice providers guard their datacentres, HSMs are likely to be replaced bycrypto chips.
Second, most of the volume smartcard markets – SIM cards and EMV cards– have only moderate physical protection requirements as a full compromiseenables the attacker to exploit one account only.
 You don’t want a bad terminalto be able to do production power-analysis attacks on every EMV card it sees,but even if that were to happen it’s not the end of the world, as that’s how mag-stripe cards got cloned, and we know how to limit the damage.
 The pay-TVmarkets used to lead innovation and customise the chips they used, as a singlebreak can enable a pirate to sell hundreds of thousands of clone cards.
 Butpay-TV is now moving to wireline broadband, and the companies learned thatmore secure chips aren’t the only way to cut losses: more complex smartcardsplayed a role, but much of the improvement came from legal action againstpirates, and from making technical and legal measures work together e�ciently.
Gadget makers nowadays lock their products into ecosystems with cloud servicesand apps, which makes manufacturing control less dependent on tamper-proofFPGAs.
5My colleagues Franck Courbon, Markus Kuhn and Sergei Skorobogatov now run just sucha course for our graduate students.
Security Engineering574Ross Anderson18.
6.
 THE RESIDUAL RISKI therefore expect that although the number and variety of crypto chips willcontinue to increase, the quality of physical protection will remain indi↵erent.
Vendors will spend only as much money as they need to in order to meet cer-tiﬁcation requirements, which will remain slippery and will be gamed.
 Securityengineers will have to get used to building systems out of grey-box components– chips from which keys and algorithms can be extracted, given some e↵ort.
I suspect that accessory control will remain the toughest hardware battle-ﬁeld.
 Aftermarket control isn’t just about printer cartridges nowadays but ex-tends to vehicles, medical devices and other high-value products.
 But where atleast one of the two devices that authenticate each other goes online at leastoccasionally, the protection requirements are much less severe than for satelliteTV.
 The real question will be how to stop attacks scaling.
18.
6The Residual RiskThe security engineer will therefore have to pay attention to the many failuremodes of systems involving tamper-resistant processors that are more or lessindependent of the price or technical tamper-resistance of the device.
18.
6.
1The trusted interface problemNone of the devices described in the above sections has a really trustworthy userinterface6.
 Some of the bank security modules have a physical lock (or two) onthe front to ensure that only the person with a given metal key (or smartcard)can perform privileged transactions.
 But whether you use a $2000 4765 or a $2smartcard to do digital signatures, you still trust the PC that drives them.
 If itshows you a text saying“Please pay amazon.
com $47.
99 for a copy of Anderson’sSecurity Engineering”while the message it actually sends for signature is“Pleaseremortgage my house at 13 Acacia Avenue and pay the proceeds to Maﬁa RealEstate Inc”, then the tamper resistance hasn’t bought you much.
Indeed, it probably makes your situation worse.
 Nick Bohm, Ian Brown andBrian Gladman pointed out that when you use a qualifying electronic signaturedevice, you’re saying ‘I agree to be unreservedly liable for all signatures thatare veriﬁed by the key that I now present to you and I will underwrite all therisks taken by anyone as a result of relying on it’ [277].
 I will discuss the historyand politics of this later in Section 26.
5.
2.
 The EU eIDAS regulation requiresall EU governments to accept qualifying electronic signatures for transactionswhere they previously required ink on paper, and set standards for technicalcertiﬁcation of signature devices.
 The industry has duly produced dozens ofcertiﬁed products.
Given the liability shift compared with ink-on-paper sig-natures, no sensible person would use a qualifying electronic signature deviceunless they had to.
 So the lobbyists have been at work, and some countriesnow insist you use them to ﬁle your taxes.
 This has led researchers in Germanyto look closely at how signatures, signature veriﬁcation services and pdf ﬁles6The iPhone secure enclave processor (SEP) has a direct link to the ﬁngerprint reader butrelies on the main application processor for everything else including FaceID.
Security Engineering575Ross Anderson18.
6.
 THE RESIDUAL RISKinteract; as you might expect, the results are somewhat shocking.
 VladislavMladenov, Christian Mainka, Kersten Mayer zu Selhausen, Martin Grothe andJ¨org Schwenk created a document signed by Amazon in Germany and backedby all the o�cial machinery, certifying that you are due a refund of one trilliondollars.
 They found three new attacks on pdf signatures, worked out how tobypass signature validation in 21 out of 22 viewers, and cheated 6 of 8 onlinevalidation services [1326].
 It’s a fair bet that this is just the tip of an iceberg.
Another example comes from the hardware wallets that some people useto store cryptocurrency.
 Early products had no trusted display and were thusvulnerable to malware.
 Some later ones combined a smartcard chip acting as asecure element, with a less secure microcontroller driving a display.
 This opensa number of possibilities – including an evil maid attack described by SaleemRashid where someone with temporary access to the device, such as a hotel maid,reﬂashes the microcontroller software [1580].
 In this case the secure element hadno idea whether the main processor was running compromised code.
Trustworthy interfaces aren’t always needed, as tamper-resistant processorsare often able to do useful work without having to authenticate users.
 Recallthe example of prepayment electricity metering, in Chapter 14: there, tamper-resistant processors can maintain a value counter, enforcing a credit limit oneach operator and limiting the loss when a vending machine is stolen.
 Postalmeters work the same way.
 In other applications from printer ink cartridgesthrough games consoles to prepaid phone cards, the vendor mainly cares aboutuse control.
18.
6.
2ConﬂictsA further set of issues is that where an application is implemented on devicesunder the control of di↵erent parties, you have to consider what happens wheneach party attacks the others.
 In banking, the card issuer, the terminal ownerand the customer are di↵erent; all the interactions of cloned cards, bogus ter-minals, gangland merchants and cheating banks need to be thought through.
A particular source of conﬂict and vulnerability is that many of the users oftamper resistance have business models that make their customers the enemy– such as rights management and accessory control.
 Their customers may ownthe product, but have the incentive to tamper with it if they can.
 In the caseof accessory control, they may also have a legal right to try to break it; andwhere the mechanisms are used to limit device lifetime and thus contribute toenvironmental pollution, they may even feel they have a moral duty.
18.
6.
3The lemons market, risk dumping and evaluationgamesEach of the product categories discussed here, from HSMs down through FPGAsto smartcards, has a wide range of o↵erings with wide variability in the qualityof protection.
 Many products have evaluations, but interpreting them is hard.
First, there are relatively few o↵erings at high levels of assurance – whetherSecurity Engineering576Ross Anderson18.
6.
 THE RESIDUAL RISKFIPS-140 level 4 or Common Criteria levels above 4.
 There are many at lowerlevels, where the tests are fairly easy to pass, and where vendors can shop aroundfor a lab that will give them an easy ride.
 This leads to a lemons market inwhich all but the best informed buyers will be tempted to go for the cheapestFIPS level 3 or CC EAL4 product.
Second, evaluation certiﬁcates don’t mean what they seem.
 Someone buyinga 4758 in 2001 might have interpreted its level 4 evaluation to mean that itwas unbreakable – and then been startled when we broke it.
 In fact, the FIPScertiﬁcate referred only to the hardware, and we found vulnerabilities in thesoftware.
 It’s happened the other way too: there’s been a smartcard with aCommon Criteria level 6 evaluation, but that referred only to the operatingsystem – which ran on a chip with no real defences against microprobing.
 I’lldiscuss the failings of evaluation systems at greater length in Part III.
Third, while HSMs tend to be evaluated under FIPS, smartcard vendorstend to use the Common Criteria.
 There the tussles are about which protectionproﬁle to use; vendors naturally want the labs to evaluate the aspects of securitythey think they’re good at.
Finally, many ﬁrms use secure processors to dump risk rather than minimiseit.
 Banks love to be able to say ‘your chip and PIN card was used, so it’s yourfault’ and in many countries the regulators let them get away with it.
 There aremany environments, from medicine to defense, where buyers want a certiﬁcateof security rather than real protection, and this interacts in many ways with theﬂaws in the evaluation system.
 Indeed, the main users of evaluated productsare precisely those system operators whose focus is on due diligence rather thanrisk reduction.
18.
6.
4Security-by-obscurityMany designers have tried hard to keep their cryptoprocessor secret.
 You havealmost always had to sign an NDA to get smartcard development tools.
 Theprotection proﬁles still used for evaluating many smartcards under the CommonCriteria emphasise design obscurity.
 Chip masks have to be secret, instructionset architectures are proprietary, sta↵ have to be vetted, developers have tosign NDAs – these all pushed up industry’s costs [650].
 Obscurity was also acommon requirement for export approval, leading to a suspicion that it covers updeliberate vulnerabilities.
 For example, a card we tested would always producethe same value when instructed to generate a private / public keypair and outputthe public part.
 Many products that incorporate encryption have been brokenbecause their random number generators weren’t random enough [775, 576] andas we discussed, the NSA got NIST to standardise a weak one.
Some HSM vendors have been an honourable exception; IBM’s CommonCryptographic Architecture has been well documented from the beginning, ashas Intel’s SGX and the core mechanisms of Arm’s TrustZone.
 This opennesshas facilitated the discovery of API attacks on IBM’s product, as well as side-channel and ROP attacks on Intel’s and more recently Arm’s.
 But most suchattacks have been disclosed responsibly and the learning process has improvedtheir products.
Security Engineering577Ross Anderson18.
7.
 SO WHAT SHOULD ONE PROTECT?One tussle in 2020 is over whether the development environment needs to beair-gapped.
 This has been common practice for years in smartcard OEMs; onelab we visited had only a single PC connected to the Internet (painted red, ona pedestal) so sta↵ could book ﬂights and hotels.
 These ﬁrms are now pushingevaluators to emphasise the risk that an attacker ends up owning the entirecompany infrastructure using an advanced persistent threat.
 That would makelife inconvenient for ﬁrms that have always operated online, as they would haveto rebuild toolchains and change their workﬂows.
A smart evaluator would not be taken in by such gamesmanship.
 Almostnone of the actual attacks on smartcards used inside information; most of themstarted out with a probing attack or side-channel attack on a card bought atretail.
 As the industry did not do hostile attacks on its own products in theearly years, its products were weak and were eventually broken by others.
 Sincethe late 1990s some organisations, such as VISA, have speciﬁed penetrationtesting [1963].
 But the incentives are still wrong; a sensible vendor will go towhatever evaluation lab o↵ers the easiest ride.
We’ll discuss the underlyingeconomics and politics of evaluation in Section 28.
2.
7.
2.
18.
6.
5Changing environmentsWe’ve already seen examples of how function creep, and changes in the environ-ment, have broken systems by undermining their design assumptions.
 A generalproblem is ‘leverage’ – where ﬁrms try to exploit infrastructure maintained byothers, without negotiating proper contracts.
 We’ve seen how the SIM card thatwas previously just a means of identifying people to the phone company becamea token that controls access to their bank accounts.
 In the second edition ofthis book, I wrote “Does this matter? .
.
.
 I’d say it probably doesn’t; using textmessages to conﬁrm bank transactions gives a valuable second authenticationchannel at almost zero marginal cost to both the bank and the customer.
” Atthat time, we had one reported case of a SIM swap attack, in South Africa.
In the following paragraph, I wrote: “But what will happen in ﬁve or tenyears’ time, once everyone is doing it? What if the iPhone takes o↵ as Applehopes, so that everyone uses an iPhone not just as their phone, but as their webbrowser? All of a sudden the two authentication channels have shrunk to one.
”And so it is; SIM swap is now going mainstream in the USA.
This is actually tied up with local law and regulation.
 In most countries,phone companies are not liable to banks for failing to authenticate their cus-tomers properly.
 After all, phone companies just sell minutes, and the marginalcost of stolen minutes is near zero.
 But one country with little SIM swap fraudis India, where regulators decided that phone companies must share liability forSIM swap fraud, and where the phone company is required to check a customer’sﬁngerprint against the national Aadhar database before selling them a SIM.
18.
7So What Should One Protect?In such a complex world, what value can tamper-resistant chips add?Security Engineering578Ross Anderson18.
7.
 SO WHAT SHOULD ONE PROTECT?First, they can tie information processing to a single physical token.
 A pay-TV subscriber card can be bought and sold in a grey market, but so long as itisn’t copied the station operator isn’t losing much revenue.
 This also applies toaccessory control, where a printer vendor wants their product to work with anygenuine ink cartridge, just not with a cheap competitor.
Second, they can maintain value counters, as with the postal metering dis-cussed in Chapter 13.
 Even if the device is stolen, the total value of the serviceit can vend is limited.
 In printers, ink cartridges can be programmed to dispenseonly so much ink and then declare themselves dry.
Third, they can reduce the need to trust human operators.
 Their main pur-pose in some government systems was ‘reducing the street value of key materialto zero’.
 A crypto ignition key for a secure phone should allow a thief onlyto masquerade as the rightful owner, and only if they have access to an actualdevice, and only so long as neither the key nor the phone has been reportedstolen.
 The same general considerations applied in ATM networks, which notonly implement a separation-of-duty policy, but transfer a lot of the trust frompeople to things.
Fourth, they can protect a physical root of trust that monitors secure boot,and thus make it hard for malware to be persistent.
 This mission of its own doesnot require high-grade physical protection; security against capable motivatedsoftware attackers is the key.
 One question is whether activists who want torun their own favoured version of Linux on their devices actually have to breakthe TPM, or whether they can just ignore it and manage the malware riskthemselves.
Fifth, they can control the risk of overproduction by untrusted hardwarecontractors: sometimes called the ‘third shift’ problem, where the factory youhire runs two shifts to make devices for you and a third shift to make some morefor grey-market sale.
 This can involve embedding part of the design in an FPGAthat’s hard to reverse engineer, or by having a TPM to control the credentialsnecessary for the device to work in your ecosystem.
 As things acquire cloudservices and apps, ﬁrms are moving from the former strategy to the latter,which has lower hardware costs and is easier to manage.
 You just release asmany credentials as the factory ships you products.
Sixth, such techniques can control some of the more general risk from coun-terfeit electronic parts.
 This covers a multitude of sins, from cheap knock-o↵sthat cause early product failure through to sophisticated supply-chain attacksby state adversaries.
 For a survey, see Guin et al [833].
 The techniques describedin this chapter also ﬁnd use in the ﬁght against counterfeiting, as do many ofthe tools.
 As for supply-chain attacks, the most pernicious may be hardwaretrojans.
 One national-security concern is that as defence systems increasinglydepend on chips fabricated overseas, the fabs might introduce extra circuitryto facilitate later attack.
 For example, some extra logic might cause a 64-bitmultiply with two speciﬁc inputs to function as a kill switch.
 This has beenthe subject of signiﬁcant research since about 2010, and mechanisms have beendeveloped for Trojan detection both pre-silicon and post-silicon; for example,you can do a di↵erential side-channel analysis of a ‘golden’ reference chip anda target of evaluation [1775].
This of course assumes that you can produceSecurity Engineering579Ross Anderson18.
8.
 SUMMARYa reference chip in a trustworthy fab.
 For a survey of this ﬁeld, see Xiao etal [2053].
This is an incomplete list.
 But what these applications have in common isthat a security property can be provided independently of the trustworthinessof the surrounding environment.
 But beware: the actual protection propertiesthat are required and provided can be quite subtle, and tamper-resistant devicesare more often a useful component than a full solution.
 Generic mechanisms failagain and again; security is not some kind of magic pixie dust that you sprinkleon a system to cause bad things to not happen.
 You need to work out whatbad things you want to stop.
 If you’re not careful you can ﬁnd yourself payingfor smartcards and crypto modules in applications where they add rather little;and if you’re really unlucky you may ﬁnd that the industry lobbied for legalmandates or industry standards to force you to use their products.
18.
8SummaryTamper-resistant devices and systems have a long history.
 Computers can beprotected against physical tampering in a number of ways, from keeping themlocked up in a guarded room, through putting them in tamper-sensing boxes, tomaking them into single chips with shields against probing and defences againstside-channel attacks.
I’ve told the story of how hardware tamper-resistance developed through aseries of cycles of attack and defence, and given examples of applications.
 Secu-rity processors are typically vulnerable to attacks on interfaces (human, sensoror system) but can often deliver value in applications where we need to link pro-cessing to physical objects and to protect security state against scalable threats,particularly in environments where any online service may be intermittent.
Research ProblemsThere are basically two strands of research in tamper-resistant processor design.
The ﬁrst concerns itself with making ‘faster, better, cheaper, more secure’ pro-cessors: how can the protection o↵ered by a high-end device be brought to chipsthat cost under a dollar? The second concerns itself with pushing forward thestate of the attack art.
 How can the latest chip testing technologies be used tomake ‘faster, better, cheaper, novel’ attacks? The best guide for the second maybe Sergei Skorobogatov’s 2018 talk, “Hardware Security: Present challenges andFuture directions” [1780].
A broader area of research is how to build more secure systems out of lesssecure components.
 How can moderately protected chips be used e↵ectively tostop various kinds of attack scaling?Security Engineering580Ross Anderson18.
8.
 SUMMARYFurther ReadingI’m not aware of any up-to-date systematisation of knowledge paper on hardwaretamper resistance.
 Colleagues and I wrote a survey of security processors in2005 [100] which might serve as a more detailed starting point, if slightly dated;of the same vintage are a summer school on attack techniques [1772] as wellas reviews of FPGA security [583] and microcontroller security [1767, 1769].
Bunnie Huang’s book on hacking the Xbox is still a good read [930].
 A slightlylater summary, from an industry perspective, is by Randy Torrance and DickJames of Chipworks in 2009 [1897].
As for the last decade of research, the best current papers often appear atconferences such as CHES (for the crypto), HOST (Trojans and backdoors),FDTC (fault attacks) and Cardis (smartcards).
 Failure analysis research tendsto appear at ISTFA and IPFA.
For the early history – the weighted codebooks and water-soluble inks – readDavid Kahn’s book ‘The Code Breakers’ [1001].
 For a handbook on the chipcard technology of the mid-to-late 1990s, see [1578], while the gory details of howwe started tampering with those generations of cards can be found in [106, 107,1078].
 The IBM products mentioned have extensive documentation online [951],where you can also ﬁnd the US FIPS documents [1397].
For modern chip testing techniques, I recommend the video of a keynotetalk by John Walker at Hardwear.
IO 2019 on how to use FIBs in reverse engi-neering [1975] as well as the talks at the same event by Chris Tarnovsky on theevolution of chip defense technology [1862].
Security Engineering581Ross Anderson